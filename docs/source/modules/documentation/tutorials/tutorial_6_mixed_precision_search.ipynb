{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 6: Mixed Precision Quantization Search with Mase and Optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we'll see how Mase can be integrated with Optuna, the popular hyperparameter optimization framework, to search for a Bert model optimized for sequence classification on the IMDb dataset. We'll take the Optuna-generated model and import it into Mase, then run the CompressionPipeline to prepare the model for edge deployment by quantizing and pruning its weights.\n",
    "\n",
    "As we'll see, running Architecture Search with Mase/Optuna involves the following steps.\n",
    "\n",
    "1. **Define the search space**: this is a dictionary containing the range of values for each parameter at each layer in the model.\n",
    "\n",
    "2. **Write the model constructor**: this is a function which uses Optuna utilities to sample a model from the search space, and constructs the model using transformers from_config class method.\n",
    "\n",
    "3. **Write the objective function**: this function calls on the model constructor defined in Step 2 and defines the training/evaluation setup for each search iteration.\n",
    "\n",
    "4. **Go!** Choose an Optuna sampler, create a study and launch the search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"prajjwal1/bert-tiny\"\n",
    "tokenizer_checkpoint = \"bert-base-uncased\"\n",
    "dataset_name = \"imdb\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are starting from scratch, you can load the Bert checkpoint directly from HuggingFace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vol/bitbucket/ug22/adls/.venv/lib/python3.11/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "model = AutoModel.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have previously ran the tutorial on Neural Architecture Search (NAS), run the following cell to import the best model obtained from the search process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    checkpoint,\n",
    "    num_labels=2  # IMDb is binary classification\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/homes/ug22/tutorial_5_best_model.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdill\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mPath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhome\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/tutorial_5_best_model.pkl\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m      5\u001b[39m     base_model = dill.load(f)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/vol/bitbucket/ug22/adls/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py:344\u001b[39m, in \u001b[36m_modified_open\u001b[39m\u001b[34m(file, *args, **kwargs)\u001b[39m\n\u001b[32m    337\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m}:\n\u001b[32m    338\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    339\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPython won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m by default \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    340\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    341\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myou can use builtins\u001b[39m\u001b[33m'\u001b[39m\u001b[33m open.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    342\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m344\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '/homes/ug22/tutorial_5_best_model.pkl'"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import dill\n",
    "\n",
    "with open(f\"{Path.home()}/tutorial_5_best_model.pkl\", \"rb\") as f:\n",
    "    base_model = dill.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, fetch the dataset using the `get_tokenized_dataset` utility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO    \u001b[0m \u001b[34mTokenizing dataset imdb with AutoTokenizer for bert-base-uncased.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from chop.tools import get_tokenized_dataset\n",
    "\n",
    "dataset, tokenizer = get_tokenized_dataset(\n",
    "    dataset=dataset_name,\n",
    "    checkpoint=tokenizer_checkpoint,\n",
    "    return_tokenizer=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Defining the Search Space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by defining a search space, i.e. enumerating the possible combinations of hyperparameters that Optuna can choose during search. We'll explore the following range of values for the model's hidden size, intermediate size, number of layers and number of heads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from chop.nn.quantized.modules.linear import (\n",
    "    LinearInteger,\n",
    "    LinearMinifloatDenorm,\n",
    "    LinearMinifloatIEEE,\n",
    "    LinearLog,\n",
    "    LinearBlockFP,\n",
    "    LinearBlockMinifloat,\n",
    "    LinearBlockLog,\n",
    "    LinearBinary,\n",
    "    LinearBinaryScaling,\n",
    "    LinearBinaryResidualSign,\n",
    ")\n",
    "\n",
    "search_space = {\n",
    "    \"linear_layer_choices\": [\n",
    "        torch.nn.Linear,\n",
    "        LinearInteger,\n",
    "    ],\n",
    "    \"width_choices\": [8, 16, 32],\n",
    "    \"frac_width_choices\": [2, 4, 8],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Writing a Model Constructor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the following function, which will get called in each iteration of the search process. The function is passed the `trial` argument, which is an Optuna object that comes with many functionalities - see the [Trial documentation](https://optuna.readthedocs.io/en/stable/reference/trial.html) for more details. Here, we use the `trial.suggest_categorical` function, which triggers the chosen sampler to choose a layer type. The suggested integer is the index into the search space for each parameter, which we defined in the previous cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chop.tools.utils import deepsetattr\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "def construct_model(trial):\n",
    "\n",
    "    # Fetch the model\n",
    "    trial_model = deepcopy(base_model)\n",
    "\n",
    "    # Quantize layers according to optuna suggestions\n",
    "    for name, layer in trial_model.named_modules():\n",
    "        if isinstance(layer, torch.nn.Linear):\n",
    "            new_layer_cls = trial.suggest_categorical(\n",
    "                f\"{name}_type\",\n",
    "                search_space[\"linear_layer_choices\"],\n",
    "            )\n",
    "\n",
    "            if new_layer_cls == torch.nn.Linear:\n",
    "                continue\n",
    "\n",
    "            kwargs = {\n",
    "                \"in_features\": layer.in_features,\n",
    "                \"out_features\": layer.out_features,\n",
    "            }\n",
    "\n",
    "            # If the chosen layer is integer, define the low precision config\n",
    "            # if new_layer_cls == LinearInteger:\n",
    "            #     kwargs[\"config\"] = {\n",
    "            #         \"data_in_width\": 8,\n",
    "            #         \"data_in_frac_width\": 4,\n",
    "            #         \"weight_width\": 8,\n",
    "            #         \"weight_frac_width\": 4,\n",
    "            #         \"bias_width\": 8,\n",
    "            #         \"bias_frac_width\": 4,\n",
    "            #     }\n",
    "            # elif... (other precisions)\n",
    "\n",
    "            # If the chosen layer is integer, let Optuna pick width/frac_width for THIS layer\n",
    "            if new_layer_cls == LinearInteger:\n",
    "                width = trial.suggest_categorical(\n",
    "                    f\"{name}_width\", \n",
    "                    search_space[\"width_choices\"]\n",
    "                )\n",
    "                frac_width = trial.suggest_categorical(\n",
    "                    f\"{name}_frac_width\", \n",
    "                    search_space[\"frac_width_choices\"]\n",
    "                )\n",
    "                \n",
    "                kwargs[\"config\"] = {\n",
    "                    \"data_in_width\": width,\n",
    "                    \"data_in_frac_width\": frac_width,\n",
    "                    \"weight_width\": width,\n",
    "                    \"weight_frac_width\": frac_width,\n",
    "                    \"bias_width\": width,\n",
    "                    \"bias_frac_width\": frac_width,\n",
    "                }\n",
    "\n",
    "            # Create the new layer (copy the weights)\n",
    "            new_layer = new_layer_cls(**kwargs)\n",
    "            new_layer.weight.data = layer.weight.data\n",
    "\n",
    "            # Replace the layer in the model\n",
    "            deepsetattr(trial_model, name, new_layer)\n",
    "\n",
    "    return trial_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Defining the Objective Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define the objective function for the search, which gets called on each trial. In each trial, we create a new model instace with chosen hyperparameters according to the defined sampler. We then use the `get_trainer` utility in Mase to run a training loop on the IMDb dataset for a number of epochs. Finally, we use `evaluate` to report back the classification accuracy on the test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chop.tools import get_trainer\n",
    "import random\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "\n",
    "    # Define the model\n",
    "    model = construct_model(trial)\n",
    "\n",
    "    trainer = get_trainer(\n",
    "        model=model,\n",
    "        tokenized_dataset=dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        evaluate_metric=\"accuracy\",\n",
    "        num_train_epochs=1,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    eval_results = trainer.evaluate()\n",
    "\n",
    "    trial.set_user_attr(\"model\", model)\n",
    "\n",
    "    return eval_results[\"eval_accuracy\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Launching the Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optuna provides a number of samplers, for example:\n",
    "\n",
    "* **GridSampler**: iterates through every possible combination of hyperparameters in the search space\n",
    "* **RandomSampler**: chooses a random combination of hyperparameters in each iteration\n",
    "* **TPESampler**: uses Tree-structured Parzen Estimator algorithm to choose hyperparameter values.\n",
    "\n",
    "You can define the chosen sampler by simply importing from `optuna.samplers` as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optuna.samplers import GridSampler, RandomSampler, TPESampler\n",
    "\n",
    "sampler = TPESampler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all the pieces in place, we can launch the search as follows. The number of trials is set to 1 so you can go get a coffee for 10 minutes, then proceed with the tutorial. However, this will essentially be a random model - for better results, set this to 100 and leave it running overnight!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-02-02 16:51:25,760] A new study created in memory with name: bert-tiny-mixed-precision-study\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'linear_layer_choices': [<class 'torch.nn.modules.linear.Linear'>, <class 'chop.nn.quantized.modules.linear.LinearInteger'>], 'width_choices': [8, 16, 32], 'frac_width_choices': [2, 4, 8]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vol/bitbucket/ug22/adls/.venv/lib/python3.11/site-packages/optuna/distributions.py:502: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'torch.nn.modules.linear.Linear'> which is of type type.\n",
      "  warnings.warn(message)\n",
      "/vol/bitbucket/ug22/adls/.venv/lib/python3.11/site-packages/optuna/distributions.py:502: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'chop.nn.quantized.modules.linear.LinearInteger'> which is of type type.\n",
      "  warnings.warn(message)\n",
      "/vol/bitbucket/ug22/adls/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:33, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.630800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.473800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.429000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.382700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.366700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.375300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:15]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-02-02 16:52:15,435] Trial 0 finished with value: 0.84796 and parameters: {'bert.encoder.layer.0.attention.self.query_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.attention.self.key_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.0.attention.self.key_width': 8, 'bert.encoder.layer.0.attention.self.key_frac_width': 8, 'bert.encoder.layer.0.attention.self.value_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.0.attention.self.value_width': 8, 'bert.encoder.layer.0.attention.self.value_frac_width': 2, 'bert.encoder.layer.0.attention.output.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.0.attention.output.dense_width': 32, 'bert.encoder.layer.0.attention.output.dense_frac_width': 8, 'bert.encoder.layer.0.intermediate.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.output.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.0.output.dense_width': 32, 'bert.encoder.layer.0.output.dense_frac_width': 2, 'bert.encoder.layer.1.attention.self.query_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.attention.self.query_width': 8, 'bert.encoder.layer.1.attention.self.query_frac_width': 4, 'bert.encoder.layer.1.attention.self.key_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.attention.self.key_width': 16, 'bert.encoder.layer.1.attention.self.key_frac_width': 2, 'bert.encoder.layer.1.attention.self.value_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.attention.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.intermediate.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.output.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.output.dense_width': 8, 'bert.encoder.layer.1.output.dense_frac_width': 4, 'bert.pooler.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'classifier_type': <class 'torch.nn.modules.linear.Linear'>}. Best is trial 0 with value: 0.84796.\n",
      "/vol/bitbucket/ug22/adls/.venv/lib/python3.11/site-packages/optuna/distributions.py:502: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'torch.nn.modules.linear.Linear'> which is of type type.\n",
      "  warnings.warn(message)\n",
      "/vol/bitbucket/ug22/adls/.venv/lib/python3.11/site-packages/optuna/distributions.py:502: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'chop.nn.quantized.modules.linear.LinearInteger'> which is of type type.\n",
      "  warnings.warn(message)\n",
      "/vol/bitbucket/ug22/adls/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:30, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.627400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.464400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.416300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.383900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.360500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.372700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:13]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-02-02 16:53:00,231] Trial 1 finished with value: 0.85688 and parameters: {'bert.encoder.layer.0.attention.self.query_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.0.attention.self.query_width': 32, 'bert.encoder.layer.0.attention.self.query_frac_width': 4, 'bert.encoder.layer.0.attention.self.key_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.0.attention.self.key_width': 32, 'bert.encoder.layer.0.attention.self.key_frac_width': 4, 'bert.encoder.layer.0.attention.self.value_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.attention.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.intermediate.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.attention.self.query_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.attention.self.key_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.attention.self.value_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.attention.output.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.attention.output.dense_width': 16, 'bert.encoder.layer.1.attention.output.dense_frac_width': 8, 'bert.encoder.layer.1.intermediate.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.pooler.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'classifier_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'classifier_width': 32, 'classifier_frac_width': 8}. Best is trial 1 with value: 0.85688.\n",
      "/vol/bitbucket/ug22/adls/.venv/lib/python3.11/site-packages/optuna/distributions.py:502: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'torch.nn.modules.linear.Linear'> which is of type type.\n",
      "  warnings.warn(message)\n",
      "/vol/bitbucket/ug22/adls/.venv/lib/python3.11/site-packages/optuna/distributions.py:502: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'chop.nn.quantized.modules.linear.LinearInteger'> which is of type type.\n",
      "  warnings.warn(message)\n",
      "/vol/bitbucket/ug22/adls/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:36, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.661300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.545800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.471600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.423000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.399000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.395600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:17]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-02-02 16:53:55,149] Trial 2 finished with value: 0.84528 and parameters: {'bert.encoder.layer.0.attention.self.query_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.0.attention.self.query_width': 32, 'bert.encoder.layer.0.attention.self.query_frac_width': 2, 'bert.encoder.layer.0.attention.self.key_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.0.attention.self.key_width': 16, 'bert.encoder.layer.0.attention.self.key_frac_width': 2, 'bert.encoder.layer.0.attention.self.value_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.0.attention.self.value_width': 8, 'bert.encoder.layer.0.attention.self.value_frac_width': 2, 'bert.encoder.layer.0.attention.output.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.0.attention.output.dense_width': 32, 'bert.encoder.layer.0.attention.output.dense_frac_width': 4, 'bert.encoder.layer.0.intermediate.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.0.intermediate.dense_width': 16, 'bert.encoder.layer.0.intermediate.dense_frac_width': 4, 'bert.encoder.layer.0.output.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.0.output.dense_width': 8, 'bert.encoder.layer.0.output.dense_frac_width': 8, 'bert.encoder.layer.1.attention.self.query_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.attention.self.query_width': 16, 'bert.encoder.layer.1.attention.self.query_frac_width': 8, 'bert.encoder.layer.1.attention.self.key_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.attention.self.value_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.attention.self.value_width': 32, 'bert.encoder.layer.1.attention.self.value_frac_width': 8, 'bert.encoder.layer.1.attention.output.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.attention.output.dense_width': 8, 'bert.encoder.layer.1.attention.output.dense_frac_width': 2, 'bert.encoder.layer.1.intermediate.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.pooler.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.pooler.dense_width': 16, 'bert.pooler.dense_frac_width': 4, 'classifier_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'classifier_width': 8, 'classifier_frac_width': 8}. Best is trial 1 with value: 0.85688.\n",
      "/vol/bitbucket/ug22/adls/.venv/lib/python3.11/site-packages/optuna/distributions.py:502: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'torch.nn.modules.linear.Linear'> which is of type type.\n",
      "  warnings.warn(message)\n",
      "/vol/bitbucket/ug22/adls/.venv/lib/python3.11/site-packages/optuna/distributions.py:502: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'chop.nn.quantized.modules.linear.LinearInteger'> which is of type type.\n",
      "  warnings.warn(message)\n",
      "/vol/bitbucket/ug22/adls/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:31, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.635700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.452700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.407200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.381600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.364300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.374900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:14]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-02-02 16:54:41,781] Trial 3 finished with value: 0.85852 and parameters: {'bert.encoder.layer.0.attention.self.query_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.attention.self.key_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.0.attention.self.key_width': 16, 'bert.encoder.layer.0.attention.self.key_frac_width': 8, 'bert.encoder.layer.0.attention.self.value_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.attention.output.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.0.attention.output.dense_width': 16, 'bert.encoder.layer.0.attention.output.dense_frac_width': 2, 'bert.encoder.layer.0.intermediate.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.attention.self.query_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.attention.self.query_width': 16, 'bert.encoder.layer.1.attention.self.query_frac_width': 2, 'bert.encoder.layer.1.attention.self.key_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.attention.self.value_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.attention.self.value_width': 8, 'bert.encoder.layer.1.attention.self.value_frac_width': 8, 'bert.encoder.layer.1.attention.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.intermediate.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.output.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.output.dense_width': 8, 'bert.encoder.layer.1.output.dense_frac_width': 8, 'bert.pooler.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'classifier_type': <class 'torch.nn.modules.linear.Linear'>}. Best is trial 3 with value: 0.85852.\n",
      "/vol/bitbucket/ug22/adls/.venv/lib/python3.11/site-packages/optuna/distributions.py:502: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'torch.nn.modules.linear.Linear'> which is of type type.\n",
      "  warnings.warn(message)\n",
      "/vol/bitbucket/ug22/adls/.venv/lib/python3.11/site-packages/optuna/distributions.py:502: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'chop.nn.quantized.modules.linear.LinearInteger'> which is of type type.\n",
      "  warnings.warn(message)\n",
      "/vol/bitbucket/ug22/adls/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:29, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.601200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.430500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.398900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.371200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.361500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.365400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:12]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-02-02 16:55:25,265] Trial 4 finished with value: 0.86148 and parameters: {'bert.encoder.layer.0.attention.self.query_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.attention.self.key_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.attention.self.value_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.attention.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.intermediate.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.attention.self.query_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.attention.self.query_width': 32, 'bert.encoder.layer.1.attention.self.query_frac_width': 2, 'bert.encoder.layer.1.attention.self.key_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.attention.self.value_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.attention.output.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.attention.output.dense_width': 32, 'bert.encoder.layer.1.attention.output.dense_frac_width': 2, 'bert.encoder.layer.1.intermediate.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.pooler.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'classifier_type': <class 'torch.nn.modules.linear.Linear'>}. Best is trial 4 with value: 0.86148.\n",
      "/vol/bitbucket/ug22/adls/.venv/lib/python3.11/site-packages/optuna/distributions.py:502: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'torch.nn.modules.linear.Linear'> which is of type type.\n",
      "  warnings.warn(message)\n",
      "/vol/bitbucket/ug22/adls/.venv/lib/python3.11/site-packages/optuna/distributions.py:502: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'chop.nn.quantized.modules.linear.LinearInteger'> which is of type type.\n",
      "  warnings.warn(message)\n",
      "/vol/bitbucket/ug22/adls/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:37, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:16]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-02-02 16:56:20,474] Trial 5 finished with value: 0.5 and parameters: {'bert.encoder.layer.0.attention.self.query_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.0.attention.self.query_width': 16, 'bert.encoder.layer.0.attention.self.query_frac_width': 8, 'bert.encoder.layer.0.attention.self.key_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.0.attention.self.key_width': 32, 'bert.encoder.layer.0.attention.self.key_frac_width': 4, 'bert.encoder.layer.0.attention.self.value_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.0.attention.self.value_width': 32, 'bert.encoder.layer.0.attention.self.value_frac_width': 8, 'bert.encoder.layer.0.attention.output.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.0.attention.output.dense_width': 16, 'bert.encoder.layer.0.attention.output.dense_frac_width': 8, 'bert.encoder.layer.0.intermediate.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.output.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.0.output.dense_width': 32, 'bert.encoder.layer.0.output.dense_frac_width': 2, 'bert.encoder.layer.1.attention.self.query_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.attention.self.key_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.attention.self.key_width': 32, 'bert.encoder.layer.1.attention.self.key_frac_width': 4, 'bert.encoder.layer.1.attention.self.value_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.attention.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.intermediate.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.output.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.output.dense_width': 16, 'bert.encoder.layer.1.output.dense_frac_width': 4, 'bert.pooler.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.pooler.dense_width': 32, 'bert.pooler.dense_frac_width': 2, 'classifier_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'classifier_width': 8, 'classifier_frac_width': 2}. Best is trial 4 with value: 0.86148.\n",
      "/vol/bitbucket/ug22/adls/.venv/lib/python3.11/site-packages/optuna/distributions.py:502: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'torch.nn.modules.linear.Linear'> which is of type type.\n",
      "  warnings.warn(message)\n",
      "/vol/bitbucket/ug22/adls/.venv/lib/python3.11/site-packages/optuna/distributions.py:502: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'chop.nn.quantized.modules.linear.LinearInteger'> which is of type type.\n",
      "  warnings.warn(message)\n",
      "/vol/bitbucket/ug22/adls/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:31, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:14]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-02-02 16:57:07,132] Trial 6 finished with value: 0.5 and parameters: {'bert.encoder.layer.0.attention.self.query_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.attention.self.key_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.attention.self.value_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.attention.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.intermediate.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.output.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.0.output.dense_width': 32, 'bert.encoder.layer.0.output.dense_frac_width': 2, 'bert.encoder.layer.1.attention.self.query_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.attention.self.query_width': 32, 'bert.encoder.layer.1.attention.self.query_frac_width': 4, 'bert.encoder.layer.1.attention.self.key_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.attention.self.key_width': 32, 'bert.encoder.layer.1.attention.self.key_frac_width': 4, 'bert.encoder.layer.1.attention.self.value_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.attention.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.intermediate.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.intermediate.dense_width': 16, 'bert.encoder.layer.1.intermediate.dense_frac_width': 8, 'bert.encoder.layer.1.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.pooler.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'classifier_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'classifier_width': 32, 'classifier_frac_width': 2}. Best is trial 4 with value: 0.86148.\n",
      "/vol/bitbucket/ug22/adls/.venv/lib/python3.11/site-packages/optuna/distributions.py:502: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'torch.nn.modules.linear.Linear'> which is of type type.\n",
      "  warnings.warn(message)\n",
      "/vol/bitbucket/ug22/adls/.venv/lib/python3.11/site-packages/optuna/distributions.py:502: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'chop.nn.quantized.modules.linear.LinearInteger'> which is of type type.\n",
      "  warnings.warn(message)\n",
      "/vol/bitbucket/ug22/adls/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:34, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.663800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.494100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.442300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.402500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.382900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.394700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:15]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-02-02 16:57:58,080] Trial 7 finished with value: 0.84768 and parameters: {'bert.encoder.layer.0.attention.self.query_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.0.attention.self.query_width': 8, 'bert.encoder.layer.0.attention.self.query_frac_width': 4, 'bert.encoder.layer.0.attention.self.key_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.0.attention.self.key_width': 16, 'bert.encoder.layer.0.attention.self.key_frac_width': 8, 'bert.encoder.layer.0.attention.self.value_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.0.attention.self.value_width': 32, 'bert.encoder.layer.0.attention.self.value_frac_width': 2, 'bert.encoder.layer.0.attention.output.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.0.attention.output.dense_width': 16, 'bert.encoder.layer.0.attention.output.dense_frac_width': 2, 'bert.encoder.layer.0.intermediate.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.0.intermediate.dense_width': 32, 'bert.encoder.layer.0.intermediate.dense_frac_width': 2, 'bert.encoder.layer.0.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.attention.self.query_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.attention.self.key_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.attention.self.value_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.attention.self.value_width': 8, 'bert.encoder.layer.1.attention.self.value_frac_width': 2, 'bert.encoder.layer.1.attention.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.intermediate.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.intermediate.dense_width': 32, 'bert.encoder.layer.1.intermediate.dense_frac_width': 2, 'bert.encoder.layer.1.output.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.output.dense_width': 32, 'bert.encoder.layer.1.output.dense_frac_width': 4, 'bert.pooler.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'classifier_type': <class 'torch.nn.modules.linear.Linear'>}. Best is trial 4 with value: 0.86148.\n",
      "/vol/bitbucket/ug22/adls/.venv/lib/python3.11/site-packages/optuna/distributions.py:502: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'torch.nn.modules.linear.Linear'> which is of type type.\n",
      "  warnings.warn(message)\n",
      "/vol/bitbucket/ug22/adls/.venv/lib/python3.11/site-packages/optuna/distributions.py:502: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'chop.nn.quantized.modules.linear.LinearInteger'> which is of type type.\n",
      "  warnings.warn(message)\n",
      "/vol/bitbucket/ug22/adls/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:32, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.620200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.460400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.418400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.381700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.355400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.364400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:14]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-02-02 16:58:46,092] Trial 8 finished with value: 0.85224 and parameters: {'bert.encoder.layer.0.attention.self.query_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.0.attention.self.query_width': 32, 'bert.encoder.layer.0.attention.self.query_frac_width': 4, 'bert.encoder.layer.0.attention.self.key_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.0.attention.self.key_width': 8, 'bert.encoder.layer.0.attention.self.key_frac_width': 2, 'bert.encoder.layer.0.attention.self.value_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.attention.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.intermediate.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.0.intermediate.dense_width': 8, 'bert.encoder.layer.0.intermediate.dense_frac_width': 4, 'bert.encoder.layer.0.output.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.0.output.dense_width': 8, 'bert.encoder.layer.0.output.dense_frac_width': 4, 'bert.encoder.layer.1.attention.self.query_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.attention.self.query_width': 32, 'bert.encoder.layer.1.attention.self.query_frac_width': 2, 'bert.encoder.layer.1.attention.self.key_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.attention.self.value_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.attention.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.intermediate.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.pooler.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.pooler.dense_width': 32, 'bert.pooler.dense_frac_width': 4, 'classifier_type': <class 'torch.nn.modules.linear.Linear'>}. Best is trial 4 with value: 0.86148.\n",
      "/vol/bitbucket/ug22/adls/.venv/lib/python3.11/site-packages/optuna/distributions.py:502: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'torch.nn.modules.linear.Linear'> which is of type type.\n",
      "  warnings.warn(message)\n",
      "/vol/bitbucket/ug22/adls/.venv/lib/python3.11/site-packages/optuna/distributions.py:502: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'chop.nn.quantized.modules.linear.LinearInteger'> which is of type type.\n",
      "  warnings.warn(message)\n",
      "/vol/bitbucket/ug22/adls/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:30, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.623600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.466800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.415800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.383000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.360600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.360700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:13]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-02-02 16:59:30,964] Trial 9 finished with value: 0.8582 and parameters: {'bert.encoder.layer.0.attention.self.query_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.0.attention.self.query_width': 16, 'bert.encoder.layer.0.attention.self.query_frac_width': 8, 'bert.encoder.layer.0.attention.self.key_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.attention.self.value_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.0.attention.self.value_width': 32, 'bert.encoder.layer.0.attention.self.value_frac_width': 4, 'bert.encoder.layer.0.attention.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.intermediate.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.attention.self.query_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.attention.self.query_width': 16, 'bert.encoder.layer.1.attention.self.query_frac_width': 8, 'bert.encoder.layer.1.attention.self.key_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.attention.self.value_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.attention.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.intermediate.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.pooler.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.pooler.dense_width': 32, 'bert.pooler.dense_frac_width': 4, 'classifier_type': <class 'torch.nn.modules.linear.Linear'>}. Best is trial 4 with value: 0.86148.\n",
      "/vol/bitbucket/ug22/adls/.venv/lib/python3.11/site-packages/optuna/distributions.py:502: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'torch.nn.modules.linear.Linear'> which is of type type.\n",
      "  warnings.warn(message)\n",
      "/vol/bitbucket/ug22/adls/.venv/lib/python3.11/site-packages/optuna/distributions.py:502: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'chop.nn.quantized.modules.linear.LinearInteger'> which is of type type.\n",
      "  warnings.warn(message)\n",
      "/vol/bitbucket/ug22/adls/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:31, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.611000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.423900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.401100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.369200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.357600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.365100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:14]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-02-02 17:00:18,063] Trial 10 finished with value: 0.85988 and parameters: {'bert.encoder.layer.0.attention.self.query_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.attention.self.key_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.attention.self.value_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.attention.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.intermediate.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.0.intermediate.dense_width': 8, 'bert.encoder.layer.0.intermediate.dense_frac_width': 8, 'bert.encoder.layer.0.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.attention.self.query_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.attention.self.key_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.attention.self.key_width': 8, 'bert.encoder.layer.1.attention.self.key_frac_width': 8, 'bert.encoder.layer.1.attention.self.value_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.attention.self.value_width': 16, 'bert.encoder.layer.1.attention.self.value_frac_width': 4, 'bert.encoder.layer.1.attention.output.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.attention.output.dense_width': 32, 'bert.encoder.layer.1.attention.output.dense_frac_width': 2, 'bert.encoder.layer.1.intermediate.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.intermediate.dense_width': 8, 'bert.encoder.layer.1.intermediate.dense_frac_width': 4, 'bert.encoder.layer.1.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.pooler.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'classifier_type': <class 'torch.nn.modules.linear.Linear'>}. Best is trial 4 with value: 0.86148.\n",
      "/vol/bitbucket/ug22/adls/.venv/lib/python3.11/site-packages/optuna/distributions.py:502: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'torch.nn.modules.linear.Linear'> which is of type type.\n",
      "  warnings.warn(message)\n",
      "/vol/bitbucket/ug22/adls/.venv/lib/python3.11/site-packages/optuna/distributions.py:502: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'chop.nn.quantized.modules.linear.LinearInteger'> which is of type type.\n",
      "  warnings.warn(message)\n",
      "/vol/bitbucket/ug22/adls/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:31, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.611000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.423900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.401100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.369200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.357600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.365100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:14]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-02-02 17:01:04,895] Trial 11 finished with value: 0.85988 and parameters: {'bert.encoder.layer.0.attention.self.query_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.attention.self.key_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.attention.self.value_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.attention.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.intermediate.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.0.intermediate.dense_width': 8, 'bert.encoder.layer.0.intermediate.dense_frac_width': 8, 'bert.encoder.layer.0.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.attention.self.query_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.attention.self.key_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.attention.self.key_width': 8, 'bert.encoder.layer.1.attention.self.key_frac_width': 8, 'bert.encoder.layer.1.attention.self.value_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.attention.self.value_width': 16, 'bert.encoder.layer.1.attention.self.value_frac_width': 4, 'bert.encoder.layer.1.attention.output.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.attention.output.dense_width': 32, 'bert.encoder.layer.1.attention.output.dense_frac_width': 2, 'bert.encoder.layer.1.intermediate.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.intermediate.dense_width': 8, 'bert.encoder.layer.1.intermediate.dense_frac_width': 4, 'bert.encoder.layer.1.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.pooler.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'classifier_type': <class 'torch.nn.modules.linear.Linear'>}. Best is trial 4 with value: 0.86148.\n",
      "/vol/bitbucket/ug22/adls/.venv/lib/python3.11/site-packages/optuna/distributions.py:502: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'torch.nn.modules.linear.Linear'> which is of type type.\n",
      "  warnings.warn(message)\n",
      "/vol/bitbucket/ug22/adls/.venv/lib/python3.11/site-packages/optuna/distributions.py:502: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'chop.nn.quantized.modules.linear.LinearInteger'> which is of type type.\n",
      "  warnings.warn(message)\n",
      "/vol/bitbucket/ug22/adls/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:31, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.610500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.423100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.404400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.371200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.359300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.363300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:14]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-02-02 17:01:51,802] Trial 12 finished with value: 0.85928 and parameters: {'bert.encoder.layer.0.attention.self.query_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.attention.self.key_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.attention.self.value_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.attention.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.intermediate.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.0.intermediate.dense_width': 8, 'bert.encoder.layer.0.intermediate.dense_frac_width': 8, 'bert.encoder.layer.0.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.attention.self.query_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.attention.self.key_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.attention.self.key_width': 8, 'bert.encoder.layer.1.attention.self.key_frac_width': 8, 'bert.encoder.layer.1.attention.self.value_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.attention.self.value_width': 16, 'bert.encoder.layer.1.attention.self.value_frac_width': 4, 'bert.encoder.layer.1.attention.output.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.attention.output.dense_width': 32, 'bert.encoder.layer.1.attention.output.dense_frac_width': 4, 'bert.encoder.layer.1.intermediate.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.intermediate.dense_width': 8, 'bert.encoder.layer.1.intermediate.dense_frac_width': 4, 'bert.encoder.layer.1.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.pooler.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'classifier_type': <class 'torch.nn.modules.linear.Linear'>}. Best is trial 4 with value: 0.86148.\n",
      "/vol/bitbucket/ug22/adls/.venv/lib/python3.11/site-packages/optuna/distributions.py:502: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'torch.nn.modules.linear.Linear'> which is of type type.\n",
      "  warnings.warn(message)\n",
      "/vol/bitbucket/ug22/adls/.venv/lib/python3.11/site-packages/optuna/distributions.py:502: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'chop.nn.quantized.modules.linear.LinearInteger'> which is of type type.\n",
      "  warnings.warn(message)\n",
      "/vol/bitbucket/ug22/adls/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:31, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.604900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.430600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.405000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.369300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.362100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.371800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:14]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-02-02 17:02:38,446] Trial 13 finished with value: 0.85852 and parameters: {'bert.encoder.layer.0.attention.self.query_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.attention.self.key_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.attention.self.value_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.attention.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.intermediate.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.0.intermediate.dense_width': 16, 'bert.encoder.layer.0.intermediate.dense_frac_width': 8, 'bert.encoder.layer.0.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.attention.self.query_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.attention.self.key_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.attention.self.key_width': 8, 'bert.encoder.layer.1.attention.self.key_frac_width': 8, 'bert.encoder.layer.1.attention.self.value_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.attention.self.value_width': 16, 'bert.encoder.layer.1.attention.self.value_frac_width': 4, 'bert.encoder.layer.1.attention.output.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.attention.output.dense_width': 32, 'bert.encoder.layer.1.attention.output.dense_frac_width': 2, 'bert.encoder.layer.1.intermediate.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.intermediate.dense_width': 8, 'bert.encoder.layer.1.intermediate.dense_frac_width': 4, 'bert.encoder.layer.1.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.pooler.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'classifier_type': <class 'torch.nn.modules.linear.Linear'>}. Best is trial 4 with value: 0.86148.\n",
      "/vol/bitbucket/ug22/adls/.venv/lib/python3.11/site-packages/optuna/distributions.py:502: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'torch.nn.modules.linear.Linear'> which is of type type.\n",
      "  warnings.warn(message)\n",
      "/vol/bitbucket/ug22/adls/.venv/lib/python3.11/site-packages/optuna/distributions.py:502: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'chop.nn.quantized.modules.linear.LinearInteger'> which is of type type.\n",
      "  warnings.warn(message)\n",
      "/vol/bitbucket/ug22/adls/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:30, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.600300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.431100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.404200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.371700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.363500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.364200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:13]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-02-02 17:03:22,831] Trial 14 finished with value: 0.86076 and parameters: {'bert.encoder.layer.0.attention.self.query_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.attention.self.key_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.attention.self.value_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.attention.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.intermediate.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.attention.self.query_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.attention.self.query_width': 32, 'bert.encoder.layer.1.attention.self.query_frac_width': 2, 'bert.encoder.layer.1.attention.self.key_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.attention.self.value_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.attention.output.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.attention.output.dense_width': 32, 'bert.encoder.layer.1.attention.output.dense_frac_width': 2, 'bert.encoder.layer.1.intermediate.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.intermediate.dense_width': 32, 'bert.encoder.layer.1.intermediate.dense_frac_width': 8, 'bert.encoder.layer.1.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.pooler.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'classifier_type': <class 'torch.nn.modules.linear.Linear'>}. Best is trial 4 with value: 0.86148.\n",
      "/vol/bitbucket/ug22/adls/.venv/lib/python3.11/site-packages/optuna/distributions.py:502: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'torch.nn.modules.linear.Linear'> which is of type type.\n",
      "  warnings.warn(message)\n",
      "/vol/bitbucket/ug22/adls/.venv/lib/python3.11/site-packages/optuna/distributions.py:502: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'chop.nn.quantized.modules.linear.LinearInteger'> which is of type type.\n",
      "  warnings.warn(message)\n",
      "/vol/bitbucket/ug22/adls/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:29, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.600300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.431100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.404200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.371700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.363500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.364200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:13]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-02-02 17:04:06,538] Trial 15 finished with value: 0.86076 and parameters: {'bert.encoder.layer.0.attention.self.query_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.attention.self.key_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.attention.self.value_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.attention.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.intermediate.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.attention.self.query_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.attention.self.query_width': 32, 'bert.encoder.layer.1.attention.self.query_frac_width': 2, 'bert.encoder.layer.1.attention.self.key_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.attention.self.value_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.attention.output.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.attention.output.dense_width': 32, 'bert.encoder.layer.1.attention.output.dense_frac_width': 2, 'bert.encoder.layer.1.intermediate.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.intermediate.dense_width': 32, 'bert.encoder.layer.1.intermediate.dense_frac_width': 8, 'bert.encoder.layer.1.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.pooler.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'classifier_type': <class 'torch.nn.modules.linear.Linear'>}. Best is trial 4 with value: 0.86148.\n",
      "/vol/bitbucket/ug22/adls/.venv/lib/python3.11/site-packages/optuna/distributions.py:502: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'torch.nn.modules.linear.Linear'> which is of type type.\n",
      "  warnings.warn(message)\n",
      "/vol/bitbucket/ug22/adls/.venv/lib/python3.11/site-packages/optuna/distributions.py:502: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'chop.nn.quantized.modules.linear.LinearInteger'> which is of type type.\n",
      "  warnings.warn(message)\n",
      "/vol/bitbucket/ug22/adls/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:29, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.600600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.433700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.406300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.376100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.367500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.363900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:12]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-02-02 17:04:49,048] Trial 16 finished with value: 0.85832 and parameters: {'bert.encoder.layer.0.attention.self.query_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.attention.self.key_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.attention.self.value_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.attention.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.intermediate.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.attention.self.query_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.attention.self.query_width': 32, 'bert.encoder.layer.1.attention.self.query_frac_width': 2, 'bert.encoder.layer.1.attention.self.key_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.attention.self.value_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.attention.output.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.attention.output.dense_width': 8, 'bert.encoder.layer.1.attention.output.dense_frac_width': 8, 'bert.encoder.layer.1.intermediate.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.pooler.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'classifier_type': <class 'torch.nn.modules.linear.Linear'>}. Best is trial 4 with value: 0.86148.\n",
      "/vol/bitbucket/ug22/adls/.venv/lib/python3.11/site-packages/optuna/distributions.py:502: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'torch.nn.modules.linear.Linear'> which is of type type.\n",
      "  warnings.warn(message)\n",
      "/vol/bitbucket/ug22/adls/.venv/lib/python3.11/site-packages/optuna/distributions.py:502: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'chop.nn.quantized.modules.linear.LinearInteger'> which is of type type.\n",
      "  warnings.warn(message)\n",
      "/vol/bitbucket/ug22/adls/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:29, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.597900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.428800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.402900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.375300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.363200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.358700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:13]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-02-02 17:05:32,815] Trial 17 finished with value: 0.8604 and parameters: {'bert.encoder.layer.0.attention.self.query_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.attention.self.key_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.attention.self.value_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.attention.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.intermediate.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.attention.self.query_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.attention.self.query_width': 32, 'bert.encoder.layer.1.attention.self.query_frac_width': 2, 'bert.encoder.layer.1.attention.self.key_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.attention.self.value_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.attention.output.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.attention.output.dense_width': 16, 'bert.encoder.layer.1.attention.output.dense_frac_width': 4, 'bert.encoder.layer.1.intermediate.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.intermediate.dense_width': 32, 'bert.encoder.layer.1.intermediate.dense_frac_width': 8, 'bert.encoder.layer.1.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.pooler.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'classifier_type': <class 'torch.nn.modules.linear.Linear'>}. Best is trial 4 with value: 0.86148.\n",
      "/vol/bitbucket/ug22/adls/.venv/lib/python3.11/site-packages/optuna/distributions.py:502: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'torch.nn.modules.linear.Linear'> which is of type type.\n",
      "  warnings.warn(message)\n",
      "/vol/bitbucket/ug22/adls/.venv/lib/python3.11/site-packages/optuna/distributions.py:502: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'chop.nn.quantized.modules.linear.LinearInteger'> which is of type type.\n",
      "  warnings.warn(message)\n",
      "/vol/bitbucket/ug22/adls/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:31, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.638000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.476600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.426200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.388800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.377300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.385400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:14]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-02-02 17:06:19,934] Trial 18 finished with value: 0.8546 and parameters: {'bert.encoder.layer.0.attention.self.query_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.attention.self.key_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.attention.self.value_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.attention.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.intermediate.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.attention.self.query_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.attention.self.query_width': 8, 'bert.encoder.layer.1.attention.self.query_frac_width': 2, 'bert.encoder.layer.1.attention.self.key_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.attention.self.value_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.attention.output.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.attention.output.dense_width': 32, 'bert.encoder.layer.1.attention.output.dense_frac_width': 2, 'bert.encoder.layer.1.intermediate.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.output.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.output.dense_width': 16, 'bert.encoder.layer.1.output.dense_frac_width': 2, 'bert.pooler.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.pooler.dense_width': 8, 'bert.pooler.dense_frac_width': 8, 'classifier_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'classifier_width': 16, 'classifier_frac_width': 4}. Best is trial 4 with value: 0.86148.\n",
      "/vol/bitbucket/ug22/adls/.venv/lib/python3.11/site-packages/optuna/distributions.py:502: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'torch.nn.modules.linear.Linear'> which is of type type.\n",
      "  warnings.warn(message)\n",
      "/vol/bitbucket/ug22/adls/.venv/lib/python3.11/site-packages/optuna/distributions.py:502: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'chop.nn.quantized.modules.linear.LinearInteger'> which is of type type.\n",
      "  warnings.warn(message)\n",
      "/vol/bitbucket/ug22/adls/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:29, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.609600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.434400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.402900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.368900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.359300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.359100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:13]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-02-02 17:07:03,807] Trial 19 finished with value: 0.85876 and parameters: {'bert.encoder.layer.0.attention.self.query_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.attention.self.key_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.attention.self.value_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.attention.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.intermediate.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.attention.self.query_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.attention.self.query_width': 32, 'bert.encoder.layer.1.attention.self.query_frac_width': 2, 'bert.encoder.layer.1.attention.self.key_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.attention.self.value_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.attention.output.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.attention.output.dense_width': 32, 'bert.encoder.layer.1.attention.output.dense_frac_width': 2, 'bert.encoder.layer.1.intermediate.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.intermediate.dense_width': 16, 'bert.encoder.layer.1.intermediate.dense_frac_width': 2, 'bert.encoder.layer.1.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.pooler.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'classifier_type': <class 'torch.nn.modules.linear.Linear'>}. Best is trial 4 with value: 0.86148.\n"
     ]
    }
   ],
   "source": [
    "# import optuna\n",
    "\n",
    "# study = optuna.create_study(\n",
    "#     direction=\"maximize\",\n",
    "#     study_name=\"bert-tiny-nas-study\",\n",
    "#     sampler=sampler,\n",
    "# )\n",
    "\n",
    "# study.optimize(\n",
    "#     objective,\n",
    "#     n_trials=1,\n",
    "#     timeout=60 * 60 * 24,\n",
    "# )\n",
    "\n",
    "import optuna\n",
    "\n",
    "print(search_space)\n",
    "\n",
    "study = optuna.create_study(\n",
    "    direction=\"maximize\",\n",
    "    study_name=\"bert-tiny-mixed-precision-study\",\n",
    "    sampler=sampler,\n",
    ")\n",
    "\n",
    "study.optimize(\n",
    "    objective,\n",
    "    n_trials=20,\n",
    "    timeout=60 * 60 * 24,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'linear_layer_choices': [<class 'torch.nn.modules.linear.Linear'>, <class 'chop.nn.quantized.modules.linear.LinearInteger'>], 'width_choices': [8, 16, 32], 'frac_width_choices': [2, 4, 8]}\n"
     ]
    }
   ],
   "source": [
    "print(search_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract accuracy from each trial in order\n",
    "trial_accuracies = [trial.value for trial in study.trials]\n",
    "\n",
    "# Compute best accuracy up to each trial\n",
    "best_so_far = []\n",
    "current_best = 0\n",
    "for acc in trial_accuracies:\n",
    "    if acc is not None and acc > current_best:\n",
    "        current_best = acc\n",
    "    best_so_far.append(current_best)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(best_so_far) + 1), best_so_far, marker='o')\n",
    "plt.xlabel(\"Trial Number\")\n",
    "plt.ylabel(\"Best Accuracy So Far\")\n",
    "plt.title(\"Mixed Precision Quantization Search Progress\")\n",
    "plt.grid(True)\n",
    "plt.savefig(\"task1_search_progress.png\", dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best accuracy: 0.86148\n",
      "Best trial number: 4\n",
      "\n",
      "Best parameters:\n",
      "  bert.encoder.layer.0.attention.self.query_type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  bert.encoder.layer.0.attention.self.key_type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  bert.encoder.layer.0.attention.self.value_type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  bert.encoder.layer.0.attention.output.dense_type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  bert.encoder.layer.0.intermediate.dense_type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  bert.encoder.layer.0.output.dense_type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  bert.encoder.layer.1.attention.self.query_type: <class 'chop.nn.quantized.modules.linear.LinearInteger'>\n",
      "  bert.encoder.layer.1.attention.self.query_width: 32\n",
      "  bert.encoder.layer.1.attention.self.query_frac_width: 2\n",
      "  bert.encoder.layer.1.attention.self.key_type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  bert.encoder.layer.1.attention.self.value_type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  bert.encoder.layer.1.attention.output.dense_type: <class 'chop.nn.quantized.modules.linear.LinearInteger'>\n",
      "  bert.encoder.layer.1.attention.output.dense_width: 32\n",
      "  bert.encoder.layer.1.attention.output.dense_frac_width: 2\n",
      "  bert.encoder.layer.1.intermediate.dense_type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  bert.encoder.layer.1.output.dense_type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  bert.pooler.dense_type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  classifier_type: <class 'torch.nn.modules.linear.Linear'>\n"
     ]
    }
   ],
   "source": [
    "# Best trial overall\n",
    "best_trial = study.best_trial\n",
    "\n",
    "print(f\"Best accuracy: {best_trial.value}\")\n",
    "print(f\"Best trial number: {best_trial.number}\")\n",
    "print(f\"\\nBest parameters:\")\n",
    "for key, value in best_trial.params.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    trial  accuracy  n_quantized  n_full_precision\n",
      "4       4   0.86148            2                12\n",
      "15     15   0.86076            3                11\n",
      "14     14   0.86076            3                11\n",
      "17     17   0.86040            3                11\n",
      "10     10   0.85988            5                 9\n",
      "11     11   0.85988            5                 9\n",
      "12     12   0.85928            5                 9\n",
      "19     19   0.85876            3                11\n",
      "13     13   0.85852            5                 9\n",
      "3       3   0.85852            5                 9\n",
      "16     16   0.85832            2                12\n",
      "9       9   0.85820            4                10\n",
      "1       1   0.85688            4                10\n",
      "18     18   0.85460            5                 9\n",
      "8       8   0.85224            6                 8\n",
      "0       0   0.84796            7                 7\n",
      "7       7   0.84768            8                 6\n",
      "2       2   0.84528           11                 3\n",
      "5       5   0.50000            9                 5\n",
      "6       6   0.50000            5                 9\n"
     ]
    }
   ],
   "source": [
    "# Summary of all trials\n",
    "import pandas as pd\n",
    "\n",
    "trials_data = []\n",
    "for trial in study.trials:\n",
    "    trial_info = {\n",
    "        \"trial\": trial.number,\n",
    "        \"accuracy\": trial.value,\n",
    "    }\n",
    "    # Count how many layers were quantized vs kept as Linear\n",
    "    n_quantized = sum(1 for k, v in trial.params.items() if \"_type\" in k and \"LinearInteger\" in str(v))\n",
    "    n_linear = sum(1 for k, v in trial.params.items() if \"_type\" in k and v == torch.nn.Linear)\n",
    "    trial_info[\"n_quantized\"] = n_quantized\n",
    "    trial_info[\"n_full_precision\"] = n_linear\n",
    "    trials_data.append(trial_info)\n",
    "\n",
    "df = pd.DataFrame(trials_data)\n",
    "print(df.sort_values(\"accuracy\", ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best accuracy: 0.86148\n",
      "Best trial number: 4\n",
      "\n",
      "Best parameters:\n",
      "  bert.encoder.layer.0.attention.self.query_type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  bert.encoder.layer.0.attention.self.key_type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  bert.encoder.layer.0.attention.self.value_type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  bert.encoder.layer.0.attention.output.dense_type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  bert.encoder.layer.0.intermediate.dense_type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  bert.encoder.layer.0.output.dense_type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  bert.encoder.layer.1.attention.self.query_type: <class 'chop.nn.quantized.modules.linear.LinearInteger'>\n",
      "  bert.encoder.layer.1.attention.self.query_width: 32\n",
      "  bert.encoder.layer.1.attention.self.query_frac_width: 2\n",
      "  bert.encoder.layer.1.attention.self.key_type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  bert.encoder.layer.1.attention.self.value_type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  bert.encoder.layer.1.attention.output.dense_type: <class 'chop.nn.quantized.modules.linear.LinearInteger'>\n",
      "  bert.encoder.layer.1.attention.output.dense_width: 32\n",
      "  bert.encoder.layer.1.attention.output.dense_frac_width: 2\n",
      "  bert.encoder.layer.1.intermediate.dense_type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  bert.encoder.layer.1.output.dense_type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  bert.pooler.dense_type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  classifier_type: <class 'torch.nn.modules.linear.Linear'>\n"
     ]
    }
   ],
   "source": [
    "# Best trial overall\n",
    "best_trial = study.best_trial\n",
    "\n",
    "print(f\"Best accuracy: {best_trial.value}\")\n",
    "print(f\"Best trial number: {best_trial.number}\")\n",
    "print(f\"\\nBest parameters:\")\n",
    "for key, value in best_trial.params.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from chop.nn.quantized.modules.linear import (\n",
    "    LinearInteger,\n",
    "    LinearMinifloatDenorm,\n",
    "    LinearMinifloatIEEE,\n",
    "    LinearLog,\n",
    ")\n",
    "from chop.tools.utils import deepsetattr\n",
    "from chop.tools import get_trainer\n",
    "from copy import deepcopy\n",
    "from optuna.samplers import TPESampler\n",
    "import optuna\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "N_TRIALS = 50\n",
    "OUTPUT_DIR = Path(\"task2_results\")\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "LAYER_TYPES = {\n",
    "    \"Linear\": torch.nn.Linear,\n",
    "    \"Integer\": LinearInteger,\n",
    "    \"MinifloatDenorm\": LinearMinifloatDenorm,\n",
    "    \"MinifloatIEEE\": LinearMinifloatIEEE,\n",
    "    \"Log\": LinearLog,\n",
    "}\n",
    "\n",
    "WIDTH_CHOICES = [8, 16, 32]\n",
    "FRAC_WIDTH_CHOICES = [2, 4, 8]\n",
    "EXPONENT_WIDTH_CHOICES = [2, 4, 8]\n",
    "EXPONENT_BIAS_CHOICES = [3, 7, 15]\n",
    "\n",
    "def build_config(trial, layer_name, precision_type):\n",
    "    if precision_type == \"Linear\":\n",
    "        return None\n",
    "    \n",
    "    elif precision_type == \"Integer\":\n",
    "        width = trial.suggest_categorical(f\"{layer_name}_width\", WIDTH_CHOICES)\n",
    "        frac_width = trial.suggest_categorical(f\"{layer_name}_frac_width\", FRAC_WIDTH_CHOICES)\n",
    "        return {\n",
    "            \"data_in_width\": width, \"data_in_frac_width\": frac_width,\n",
    "            \"weight_width\": width, \"weight_frac_width\": frac_width,\n",
    "            \"bias_width\": width, \"bias_frac_width\": frac_width,\n",
    "        }\n",
    "    \n",
    "    elif precision_type in [\"MinifloatDenorm\", \"MinifloatIEEE\"]:\n",
    "        width = trial.suggest_categorical(f\"{layer_name}_width\", WIDTH_CHOICES)\n",
    "        exp_width = trial.suggest_categorical(f\"{layer_name}_exp_width\", EXPONENT_WIDTH_CHOICES)\n",
    "        exp_bias = trial.suggest_categorical(f\"{layer_name}_exp_bias\", EXPONENT_BIAS_CHOICES)\n",
    "        return {\n",
    "            \"data_in_width\": width, \"data_in_exponent_width\": exp_width, \"data_in_exponent_bias\": exp_bias,\n",
    "            \"weight_width\": width, \"weight_exponent_width\": exp_width, \"weight_exponent_bias\": exp_bias,\n",
    "            \"bias_width\": width, \"bias_exponent_width\": exp_width, \"bias_exponent_bias\": exp_bias,\n",
    "        }\n",
    "    \n",
    "    elif precision_type == \"Log\":\n",
    "        width = trial.suggest_categorical(f\"{layer_name}_width\", WIDTH_CHOICES)\n",
    "        exp_bias = trial.suggest_categorical(f\"{layer_name}_exp_bias\", EXPONENT_BIAS_CHOICES)\n",
    "        return {\n",
    "            \"data_in_width\": width, \"data_in_exponent_bias\": exp_bias,\n",
    "            \"weight_width\": width, \"weight_exponent_bias\": exp_bias,\n",
    "            \"bias_width\": width, \"bias_exponent_bias\": exp_bias,\n",
    "        }\n",
    "\n",
    "def construct_model_task2(trial):\n",
    "    trial_model = deepcopy(base_model)\n",
    "    \n",
    "    for name, layer in trial_model.named_modules():\n",
    "        if isinstance(layer, torch.nn.Linear):\n",
    "            # Optuna picks layer type\n",
    "            layer_type_name = trial.suggest_categorical(\n",
    "                f\"{name}_type\", \n",
    "                list(LAYER_TYPES.keys())\n",
    "            )\n",
    "            \n",
    "            if layer_type_name == \"Linear\":\n",
    "                continue\n",
    "            \n",
    "            config = build_config(trial, name, layer_type_name)\n",
    "            layer_cls = LAYER_TYPES[layer_type_name]\n",
    "            \n",
    "            new_layer = layer_cls(\n",
    "                in_features=layer.in_features,\n",
    "                out_features=layer.out_features,\n",
    "                config=config\n",
    "            )\n",
    "            new_layer.weight.data = layer.weight.data\n",
    "            if layer.bias is not None and new_layer.bias is not None:\n",
    "                new_layer.bias.data = layer.bias.data\n",
    "            \n",
    "            deepsetattr(trial_model, name, new_layer)\n",
    "    \n",
    "    return trial_model\n",
    "\n",
    "def objective_task2(trial):\n",
    "    model = construct_model_task2(trial)\n",
    "    \n",
    "    trainer = get_trainer(\n",
    "        model=model,\n",
    "        tokenized_dataset=dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        evaluate_metric=\"accuracy\",\n",
    "        num_train_epochs=1,\n",
    "    )\n",
    "    \n",
    "    trainer.train()\n",
    "    eval_results = trainer.evaluate()\n",
    "    accuracy = eval_results[\"eval_accuracy\"]\n",
    "    print(f\"Trial {trial.number}: accuracy = {accuracy:.4f}\")\n",
    "    return accuracy\n",
    "\n",
    "def save_callback(study, trial):\n",
    "    with open(OUTPUT_DIR / \"current_best.json\", \"w\") as f:\n",
    "        json.dump({\n",
    "            \"trial_number\": study.best_trial.number,\n",
    "            \"accuracy\": study.best_trial.value,\n",
    "        }, f, indent=2)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(f\"Starting Task 2 search with {N_TRIALS} trials...\")\n",
    "print(f\"Layer types: {list(LAYER_TYPES.keys())}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "study = optuna.create_study(\n",
    "    direction=\"maximize\",\n",
    "    study_name=\"bert-multi-precision-search\",\n",
    "    sampler=TPESampler(),\n",
    ")\n",
    "\n",
    "start_time = datetime.now()\n",
    "\n",
    "study.optimize(\n",
    "    objective_task2, \n",
    "    n_trials=N_TRIALS, \n",
    "    timeout=3600 * 8,\n",
    "    callbacks=[save_callback],\n",
    ")\n",
    "\n",
    "end_time = datetime.now()\n",
    "duration = end_time - start_time\n",
    "\n",
    "print(\"Saving results...\")\n",
    "\n",
    "with open(OUTPUT_DIR / \"study.pkl\", \"wb\") as f:\n",
    "    pickle.dump(study, f)\n",
    "\n",
    "with open(OUTPUT_DIR / \"best_config.json\", \"w\") as f:\n",
    "    json.dump({\n",
    "        \"trial_number\": study.best_trial.number,\n",
    "        \"accuracy\": study.best_trial.value,\n",
    "        \"params\": {k: str(v) for k, v in study.best_trial.params.items()},\n",
    "        \"duration_seconds\": duration.total_seconds(),\n",
    "    }, f, indent=2)\n",
    "\n",
    "accuracies = [t.value if t.value is not None else 0 for t in study.trials]\n",
    "best_so_far = []\n",
    "current_best = 0\n",
    "for acc in accuracies:\n",
    "    if acc is not None and acc > current_best:\n",
    "        current_best = acc\n",
    "    best_so_far.append(current_best)\n",
    "\n",
    "plt.figure(figsize=(12, 7))\n",
    "plt.plot(range(1, len(best_so_far) + 1), best_so_far, marker='o', linewidth=2, markersize=4)\n",
    "plt.xlabel(\"Trial Number\", fontsize=12)\n",
    "plt.ylabel(\"Best Accuracy So Far\", fontsize=12)\n",
    "plt.title(\"Task 2: Multi-Precision Search - Best Accuracy Over Trials\", fontsize=14)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / \"best_accuracy_over_trials.png\", dpi=150)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-02-04 14:13:07,646] A new study created in memory with name: bert-Integer-search\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TASK 2: Separate Search Per Precision Type\n",
      "Trials per precision: 20\n",
      "Precision types: ['Integer', 'MinifloatDenorm', 'MinifloatIEEE', 'Log']\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Searching: Integer\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vol/bitbucket/ug22/adls/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:31, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.651000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.494000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.442900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.410400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.385800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.392700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:14]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-02-04 14:13:55,322] Trial 0 finished with value: 0.85216 and parameters: {'bert.encoder.layer.0.attention.self.query_quantize': False, 'bert.encoder.layer.0.attention.self.key_quantize': False, 'bert.encoder.layer.0.attention.self.value_quantize': False, 'bert.encoder.layer.0.attention.output.dense_quantize': False, 'bert.encoder.layer.0.intermediate.dense_quantize': True, 'bert.encoder.layer.0.intermediate.dense_width': 8, 'bert.encoder.layer.0.intermediate.dense_frac_width': 2, 'bert.encoder.layer.0.output.dense_quantize': True, 'bert.encoder.layer.0.output.dense_width': 32, 'bert.encoder.layer.0.output.dense_frac_width': 8, 'bert.encoder.layer.1.attention.self.query_quantize': False, 'bert.encoder.layer.1.attention.self.key_quantize': True, 'bert.encoder.layer.1.attention.self.key_width': 16, 'bert.encoder.layer.1.attention.self.key_frac_width': 8, 'bert.encoder.layer.1.attention.self.value_quantize': True, 'bert.encoder.layer.1.attention.self.value_width': 8, 'bert.encoder.layer.1.attention.self.value_frac_width': 2, 'bert.encoder.layer.1.attention.output.dense_quantize': True, 'bert.encoder.layer.1.attention.output.dense_width': 16, 'bert.encoder.layer.1.attention.output.dense_frac_width': 4, 'bert.encoder.layer.1.intermediate.dense_quantize': False, 'bert.encoder.layer.1.output.dense_quantize': False, 'bert.pooler.dense_quantize': False, 'classifier_quantize': True, 'classifier_width': 32, 'classifier_frac_width': 4}. Best is trial 0 with value: 0.85216.\n",
      "/vol/bitbucket/ug22/adls/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:33, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:15]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-02-04 14:14:45,469] Trial 1 finished with value: 0.5 and parameters: {'bert.encoder.layer.0.attention.self.query_quantize': False, 'bert.encoder.layer.0.attention.self.key_quantize': True, 'bert.encoder.layer.0.attention.self.key_width': 32, 'bert.encoder.layer.0.attention.self.key_frac_width': 4, 'bert.encoder.layer.0.attention.self.value_quantize': False, 'bert.encoder.layer.0.attention.output.dense_quantize': False, 'bert.encoder.layer.0.intermediate.dense_quantize': True, 'bert.encoder.layer.0.intermediate.dense_width': 32, 'bert.encoder.layer.0.intermediate.dense_frac_width': 8, 'bert.encoder.layer.0.output.dense_quantize': True, 'bert.encoder.layer.0.output.dense_width': 32, 'bert.encoder.layer.0.output.dense_frac_width': 4, 'bert.encoder.layer.1.attention.self.query_quantize': False, 'bert.encoder.layer.1.attention.self.key_quantize': False, 'bert.encoder.layer.1.attention.self.value_quantize': True, 'bert.encoder.layer.1.attention.self.value_width': 8, 'bert.encoder.layer.1.attention.self.value_frac_width': 8, 'bert.encoder.layer.1.attention.output.dense_quantize': False, 'bert.encoder.layer.1.intermediate.dense_quantize': True, 'bert.encoder.layer.1.intermediate.dense_width': 32, 'bert.encoder.layer.1.intermediate.dense_frac_width': 8, 'bert.encoder.layer.1.output.dense_quantize': True, 'bert.encoder.layer.1.output.dense_width': 8, 'bert.encoder.layer.1.output.dense_frac_width': 8, 'bert.pooler.dense_quantize': True, 'bert.pooler.dense_width': 8, 'bert.pooler.dense_frac_width': 8, 'classifier_quantize': True, 'classifier_width': 8, 'classifier_frac_width': 2}. Best is trial 0 with value: 0.85216.\n",
      "/vol/bitbucket/ug22/adls/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:31, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.626700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.455300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.410100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.388800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.365500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.372100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:14]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-02-04 14:15:33,052] Trial 2 finished with value: 0.85544 and parameters: {'bert.encoder.layer.0.attention.self.query_quantize': False, 'bert.encoder.layer.0.attention.self.key_quantize': True, 'bert.encoder.layer.0.attention.self.key_width': 8, 'bert.encoder.layer.0.attention.self.key_frac_width': 4, 'bert.encoder.layer.0.attention.self.value_quantize': False, 'bert.encoder.layer.0.attention.output.dense_quantize': False, 'bert.encoder.layer.0.intermediate.dense_quantize': True, 'bert.encoder.layer.0.intermediate.dense_width': 8, 'bert.encoder.layer.0.intermediate.dense_frac_width': 4, 'bert.encoder.layer.0.output.dense_quantize': True, 'bert.encoder.layer.0.output.dense_width': 32, 'bert.encoder.layer.0.output.dense_frac_width': 4, 'bert.encoder.layer.1.attention.self.query_quantize': False, 'bert.encoder.layer.1.attention.self.key_quantize': True, 'bert.encoder.layer.1.attention.self.key_width': 16, 'bert.encoder.layer.1.attention.self.key_frac_width': 2, 'bert.encoder.layer.1.attention.self.value_quantize': False, 'bert.encoder.layer.1.attention.output.dense_quantize': False, 'bert.encoder.layer.1.intermediate.dense_quantize': False, 'bert.encoder.layer.1.output.dense_quantize': True, 'bert.encoder.layer.1.output.dense_width': 32, 'bert.encoder.layer.1.output.dense_frac_width': 8, 'bert.pooler.dense_quantize': True, 'bert.pooler.dense_width': 32, 'bert.pooler.dense_frac_width': 2, 'classifier_quantize': False}. Best is trial 2 with value: 0.85544.\n",
      "/vol/bitbucket/ug22/adls/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:35, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.647000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.515800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.439300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.393600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.372800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.386300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:17]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-02-04 14:16:26,764] Trial 3 finished with value: 0.84896 and parameters: {'bert.encoder.layer.0.attention.self.query_quantize': True, 'bert.encoder.layer.0.attention.self.query_width': 8, 'bert.encoder.layer.0.attention.self.query_frac_width': 8, 'bert.encoder.layer.0.attention.self.key_quantize': True, 'bert.encoder.layer.0.attention.self.key_width': 8, 'bert.encoder.layer.0.attention.self.key_frac_width': 4, 'bert.encoder.layer.0.attention.self.value_quantize': True, 'bert.encoder.layer.0.attention.self.value_width': 8, 'bert.encoder.layer.0.attention.self.value_frac_width': 2, 'bert.encoder.layer.0.attention.output.dense_quantize': False, 'bert.encoder.layer.0.intermediate.dense_quantize': True, 'bert.encoder.layer.0.intermediate.dense_width': 8, 'bert.encoder.layer.0.intermediate.dense_frac_width': 8, 'bert.encoder.layer.0.output.dense_quantize': True, 'bert.encoder.layer.0.output.dense_width': 16, 'bert.encoder.layer.0.output.dense_frac_width': 4, 'bert.encoder.layer.1.attention.self.query_quantize': True, 'bert.encoder.layer.1.attention.self.query_width': 16, 'bert.encoder.layer.1.attention.self.query_frac_width': 4, 'bert.encoder.layer.1.attention.self.key_quantize': True, 'bert.encoder.layer.1.attention.self.key_width': 16, 'bert.encoder.layer.1.attention.self.key_frac_width': 8, 'bert.encoder.layer.1.attention.self.value_quantize': False, 'bert.encoder.layer.1.attention.output.dense_quantize': True, 'bert.encoder.layer.1.attention.output.dense_width': 8, 'bert.encoder.layer.1.attention.output.dense_frac_width': 8, 'bert.encoder.layer.1.intermediate.dense_quantize': True, 'bert.encoder.layer.1.intermediate.dense_width': 16, 'bert.encoder.layer.1.intermediate.dense_frac_width': 8, 'bert.encoder.layer.1.output.dense_quantize': False, 'bert.pooler.dense_quantize': True, 'bert.pooler.dense_width': 32, 'bert.pooler.dense_frac_width': 4, 'classifier_quantize': False}. Best is trial 2 with value: 0.85544.\n",
      "/vol/bitbucket/ug22/adls/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:33, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.648800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.493100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.446800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.411800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.381500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.400500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:15]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-02-04 14:17:16,927] Trial 4 finished with value: 0.85136 and parameters: {'bert.encoder.layer.0.attention.self.query_quantize': True, 'bert.encoder.layer.0.attention.self.query_width': 16, 'bert.encoder.layer.0.attention.self.query_frac_width': 4, 'bert.encoder.layer.0.attention.self.key_quantize': False, 'bert.encoder.layer.0.attention.self.value_quantize': True, 'bert.encoder.layer.0.attention.self.value_width': 32, 'bert.encoder.layer.0.attention.self.value_frac_width': 2, 'bert.encoder.layer.0.attention.output.dense_quantize': False, 'bert.encoder.layer.0.intermediate.dense_quantize': True, 'bert.encoder.layer.0.intermediate.dense_width': 8, 'bert.encoder.layer.0.intermediate.dense_frac_width': 8, 'bert.encoder.layer.0.output.dense_quantize': False, 'bert.encoder.layer.1.attention.self.query_quantize': True, 'bert.encoder.layer.1.attention.self.query_width': 16, 'bert.encoder.layer.1.attention.self.query_frac_width': 2, 'bert.encoder.layer.1.attention.self.key_quantize': False, 'bert.encoder.layer.1.attention.self.value_quantize': True, 'bert.encoder.layer.1.attention.self.value_width': 8, 'bert.encoder.layer.1.attention.self.value_frac_width': 2, 'bert.encoder.layer.1.attention.output.dense_quantize': False, 'bert.encoder.layer.1.intermediate.dense_quantize': False, 'bert.encoder.layer.1.output.dense_quantize': True, 'bert.encoder.layer.1.output.dense_width': 16, 'bert.encoder.layer.1.output.dense_frac_width': 8, 'bert.pooler.dense_quantize': True, 'bert.pooler.dense_width': 8, 'bert.pooler.dense_frac_width': 8, 'classifier_quantize': True, 'classifier_width': 16, 'classifier_frac_width': 8}. Best is trial 2 with value: 0.85544.\n",
      "/vol/bitbucket/ug22/adls/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:32, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.628300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.462200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.416500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.384400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.361400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.364200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:15]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-02-04 14:18:05,908] Trial 5 finished with value: 0.8582 and parameters: {'bert.encoder.layer.0.attention.self.query_quantize': False, 'bert.encoder.layer.0.attention.self.key_quantize': True, 'bert.encoder.layer.0.attention.self.key_width': 32, 'bert.encoder.layer.0.attention.self.key_frac_width': 4, 'bert.encoder.layer.0.attention.self.value_quantize': True, 'bert.encoder.layer.0.attention.self.value_width': 16, 'bert.encoder.layer.0.attention.self.value_frac_width': 8, 'bert.encoder.layer.0.attention.output.dense_quantize': False, 'bert.encoder.layer.0.intermediate.dense_quantize': True, 'bert.encoder.layer.0.intermediate.dense_width': 16, 'bert.encoder.layer.0.intermediate.dense_frac_width': 4, 'bert.encoder.layer.0.output.dense_quantize': False, 'bert.encoder.layer.1.attention.self.query_quantize': False, 'bert.encoder.layer.1.attention.self.key_quantize': True, 'bert.encoder.layer.1.attention.self.key_width': 32, 'bert.encoder.layer.1.attention.self.key_frac_width': 2, 'bert.encoder.layer.1.attention.self.value_quantize': False, 'bert.encoder.layer.1.attention.output.dense_quantize': False, 'bert.encoder.layer.1.intermediate.dense_quantize': True, 'bert.encoder.layer.1.intermediate.dense_width': 16, 'bert.encoder.layer.1.intermediate.dense_frac_width': 2, 'bert.encoder.layer.1.output.dense_quantize': True, 'bert.encoder.layer.1.output.dense_width': 32, 'bert.encoder.layer.1.output.dense_frac_width': 4, 'bert.pooler.dense_quantize': False, 'classifier_quantize': True, 'classifier_width': 8, 'classifier_frac_width': 4}. Best is trial 5 with value: 0.8582.\n",
      "/vol/bitbucket/ug22/adls/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:31, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.638300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.490700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.441000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.404500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.388100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.388500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:14]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-02-04 14:18:53,013] Trial 6 finished with value: 0.85024 and parameters: {'bert.encoder.layer.0.attention.self.query_quantize': True, 'bert.encoder.layer.0.attention.self.query_width': 8, 'bert.encoder.layer.0.attention.self.query_frac_width': 2, 'bert.encoder.layer.0.attention.self.key_quantize': True, 'bert.encoder.layer.0.attention.self.key_width': 8, 'bert.encoder.layer.0.attention.self.key_frac_width': 4, 'bert.encoder.layer.0.attention.self.value_quantize': False, 'bert.encoder.layer.0.attention.output.dense_quantize': False, 'bert.encoder.layer.0.intermediate.dense_quantize': True, 'bert.encoder.layer.0.intermediate.dense_width': 32, 'bert.encoder.layer.0.intermediate.dense_frac_width': 8, 'bert.encoder.layer.0.output.dense_quantize': True, 'bert.encoder.layer.0.output.dense_width': 16, 'bert.encoder.layer.0.output.dense_frac_width': 8, 'bert.encoder.layer.1.attention.self.query_quantize': False, 'bert.encoder.layer.1.attention.self.key_quantize': True, 'bert.encoder.layer.1.attention.self.key_width': 8, 'bert.encoder.layer.1.attention.self.key_frac_width': 4, 'bert.encoder.layer.1.attention.self.value_quantize': False, 'bert.encoder.layer.1.attention.output.dense_quantize': False, 'bert.encoder.layer.1.intermediate.dense_quantize': False, 'bert.encoder.layer.1.output.dense_quantize': False, 'bert.pooler.dense_quantize': False, 'classifier_quantize': True, 'classifier_width': 8, 'classifier_frac_width': 8}. Best is trial 5 with value: 0.8582.\n",
      "/vol/bitbucket/ug22/adls/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:33, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.618500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.458800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.415500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.387500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.369700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.371500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:15]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-02-04 14:19:43,279] Trial 7 finished with value: 0.849 and parameters: {'bert.encoder.layer.0.attention.self.query_quantize': False, 'bert.encoder.layer.0.attention.self.key_quantize': True, 'bert.encoder.layer.0.attention.self.key_width': 8, 'bert.encoder.layer.0.attention.self.key_frac_width': 2, 'bert.encoder.layer.0.attention.self.value_quantize': True, 'bert.encoder.layer.0.attention.self.value_width': 32, 'bert.encoder.layer.0.attention.self.value_frac_width': 8, 'bert.encoder.layer.0.attention.output.dense_quantize': True, 'bert.encoder.layer.0.attention.output.dense_width': 8, 'bert.encoder.layer.0.attention.output.dense_frac_width': 8, 'bert.encoder.layer.0.intermediate.dense_quantize': False, 'bert.encoder.layer.0.output.dense_quantize': True, 'bert.encoder.layer.0.output.dense_width': 16, 'bert.encoder.layer.0.output.dense_frac_width': 8, 'bert.encoder.layer.1.attention.self.query_quantize': True, 'bert.encoder.layer.1.attention.self.query_width': 32, 'bert.encoder.layer.1.attention.self.query_frac_width': 4, 'bert.encoder.layer.1.attention.self.key_quantize': True, 'bert.encoder.layer.1.attention.self.key_width': 8, 'bert.encoder.layer.1.attention.self.key_frac_width': 2, 'bert.encoder.layer.1.attention.self.value_quantize': False, 'bert.encoder.layer.1.attention.output.dense_quantize': False, 'bert.encoder.layer.1.intermediate.dense_quantize': True, 'bert.encoder.layer.1.intermediate.dense_width': 16, 'bert.encoder.layer.1.intermediate.dense_frac_width': 8, 'bert.encoder.layer.1.output.dense_quantize': True, 'bert.encoder.layer.1.output.dense_width': 32, 'bert.encoder.layer.1.output.dense_frac_width': 8, 'bert.pooler.dense_quantize': False, 'classifier_quantize': False}. Best is trial 5 with value: 0.8582.\n",
      "/vol/bitbucket/ug22/adls/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:32, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.633000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.475100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.429400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.398300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.379500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.389900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:15]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-02-04 14:20:31,606] Trial 8 finished with value: 0.85512 and parameters: {'bert.encoder.layer.0.attention.self.query_quantize': True, 'bert.encoder.layer.0.attention.self.query_width': 8, 'bert.encoder.layer.0.attention.self.query_frac_width': 2, 'bert.encoder.layer.0.attention.self.key_quantize': False, 'bert.encoder.layer.0.attention.self.value_quantize': False, 'bert.encoder.layer.0.attention.output.dense_quantize': True, 'bert.encoder.layer.0.attention.output.dense_width': 32, 'bert.encoder.layer.0.attention.output.dense_frac_width': 4, 'bert.encoder.layer.0.intermediate.dense_quantize': True, 'bert.encoder.layer.0.intermediate.dense_width': 16, 'bert.encoder.layer.0.intermediate.dense_frac_width': 4, 'bert.encoder.layer.0.output.dense_quantize': False, 'bert.encoder.layer.1.attention.self.query_quantize': False, 'bert.encoder.layer.1.attention.self.key_quantize': True, 'bert.encoder.layer.1.attention.self.key_width': 8, 'bert.encoder.layer.1.attention.self.key_frac_width': 8, 'bert.encoder.layer.1.attention.self.value_quantize': False, 'bert.encoder.layer.1.attention.output.dense_quantize': True, 'bert.encoder.layer.1.attention.output.dense_width': 16, 'bert.encoder.layer.1.attention.output.dense_frac_width': 4, 'bert.encoder.layer.1.intermediate.dense_quantize': False, 'bert.encoder.layer.1.output.dense_quantize': True, 'bert.encoder.layer.1.output.dense_width': 32, 'bert.encoder.layer.1.output.dense_frac_width': 8, 'bert.pooler.dense_quantize': False, 'classifier_quantize': True, 'classifier_width': 8, 'classifier_frac_width': 8}. Best is trial 5 with value: 0.8582.\n",
      "/vol/bitbucket/ug22/adls/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:33, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.640600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.465700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.411100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.380600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.362500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.374100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:15]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-02-04 14:21:21,689] Trial 9 finished with value: 0.85412 and parameters: {'bert.encoder.layer.0.attention.self.query_quantize': True, 'bert.encoder.layer.0.attention.self.query_width': 32, 'bert.encoder.layer.0.attention.self.query_frac_width': 4, 'bert.encoder.layer.0.attention.self.key_quantize': False, 'bert.encoder.layer.0.attention.self.value_quantize': True, 'bert.encoder.layer.0.attention.self.value_width': 16, 'bert.encoder.layer.0.attention.self.value_frac_width': 2, 'bert.encoder.layer.0.attention.output.dense_quantize': False, 'bert.encoder.layer.0.intermediate.dense_quantize': True, 'bert.encoder.layer.0.intermediate.dense_width': 16, 'bert.encoder.layer.0.intermediate.dense_frac_width': 4, 'bert.encoder.layer.0.output.dense_quantize': True, 'bert.encoder.layer.0.output.dense_width': 8, 'bert.encoder.layer.0.output.dense_frac_width': 8, 'bert.encoder.layer.1.attention.self.query_quantize': False, 'bert.encoder.layer.1.attention.self.key_quantize': True, 'bert.encoder.layer.1.attention.self.key_width': 16, 'bert.encoder.layer.1.attention.self.key_frac_width': 8, 'bert.encoder.layer.1.attention.self.value_quantize': True, 'bert.encoder.layer.1.attention.self.value_width': 8, 'bert.encoder.layer.1.attention.self.value_frac_width': 4, 'bert.encoder.layer.1.attention.output.dense_quantize': False, 'bert.encoder.layer.1.intermediate.dense_quantize': True, 'bert.encoder.layer.1.intermediate.dense_width': 32, 'bert.encoder.layer.1.intermediate.dense_frac_width': 2, 'bert.encoder.layer.1.output.dense_quantize': True, 'bert.encoder.layer.1.output.dense_width': 16, 'bert.encoder.layer.1.output.dense_frac_width': 2, 'bert.pooler.dense_quantize': False, 'classifier_quantize': False}. Best is trial 5 with value: 0.8582.\n",
      "/vol/bitbucket/ug22/adls/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:32, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.621700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.451700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.415100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.389900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.367600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.373000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:15]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-02-04 14:22:11,552] Trial 10 finished with value: 0.85684 and parameters: {'bert.encoder.layer.0.attention.self.query_quantize': False, 'bert.encoder.layer.0.attention.self.key_quantize': True, 'bert.encoder.layer.0.attention.self.key_width': 16, 'bert.encoder.layer.0.attention.self.key_frac_width': 8, 'bert.encoder.layer.0.attention.self.value_quantize': True, 'bert.encoder.layer.0.attention.self.value_width': 16, 'bert.encoder.layer.0.attention.self.value_frac_width': 8, 'bert.encoder.layer.0.attention.output.dense_quantize': True, 'bert.encoder.layer.0.attention.output.dense_width': 16, 'bert.encoder.layer.0.attention.output.dense_frac_width': 2, 'bert.encoder.layer.0.intermediate.dense_quantize': False, 'bert.encoder.layer.0.output.dense_quantize': False, 'bert.encoder.layer.1.attention.self.query_quantize': True, 'bert.encoder.layer.1.attention.self.query_width': 8, 'bert.encoder.layer.1.attention.self.query_frac_width': 8, 'bert.encoder.layer.1.attention.self.key_quantize': False, 'bert.encoder.layer.1.attention.self.value_quantize': False, 'bert.encoder.layer.1.attention.output.dense_quantize': True, 'bert.encoder.layer.1.attention.output.dense_width': 32, 'bert.encoder.layer.1.attention.output.dense_frac_width': 2, 'bert.encoder.layer.1.intermediate.dense_quantize': True, 'bert.encoder.layer.1.intermediate.dense_width': 8, 'bert.encoder.layer.1.intermediate.dense_frac_width': 2, 'bert.encoder.layer.1.output.dense_quantize': False, 'bert.pooler.dense_quantize': False, 'classifier_quantize': True, 'classifier_width': 32, 'classifier_frac_width': 4}. Best is trial 5 with value: 0.8582.\n",
      "/vol/bitbucket/ug22/adls/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:32, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.621700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.451700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.415100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.389900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.367600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.373000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:15]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-02-04 14:23:00,459] Trial 11 finished with value: 0.85684 and parameters: {'bert.encoder.layer.0.attention.self.query_quantize': False, 'bert.encoder.layer.0.attention.self.key_quantize': True, 'bert.encoder.layer.0.attention.self.key_width': 16, 'bert.encoder.layer.0.attention.self.key_frac_width': 8, 'bert.encoder.layer.0.attention.self.value_quantize': True, 'bert.encoder.layer.0.attention.self.value_width': 16, 'bert.encoder.layer.0.attention.self.value_frac_width': 8, 'bert.encoder.layer.0.attention.output.dense_quantize': True, 'bert.encoder.layer.0.attention.output.dense_width': 16, 'bert.encoder.layer.0.attention.output.dense_frac_width': 2, 'bert.encoder.layer.0.intermediate.dense_quantize': False, 'bert.encoder.layer.0.output.dense_quantize': False, 'bert.encoder.layer.1.attention.self.query_quantize': True, 'bert.encoder.layer.1.attention.self.query_width': 8, 'bert.encoder.layer.1.attention.self.query_frac_width': 8, 'bert.encoder.layer.1.attention.self.key_quantize': False, 'bert.encoder.layer.1.attention.self.value_quantize': False, 'bert.encoder.layer.1.attention.output.dense_quantize': True, 'bert.encoder.layer.1.attention.output.dense_width': 32, 'bert.encoder.layer.1.attention.output.dense_frac_width': 2, 'bert.encoder.layer.1.intermediate.dense_quantize': True, 'bert.encoder.layer.1.intermediate.dense_width': 8, 'bert.encoder.layer.1.intermediate.dense_frac_width': 2, 'bert.encoder.layer.1.output.dense_quantize': False, 'bert.pooler.dense_quantize': False, 'classifier_quantize': True, 'classifier_width': 32, 'classifier_frac_width': 4}. Best is trial 5 with value: 0.8582.\n",
      "/vol/bitbucket/ug22/adls/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:31, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.617000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.448800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.413600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.392000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.368800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.380300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:15]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-02-04 14:23:48,629] Trial 12 finished with value: 0.85668 and parameters: {'bert.encoder.layer.0.attention.self.query_quantize': False, 'bert.encoder.layer.0.attention.self.key_quantize': True, 'bert.encoder.layer.0.attention.self.key_width': 32, 'bert.encoder.layer.0.attention.self.key_frac_width': 8, 'bert.encoder.layer.0.attention.self.value_quantize': True, 'bert.encoder.layer.0.attention.self.value_width': 16, 'bert.encoder.layer.0.attention.self.value_frac_width': 4, 'bert.encoder.layer.0.attention.output.dense_quantize': True, 'bert.encoder.layer.0.attention.output.dense_width': 16, 'bert.encoder.layer.0.attention.output.dense_frac_width': 2, 'bert.encoder.layer.0.intermediate.dense_quantize': False, 'bert.encoder.layer.0.output.dense_quantize': False, 'bert.encoder.layer.1.attention.self.query_quantize': True, 'bert.encoder.layer.1.attention.self.query_width': 8, 'bert.encoder.layer.1.attention.self.query_frac_width': 8, 'bert.encoder.layer.1.attention.self.key_quantize': False, 'bert.encoder.layer.1.attention.self.value_quantize': False, 'bert.encoder.layer.1.attention.output.dense_quantize': True, 'bert.encoder.layer.1.attention.output.dense_width': 32, 'bert.encoder.layer.1.attention.output.dense_frac_width': 2, 'bert.encoder.layer.1.intermediate.dense_quantize': True, 'bert.encoder.layer.1.intermediate.dense_width': 8, 'bert.encoder.layer.1.intermediate.dense_frac_width': 4, 'bert.encoder.layer.1.output.dense_quantize': False, 'bert.pooler.dense_quantize': False, 'classifier_quantize': True, 'classifier_width': 32, 'classifier_frac_width': 4}. Best is trial 5 with value: 0.8582.\n",
      "/vol/bitbucket/ug22/adls/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:31, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.621700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.451700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.415100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.390300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.368200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.375400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:15]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-02-04 14:24:36,873] Trial 13 finished with value: 0.85688 and parameters: {'bert.encoder.layer.0.attention.self.query_quantize': False, 'bert.encoder.layer.0.attention.self.key_quantize': True, 'bert.encoder.layer.0.attention.self.key_width': 16, 'bert.encoder.layer.0.attention.self.key_frac_width': 8, 'bert.encoder.layer.0.attention.self.value_quantize': True, 'bert.encoder.layer.0.attention.self.value_width': 16, 'bert.encoder.layer.0.attention.self.value_frac_width': 8, 'bert.encoder.layer.0.attention.output.dense_quantize': True, 'bert.encoder.layer.0.attention.output.dense_width': 16, 'bert.encoder.layer.0.attention.output.dense_frac_width': 2, 'bert.encoder.layer.0.intermediate.dense_quantize': False, 'bert.encoder.layer.0.output.dense_quantize': False, 'bert.encoder.layer.1.attention.self.query_quantize': True, 'bert.encoder.layer.1.attention.self.query_width': 8, 'bert.encoder.layer.1.attention.self.query_frac_width': 8, 'bert.encoder.layer.1.attention.self.key_quantize': False, 'bert.encoder.layer.1.attention.self.value_quantize': False, 'bert.encoder.layer.1.attention.output.dense_quantize': True, 'bert.encoder.layer.1.attention.output.dense_width': 32, 'bert.encoder.layer.1.attention.output.dense_frac_width': 2, 'bert.encoder.layer.1.intermediate.dense_quantize': True, 'bert.encoder.layer.1.intermediate.dense_width': 8, 'bert.encoder.layer.1.intermediate.dense_frac_width': 2, 'bert.encoder.layer.1.output.dense_quantize': False, 'bert.pooler.dense_quantize': False, 'classifier_quantize': True, 'classifier_width': 16, 'classifier_frac_width': 4}. Best is trial 5 with value: 0.8582.\n",
      "/vol/bitbucket/ug22/adls/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:31, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.626500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.477500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.428700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.393900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.379800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.375000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:14]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-02-04 14:25:24,465] Trial 14 finished with value: 0.85016 and parameters: {'bert.encoder.layer.0.attention.self.query_quantize': False, 'bert.encoder.layer.0.attention.self.key_quantize': True, 'bert.encoder.layer.0.attention.self.key_width': 32, 'bert.encoder.layer.0.attention.self.key_frac_width': 2, 'bert.encoder.layer.0.attention.self.value_quantize': True, 'bert.encoder.layer.0.attention.self.value_width': 16, 'bert.encoder.layer.0.attention.self.value_frac_width': 8, 'bert.encoder.layer.0.attention.output.dense_quantize': True, 'bert.encoder.layer.0.attention.output.dense_width': 32, 'bert.encoder.layer.0.attention.output.dense_frac_width': 4, 'bert.encoder.layer.0.intermediate.dense_quantize': False, 'bert.encoder.layer.0.output.dense_quantize': False, 'bert.encoder.layer.1.attention.self.query_quantize': False, 'bert.encoder.layer.1.attention.self.key_quantize': False, 'bert.encoder.layer.1.attention.self.value_quantize': False, 'bert.encoder.layer.1.attention.output.dense_quantize': False, 'bert.encoder.layer.1.intermediate.dense_quantize': True, 'bert.encoder.layer.1.intermediate.dense_width': 16, 'bert.encoder.layer.1.intermediate.dense_frac_width': 2, 'bert.encoder.layer.1.output.dense_quantize': True, 'bert.encoder.layer.1.output.dense_width': 8, 'bert.encoder.layer.1.output.dense_frac_width': 4, 'bert.pooler.dense_quantize': False, 'classifier_quantize': True, 'classifier_width': 16, 'classifier_frac_width': 4}. Best is trial 5 with value: 0.8582.\n",
      "/vol/bitbucket/ug22/adls/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:32, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:15]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-02-04 14:26:13,449] Trial 15 finished with value: 0.5 and parameters: {'bert.encoder.layer.0.attention.self.query_quantize': False, 'bert.encoder.layer.0.attention.self.key_quantize': True, 'bert.encoder.layer.0.attention.self.key_width': 16, 'bert.encoder.layer.0.attention.self.key_frac_width': 8, 'bert.encoder.layer.0.attention.self.value_quantize': True, 'bert.encoder.layer.0.attention.self.value_width': 8, 'bert.encoder.layer.0.attention.self.value_frac_width': 8, 'bert.encoder.layer.0.attention.output.dense_quantize': True, 'bert.encoder.layer.0.attention.output.dense_width': 8, 'bert.encoder.layer.0.attention.output.dense_frac_width': 8, 'bert.encoder.layer.0.intermediate.dense_quantize': False, 'bert.encoder.layer.0.output.dense_quantize': False, 'bert.encoder.layer.1.attention.self.query_quantize': True, 'bert.encoder.layer.1.attention.self.query_width': 32, 'bert.encoder.layer.1.attention.self.query_frac_width': 2, 'bert.encoder.layer.1.attention.self.key_quantize': False, 'bert.encoder.layer.1.attention.self.value_quantize': False, 'bert.encoder.layer.1.attention.output.dense_quantize': True, 'bert.encoder.layer.1.attention.output.dense_width': 8, 'bert.encoder.layer.1.attention.output.dense_frac_width': 8, 'bert.encoder.layer.1.intermediate.dense_quantize': True, 'bert.encoder.layer.1.intermediate.dense_width': 16, 'bert.encoder.layer.1.intermediate.dense_frac_width': 2, 'bert.encoder.layer.1.output.dense_quantize': False, 'bert.pooler.dense_quantize': False, 'classifier_quantize': True, 'classifier_width': 16, 'classifier_frac_width': 2}. Best is trial 5 with value: 0.8582.\n",
      "/vol/bitbucket/ug22/adls/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:33, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.613200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.444900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.414600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.394000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.366700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.380600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:15]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-02-04 14:27:03,781] Trial 16 finished with value: 0.855 and parameters: {'bert.encoder.layer.0.attention.self.query_quantize': False, 'bert.encoder.layer.0.attention.self.key_quantize': True, 'bert.encoder.layer.0.attention.self.key_width': 32, 'bert.encoder.layer.0.attention.self.key_frac_width': 8, 'bert.encoder.layer.0.attention.self.value_quantize': True, 'bert.encoder.layer.0.attention.self.value_width': 16, 'bert.encoder.layer.0.attention.self.value_frac_width': 4, 'bert.encoder.layer.0.attention.output.dense_quantize': True, 'bert.encoder.layer.0.attention.output.dense_width': 16, 'bert.encoder.layer.0.attention.output.dense_frac_width': 2, 'bert.encoder.layer.0.intermediate.dense_quantize': False, 'bert.encoder.layer.0.output.dense_quantize': False, 'bert.encoder.layer.1.attention.self.query_quantize': True, 'bert.encoder.layer.1.attention.self.query_width': 8, 'bert.encoder.layer.1.attention.self.query_frac_width': 8, 'bert.encoder.layer.1.attention.self.key_quantize': False, 'bert.encoder.layer.1.attention.self.value_quantize': False, 'bert.encoder.layer.1.attention.output.dense_quantize': True, 'bert.encoder.layer.1.attention.output.dense_width': 32, 'bert.encoder.layer.1.attention.output.dense_frac_width': 2, 'bert.encoder.layer.1.intermediate.dense_quantize': True, 'bert.encoder.layer.1.intermediate.dense_width': 8, 'bert.encoder.layer.1.intermediate.dense_frac_width': 4, 'bert.encoder.layer.1.output.dense_quantize': True, 'bert.encoder.layer.1.output.dense_width': 32, 'bert.encoder.layer.1.output.dense_frac_width': 4, 'bert.pooler.dense_quantize': False, 'classifier_quantize': True, 'classifier_width': 8, 'classifier_frac_width': 4}. Best is trial 5 with value: 0.8582.\n",
      "/vol/bitbucket/ug22/adls/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:30, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.626300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.458900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.416400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.386500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.365700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.370100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:14]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-02-04 14:27:49,867] Trial 17 finished with value: 0.85768 and parameters: {'bert.encoder.layer.0.attention.self.query_quantize': False, 'bert.encoder.layer.0.attention.self.key_quantize': True, 'bert.encoder.layer.0.attention.self.key_width': 16, 'bert.encoder.layer.0.attention.self.key_frac_width': 4, 'bert.encoder.layer.0.attention.self.value_quantize': True, 'bert.encoder.layer.0.attention.self.value_width': 16, 'bert.encoder.layer.0.attention.self.value_frac_width': 8, 'bert.encoder.layer.0.attention.output.dense_quantize': False, 'bert.encoder.layer.0.intermediate.dense_quantize': False, 'bert.encoder.layer.0.output.dense_quantize': False, 'bert.encoder.layer.1.attention.self.query_quantize': False, 'bert.encoder.layer.1.attention.self.key_quantize': True, 'bert.encoder.layer.1.attention.self.key_width': 32, 'bert.encoder.layer.1.attention.self.key_frac_width': 2, 'bert.encoder.layer.1.attention.self.value_quantize': False, 'bert.encoder.layer.1.attention.output.dense_quantize': False, 'bert.encoder.layer.1.intermediate.dense_quantize': True, 'bert.encoder.layer.1.intermediate.dense_width': 16, 'bert.encoder.layer.1.intermediate.dense_frac_width': 2, 'bert.encoder.layer.1.output.dense_quantize': False, 'bert.pooler.dense_quantize': False, 'classifier_quantize': True, 'classifier_width': 16, 'classifier_frac_width': 4}. Best is trial 5 with value: 0.8582.\n",
      "/vol/bitbucket/ug22/adls/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:31, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.633800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.464900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.423000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.397400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.373900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.377300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:14]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-02-04 14:28:37,610] Trial 18 finished with value: 0.85252 and parameters: {'bert.encoder.layer.0.attention.self.query_quantize': False, 'bert.encoder.layer.0.attention.self.key_quantize': False, 'bert.encoder.layer.0.attention.self.value_quantize': True, 'bert.encoder.layer.0.attention.self.value_width': 16, 'bert.encoder.layer.0.attention.self.value_frac_width': 8, 'bert.encoder.layer.0.attention.output.dense_quantize': False, 'bert.encoder.layer.0.intermediate.dense_quantize': True, 'bert.encoder.layer.0.intermediate.dense_width': 16, 'bert.encoder.layer.0.intermediate.dense_frac_width': 2, 'bert.encoder.layer.0.output.dense_quantize': False, 'bert.encoder.layer.1.attention.self.query_quantize': False, 'bert.encoder.layer.1.attention.self.key_quantize': True, 'bert.encoder.layer.1.attention.self.key_width': 32, 'bert.encoder.layer.1.attention.self.key_frac_width': 2, 'bert.encoder.layer.1.attention.self.value_quantize': True, 'bert.encoder.layer.1.attention.self.value_width': 32, 'bert.encoder.layer.1.attention.self.value_frac_width': 8, 'bert.encoder.layer.1.attention.output.dense_quantize': False, 'bert.encoder.layer.1.intermediate.dense_quantize': True, 'bert.encoder.layer.1.intermediate.dense_width': 16, 'bert.encoder.layer.1.intermediate.dense_frac_width': 2, 'bert.encoder.layer.1.output.dense_quantize': False, 'bert.pooler.dense_quantize': True, 'bert.pooler.dense_width': 16, 'bert.pooler.dense_frac_width': 2, 'classifier_quantize': False}. Best is trial 5 with value: 0.8582.\n",
      "/vol/bitbucket/ug22/adls/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:32, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.625000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.458600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.416000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.386900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.366300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.367600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:14]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-02-04 14:29:25,453] Trial 19 finished with value: 0.85784 and parameters: {'bert.encoder.layer.0.attention.self.query_quantize': False, 'bert.encoder.layer.0.attention.self.key_quantize': True, 'bert.encoder.layer.0.attention.self.key_width': 32, 'bert.encoder.layer.0.attention.self.key_frac_width': 4, 'bert.encoder.layer.0.attention.self.value_quantize': True, 'bert.encoder.layer.0.attention.self.value_width': 32, 'bert.encoder.layer.0.attention.self.value_frac_width': 8, 'bert.encoder.layer.0.attention.output.dense_quantize': False, 'bert.encoder.layer.0.intermediate.dense_quantize': False, 'bert.encoder.layer.0.output.dense_quantize': False, 'bert.encoder.layer.1.attention.self.query_quantize': False, 'bert.encoder.layer.1.attention.self.key_quantize': True, 'bert.encoder.layer.1.attention.self.key_width': 32, 'bert.encoder.layer.1.attention.self.key_frac_width': 2, 'bert.encoder.layer.1.attention.self.value_quantize': False, 'bert.encoder.layer.1.attention.output.dense_quantize': False, 'bert.encoder.layer.1.intermediate.dense_quantize': True, 'bert.encoder.layer.1.intermediate.dense_width': 16, 'bert.encoder.layer.1.intermediate.dense_frac_width': 2, 'bert.encoder.layer.1.output.dense_quantize': True, 'bert.encoder.layer.1.output.dense_width': 32, 'bert.encoder.layer.1.output.dense_frac_width': 4, 'bert.pooler.dense_quantize': False, 'classifier_quantize': True, 'classifier_width': 16, 'classifier_frac_width': 4}. Best is trial 5 with value: 0.8582.\n",
      "[I 2026-02-04 14:29:25,458] A new study created in memory with name: bert-MinifloatDenorm-search\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Integer - Best: 0.8582\n",
      "\n",
      "============================================================\n",
      "Searching: MinifloatDenorm\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vol/bitbucket/ug22/adls/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 01:00, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.693200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:42]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-02-04 14:31:09,855] Trial 0 finished with value: 0.5 and parameters: {'bert.encoder.layer.0.attention.self.query_quantize': True, 'bert.encoder.layer.0.attention.self.query_width': 16, 'bert.encoder.layer.0.attention.self.query_exp_width': 2, 'bert.encoder.layer.0.attention.self.query_exp_bias': 15, 'bert.encoder.layer.0.attention.self.key_quantize': True, 'bert.encoder.layer.0.attention.self.key_width': 32, 'bert.encoder.layer.0.attention.self.key_exp_width': 8, 'bert.encoder.layer.0.attention.self.key_exp_bias': 3, 'bert.encoder.layer.0.attention.self.value_quantize': True, 'bert.encoder.layer.0.attention.self.value_width': 32, 'bert.encoder.layer.0.attention.self.value_exp_width': 4, 'bert.encoder.layer.0.attention.self.value_exp_bias': 3, 'bert.encoder.layer.0.attention.output.dense_quantize': False, 'bert.encoder.layer.0.intermediate.dense_quantize': True, 'bert.encoder.layer.0.intermediate.dense_width': 16, 'bert.encoder.layer.0.intermediate.dense_exp_width': 2, 'bert.encoder.layer.0.intermediate.dense_exp_bias': 15, 'bert.encoder.layer.0.output.dense_quantize': True, 'bert.encoder.layer.0.output.dense_width': 16, 'bert.encoder.layer.0.output.dense_exp_width': 8, 'bert.encoder.layer.0.output.dense_exp_bias': 7, 'bert.encoder.layer.1.attention.self.query_quantize': True, 'bert.encoder.layer.1.attention.self.query_width': 8, 'bert.encoder.layer.1.attention.self.query_exp_width': 8, 'bert.encoder.layer.1.attention.self.query_exp_bias': 3, 'bert.encoder.layer.1.attention.self.key_quantize': True, 'bert.encoder.layer.1.attention.self.key_width': 16, 'bert.encoder.layer.1.attention.self.key_exp_width': 8, 'bert.encoder.layer.1.attention.self.key_exp_bias': 15, 'bert.encoder.layer.1.attention.self.value_quantize': True, 'bert.encoder.layer.1.attention.self.value_width': 8, 'bert.encoder.layer.1.attention.self.value_exp_width': 2, 'bert.encoder.layer.1.attention.self.value_exp_bias': 15, 'bert.encoder.layer.1.attention.output.dense_quantize': True, 'bert.encoder.layer.1.attention.output.dense_width': 32, 'bert.encoder.layer.1.attention.output.dense_exp_width': 8, 'bert.encoder.layer.1.attention.output.dense_exp_bias': 7, 'bert.encoder.layer.1.intermediate.dense_quantize': False, 'bert.encoder.layer.1.output.dense_quantize': False, 'bert.pooler.dense_quantize': True, 'bert.pooler.dense_width': 32, 'bert.pooler.dense_exp_width': 4, 'bert.pooler.dense_exp_bias': 7, 'classifier_quantize': True, 'classifier_width': 32, 'classifier_exp_width': 2, 'classifier_exp_bias': 15}. Best is trial 0 with value: 0.5.\n",
      "/vol/bitbucket/ug22/adls/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:45, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.611400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.428100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.401300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.376600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.357400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.361800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:28]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-02-04 14:32:25,501] Trial 1 finished with value: 0.85916 and parameters: {'bert.encoder.layer.0.attention.self.query_quantize': False, 'bert.encoder.layer.0.attention.self.key_quantize': True, 'bert.encoder.layer.0.attention.self.key_width': 32, 'bert.encoder.layer.0.attention.self.key_exp_width': 8, 'bert.encoder.layer.0.attention.self.key_exp_bias': 15, 'bert.encoder.layer.0.attention.self.value_quantize': False, 'bert.encoder.layer.0.attention.output.dense_quantize': False, 'bert.encoder.layer.0.intermediate.dense_quantize': False, 'bert.encoder.layer.0.output.dense_quantize': False, 'bert.encoder.layer.1.attention.self.query_quantize': True, 'bert.encoder.layer.1.attention.self.query_width': 32, 'bert.encoder.layer.1.attention.self.query_exp_width': 2, 'bert.encoder.layer.1.attention.self.query_exp_bias': 3, 'bert.encoder.layer.1.attention.self.key_quantize': True, 'bert.encoder.layer.1.attention.self.key_width': 16, 'bert.encoder.layer.1.attention.self.key_exp_width': 4, 'bert.encoder.layer.1.attention.self.key_exp_bias': 3, 'bert.encoder.layer.1.attention.self.value_quantize': True, 'bert.encoder.layer.1.attention.self.value_width': 16, 'bert.encoder.layer.1.attention.self.value_exp_width': 8, 'bert.encoder.layer.1.attention.self.value_exp_bias': 3, 'bert.encoder.layer.1.attention.output.dense_quantize': True, 'bert.encoder.layer.1.attention.output.dense_width': 16, 'bert.encoder.layer.1.attention.output.dense_exp_width': 4, 'bert.encoder.layer.1.attention.output.dense_exp_bias': 3, 'bert.encoder.layer.1.intermediate.dense_quantize': False, 'bert.encoder.layer.1.output.dense_quantize': True, 'bert.encoder.layer.1.output.dense_width': 8, 'bert.encoder.layer.1.output.dense_exp_width': 4, 'bert.encoder.layer.1.output.dense_exp_bias': 7, 'bert.pooler.dense_quantize': False, 'classifier_quantize': False}. Best is trial 1 with value: 0.85916.\n",
      "/vol/bitbucket/ug22/adls/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:42, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.688000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.660400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.636400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.618900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.605500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.598800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:25]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-02-04 14:33:34,846] Trial 2 finished with value: 0.80888 and parameters: {'bert.encoder.layer.0.attention.self.query_quantize': True, 'bert.encoder.layer.0.attention.self.query_width': 8, 'bert.encoder.layer.0.attention.self.query_exp_width': 8, 'bert.encoder.layer.0.attention.self.query_exp_bias': 7, 'bert.encoder.layer.0.attention.self.key_quantize': True, 'bert.encoder.layer.0.attention.self.key_width': 16, 'bert.encoder.layer.0.attention.self.key_exp_width': 8, 'bert.encoder.layer.0.attention.self.key_exp_bias': 15, 'bert.encoder.layer.0.attention.self.value_quantize': False, 'bert.encoder.layer.0.attention.output.dense_quantize': True, 'bert.encoder.layer.0.attention.output.dense_width': 32, 'bert.encoder.layer.0.attention.output.dense_exp_width': 4, 'bert.encoder.layer.0.attention.output.dense_exp_bias': 15, 'bert.encoder.layer.0.intermediate.dense_quantize': False, 'bert.encoder.layer.0.output.dense_quantize': False, 'bert.encoder.layer.1.attention.self.query_quantize': False, 'bert.encoder.layer.1.attention.self.key_quantize': True, 'bert.encoder.layer.1.attention.self.key_width': 32, 'bert.encoder.layer.1.attention.self.key_exp_width': 8, 'bert.encoder.layer.1.attention.self.key_exp_bias': 3, 'bert.encoder.layer.1.attention.self.value_quantize': False, 'bert.encoder.layer.1.attention.output.dense_quantize': False, 'bert.encoder.layer.1.intermediate.dense_quantize': False, 'bert.encoder.layer.1.output.dense_quantize': False, 'bert.pooler.dense_quantize': False, 'classifier_quantize': True, 'classifier_width': 16, 'classifier_exp_width': 2, 'classifier_exp_bias': 7}. Best is trial 1 with value: 0.85916.\n",
      "/vol/bitbucket/ug22/adls/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:42, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.740200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.772500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.711300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.703500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.729900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.727000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:25]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-02-04 14:34:43,897] Trial 3 finished with value: 0.5 and parameters: {'bert.encoder.layer.0.attention.self.query_quantize': False, 'bert.encoder.layer.0.attention.self.key_quantize': True, 'bert.encoder.layer.0.attention.self.key_width': 8, 'bert.encoder.layer.0.attention.self.key_exp_width': 8, 'bert.encoder.layer.0.attention.self.key_exp_bias': 15, 'bert.encoder.layer.0.attention.self.value_quantize': False, 'bert.encoder.layer.0.attention.output.dense_quantize': False, 'bert.encoder.layer.0.intermediate.dense_quantize': False, 'bert.encoder.layer.0.output.dense_quantize': False, 'bert.encoder.layer.1.attention.self.query_quantize': False, 'bert.encoder.layer.1.attention.self.key_quantize': False, 'bert.encoder.layer.1.attention.self.value_quantize': True, 'bert.encoder.layer.1.attention.self.value_width': 16, 'bert.encoder.layer.1.attention.self.value_exp_width': 8, 'bert.encoder.layer.1.attention.self.value_exp_bias': 7, 'bert.encoder.layer.1.attention.output.dense_quantize': True, 'bert.encoder.layer.1.attention.output.dense_width': 16, 'bert.encoder.layer.1.attention.output.dense_exp_width': 4, 'bert.encoder.layer.1.attention.output.dense_exp_bias': 3, 'bert.encoder.layer.1.intermediate.dense_quantize': True, 'bert.encoder.layer.1.intermediate.dense_width': 8, 'bert.encoder.layer.1.intermediate.dense_exp_width': 2, 'bert.encoder.layer.1.intermediate.dense_exp_bias': 3, 'bert.encoder.layer.1.output.dense_quantize': False, 'bert.pooler.dense_quantize': True, 'bert.pooler.dense_width': 8, 'bert.pooler.dense_exp_width': 8, 'bert.pooler.dense_exp_bias': 3, 'classifier_quantize': False}. Best is trial 1 with value: 0.85916.\n",
      "/vol/bitbucket/ug22/adls/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:45, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.672600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.553300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.453800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.423000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.396600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.401800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:28]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-02-04 14:35:59,134] Trial 4 finished with value: 0.84456 and parameters: {'bert.encoder.layer.0.attention.self.query_quantize': False, 'bert.encoder.layer.0.attention.self.key_quantize': True, 'bert.encoder.layer.0.attention.self.key_width': 8, 'bert.encoder.layer.0.attention.self.key_exp_width': 4, 'bert.encoder.layer.0.attention.self.key_exp_bias': 7, 'bert.encoder.layer.0.attention.self.value_quantize': False, 'bert.encoder.layer.0.attention.output.dense_quantize': False, 'bert.encoder.layer.0.intermediate.dense_quantize': True, 'bert.encoder.layer.0.intermediate.dense_width': 8, 'bert.encoder.layer.0.intermediate.dense_exp_width': 2, 'bert.encoder.layer.0.intermediate.dense_exp_bias': 7, 'bert.encoder.layer.0.output.dense_quantize': True, 'bert.encoder.layer.0.output.dense_width': 8, 'bert.encoder.layer.0.output.dense_exp_width': 4, 'bert.encoder.layer.0.output.dense_exp_bias': 15, 'bert.encoder.layer.1.attention.self.query_quantize': False, 'bert.encoder.layer.1.attention.self.key_quantize': True, 'bert.encoder.layer.1.attention.self.key_width': 8, 'bert.encoder.layer.1.attention.self.key_exp_width': 8, 'bert.encoder.layer.1.attention.self.key_exp_bias': 15, 'bert.encoder.layer.1.attention.self.value_quantize': False, 'bert.encoder.layer.1.attention.output.dense_quantize': True, 'bert.encoder.layer.1.attention.output.dense_width': 32, 'bert.encoder.layer.1.attention.output.dense_exp_width': 4, 'bert.encoder.layer.1.attention.output.dense_exp_bias': 7, 'bert.encoder.layer.1.intermediate.dense_quantize': False, 'bert.encoder.layer.1.output.dense_quantize': False, 'bert.pooler.dense_quantize': False, 'classifier_quantize': True, 'classifier_width': 8, 'classifier_exp_width': 4, 'classifier_exp_bias': 15}. Best is trial 1 with value: 0.85916.\n",
      "/vol/bitbucket/ug22/adls/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:45, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.670400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.559200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.506000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.434400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.396800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.390800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:28]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-02-04 14:37:14,414] Trial 5 finished with value: 0.8438 and parameters: {'bert.encoder.layer.0.attention.self.query_quantize': True, 'bert.encoder.layer.0.attention.self.query_width': 16, 'bert.encoder.layer.0.attention.self.query_exp_width': 4, 'bert.encoder.layer.0.attention.self.query_exp_bias': 3, 'bert.encoder.layer.0.attention.self.key_quantize': True, 'bert.encoder.layer.0.attention.self.key_width': 16, 'bert.encoder.layer.0.attention.self.key_exp_width': 2, 'bert.encoder.layer.0.attention.self.key_exp_bias': 3, 'bert.encoder.layer.0.attention.self.value_quantize': True, 'bert.encoder.layer.0.attention.self.value_width': 8, 'bert.encoder.layer.0.attention.self.value_exp_width': 2, 'bert.encoder.layer.0.attention.self.value_exp_bias': 15, 'bert.encoder.layer.0.attention.output.dense_quantize': True, 'bert.encoder.layer.0.attention.output.dense_width': 16, 'bert.encoder.layer.0.attention.output.dense_exp_width': 2, 'bert.encoder.layer.0.attention.output.dense_exp_bias': 7, 'bert.encoder.layer.0.intermediate.dense_quantize': False, 'bert.encoder.layer.0.output.dense_quantize': False, 'bert.encoder.layer.1.attention.self.query_quantize': False, 'bert.encoder.layer.1.attention.self.key_quantize': True, 'bert.encoder.layer.1.attention.self.key_width': 32, 'bert.encoder.layer.1.attention.self.key_exp_width': 4, 'bert.encoder.layer.1.attention.self.key_exp_bias': 7, 'bert.encoder.layer.1.attention.self.value_quantize': False, 'bert.encoder.layer.1.attention.output.dense_quantize': False, 'bert.encoder.layer.1.intermediate.dense_quantize': False, 'bert.encoder.layer.1.output.dense_quantize': False, 'bert.pooler.dense_quantize': False, 'classifier_quantize': True, 'classifier_width': 8, 'classifier_exp_width': 4, 'classifier_exp_bias': 7}. Best is trial 1 with value: 0.85916.\n",
      "/vol/bitbucket/ug22/adls/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:48, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.751800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.713000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.713500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.721400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.699900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.700500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:31]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-02-04 14:38:34,835] Trial 6 finished with value: 0.50012 and parameters: {'bert.encoder.layer.0.attention.self.query_quantize': False, 'bert.encoder.layer.0.attention.self.key_quantize': False, 'bert.encoder.layer.0.attention.self.value_quantize': True, 'bert.encoder.layer.0.attention.self.value_width': 32, 'bert.encoder.layer.0.attention.self.value_exp_width': 2, 'bert.encoder.layer.0.attention.self.value_exp_bias': 3, 'bert.encoder.layer.0.attention.output.dense_quantize': False, 'bert.encoder.layer.0.intermediate.dense_quantize': True, 'bert.encoder.layer.0.intermediate.dense_width': 16, 'bert.encoder.layer.0.intermediate.dense_exp_width': 4, 'bert.encoder.layer.0.intermediate.dense_exp_bias': 3, 'bert.encoder.layer.0.output.dense_quantize': True, 'bert.encoder.layer.0.output.dense_width': 16, 'bert.encoder.layer.0.output.dense_exp_width': 4, 'bert.encoder.layer.0.output.dense_exp_bias': 3, 'bert.encoder.layer.1.attention.self.query_quantize': False, 'bert.encoder.layer.1.attention.self.key_quantize': False, 'bert.encoder.layer.1.attention.self.value_quantize': False, 'bert.encoder.layer.1.attention.output.dense_quantize': False, 'bert.encoder.layer.1.intermediate.dense_quantize': True, 'bert.encoder.layer.1.intermediate.dense_width': 8, 'bert.encoder.layer.1.intermediate.dense_exp_width': 2, 'bert.encoder.layer.1.intermediate.dense_exp_bias': 7, 'bert.encoder.layer.1.output.dense_quantize': True, 'bert.encoder.layer.1.output.dense_width': 8, 'bert.encoder.layer.1.output.dense_exp_width': 2, 'bert.encoder.layer.1.output.dense_exp_bias': 3, 'bert.pooler.dense_quantize': True, 'bert.pooler.dense_width': 8, 'bert.pooler.dense_exp_width': 8, 'bert.pooler.dense_exp_bias': 7, 'classifier_quantize': True, 'classifier_width': 32, 'classifier_exp_width': 2, 'classifier_exp_bias': 3}. Best is trial 1 with value: 0.85916.\n",
      "/vol/bitbucket/ug22/adls/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:36, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.620600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.447000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.404400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.378700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.362800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.367100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:19]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-02-04 14:39:32,456] Trial 7 finished with value: 0.85788 and parameters: {'bert.encoder.layer.0.attention.self.query_quantize': False, 'bert.encoder.layer.0.attention.self.key_quantize': False, 'bert.encoder.layer.0.attention.self.value_quantize': False, 'bert.encoder.layer.0.attention.output.dense_quantize': False, 'bert.encoder.layer.0.intermediate.dense_quantize': False, 'bert.encoder.layer.0.output.dense_quantize': False, 'bert.encoder.layer.1.attention.self.query_quantize': False, 'bert.encoder.layer.1.attention.self.key_quantize': False, 'bert.encoder.layer.1.attention.self.value_quantize': True, 'bert.encoder.layer.1.attention.self.value_width': 8, 'bert.encoder.layer.1.attention.self.value_exp_width': 4, 'bert.encoder.layer.1.attention.self.value_exp_bias': 7, 'bert.encoder.layer.1.attention.output.dense_quantize': True, 'bert.encoder.layer.1.attention.output.dense_width': 16, 'bert.encoder.layer.1.attention.output.dense_exp_width': 2, 'bert.encoder.layer.1.attention.output.dense_exp_bias': 3, 'bert.encoder.layer.1.intermediate.dense_quantize': False, 'bert.encoder.layer.1.output.dense_quantize': True, 'bert.encoder.layer.1.output.dense_width': 32, 'bert.encoder.layer.1.output.dense_exp_width': 4, 'bert.encoder.layer.1.output.dense_exp_bias': 3, 'bert.pooler.dense_quantize': False, 'classifier_quantize': False}. Best is trial 1 with value: 0.85916.\n",
      "/vol/bitbucket/ug22/adls/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:42, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.755400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.754800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.702500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.704700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.693600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.696100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:25]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-02-04 14:40:41,395] Trial 8 finished with value: 0.48792 and parameters: {'bert.encoder.layer.0.attention.self.query_quantize': False, 'bert.encoder.layer.0.attention.self.key_quantize': False, 'bert.encoder.layer.0.attention.self.value_quantize': False, 'bert.encoder.layer.0.attention.output.dense_quantize': False, 'bert.encoder.layer.0.intermediate.dense_quantize': True, 'bert.encoder.layer.0.intermediate.dense_width': 8, 'bert.encoder.layer.0.intermediate.dense_exp_width': 2, 'bert.encoder.layer.0.intermediate.dense_exp_bias': 15, 'bert.encoder.layer.0.output.dense_quantize': True, 'bert.encoder.layer.0.output.dense_width': 16, 'bert.encoder.layer.0.output.dense_exp_width': 4, 'bert.encoder.layer.0.output.dense_exp_bias': 15, 'bert.encoder.layer.1.attention.self.query_quantize': False, 'bert.encoder.layer.1.attention.self.key_quantize': False, 'bert.encoder.layer.1.attention.self.value_quantize': True, 'bert.encoder.layer.1.attention.self.value_width': 8, 'bert.encoder.layer.1.attention.self.value_exp_width': 4, 'bert.encoder.layer.1.attention.self.value_exp_bias': 7, 'bert.encoder.layer.1.attention.output.dense_quantize': False, 'bert.encoder.layer.1.intermediate.dense_quantize': False, 'bert.encoder.layer.1.output.dense_quantize': False, 'bert.pooler.dense_quantize': True, 'bert.pooler.dense_width': 8, 'bert.pooler.dense_exp_width': 8, 'bert.pooler.dense_exp_bias': 15, 'classifier_quantize': True, 'classifier_width': 32, 'classifier_exp_width': 8, 'classifier_exp_bias': 3}. Best is trial 1 with value: 0.85916.\n",
      "/vol/bitbucket/ug22/adls/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:48, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.694000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.655100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.569400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.516300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.472000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.466800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:31]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-02-04 14:42:02,238] Trial 9 finished with value: 0.83412 and parameters: {'bert.encoder.layer.0.attention.self.query_quantize': True, 'bert.encoder.layer.0.attention.self.query_width': 16, 'bert.encoder.layer.0.attention.self.query_exp_width': 4, 'bert.encoder.layer.0.attention.self.query_exp_bias': 15, 'bert.encoder.layer.0.attention.self.key_quantize': False, 'bert.encoder.layer.0.attention.self.value_quantize': True, 'bert.encoder.layer.0.attention.self.value_width': 16, 'bert.encoder.layer.0.attention.self.value_exp_width': 4, 'bert.encoder.layer.0.attention.self.value_exp_bias': 15, 'bert.encoder.layer.0.attention.output.dense_quantize': False, 'bert.encoder.layer.0.intermediate.dense_quantize': True, 'bert.encoder.layer.0.intermediate.dense_width': 32, 'bert.encoder.layer.0.intermediate.dense_exp_width': 8, 'bert.encoder.layer.0.intermediate.dense_exp_bias': 7, 'bert.encoder.layer.0.output.dense_quantize': True, 'bert.encoder.layer.0.output.dense_width': 8, 'bert.encoder.layer.0.output.dense_exp_width': 4, 'bert.encoder.layer.0.output.dense_exp_bias': 3, 'bert.encoder.layer.1.attention.self.query_quantize': True, 'bert.encoder.layer.1.attention.self.query_width': 8, 'bert.encoder.layer.1.attention.self.query_exp_width': 4, 'bert.encoder.layer.1.attention.self.query_exp_bias': 3, 'bert.encoder.layer.1.attention.self.key_quantize': False, 'bert.encoder.layer.1.attention.self.value_quantize': False, 'bert.encoder.layer.1.attention.output.dense_quantize': True, 'bert.encoder.layer.1.attention.output.dense_width': 32, 'bert.encoder.layer.1.attention.output.dense_exp_width': 4, 'bert.encoder.layer.1.attention.output.dense_exp_bias': 7, 'bert.encoder.layer.1.intermediate.dense_quantize': True, 'bert.encoder.layer.1.intermediate.dense_width': 8, 'bert.encoder.layer.1.intermediate.dense_exp_width': 8, 'bert.encoder.layer.1.intermediate.dense_exp_bias': 15, 'bert.encoder.layer.1.output.dense_quantize': False, 'bert.pooler.dense_quantize': False, 'classifier_quantize': False}. Best is trial 1 with value: 0.85916.\n",
      "/vol/bitbucket/ug22/adls/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:52, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.695200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.687100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.683400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.681400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.681400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.684700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:34]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-02-04 14:43:29,490] Trial 10 finished with value: 0.60676 and parameters: {'bert.encoder.layer.0.attention.self.query_quantize': False, 'bert.encoder.layer.0.attention.self.key_quantize': True, 'bert.encoder.layer.0.attention.self.key_width': 32, 'bert.encoder.layer.0.attention.self.key_exp_width': 2, 'bert.encoder.layer.0.attention.self.key_exp_bias': 15, 'bert.encoder.layer.0.attention.self.value_quantize': False, 'bert.encoder.layer.0.attention.output.dense_quantize': True, 'bert.encoder.layer.0.attention.output.dense_width': 8, 'bert.encoder.layer.0.attention.output.dense_exp_width': 8, 'bert.encoder.layer.0.attention.output.dense_exp_bias': 3, 'bert.encoder.layer.0.intermediate.dense_quantize': False, 'bert.encoder.layer.0.output.dense_quantize': False, 'bert.encoder.layer.1.attention.self.query_quantize': True, 'bert.encoder.layer.1.attention.self.query_width': 32, 'bert.encoder.layer.1.attention.self.query_exp_width': 2, 'bert.encoder.layer.1.attention.self.query_exp_bias': 7, 'bert.encoder.layer.1.attention.self.key_quantize': True, 'bert.encoder.layer.1.attention.self.key_width': 16, 'bert.encoder.layer.1.attention.self.key_exp_width': 4, 'bert.encoder.layer.1.attention.self.key_exp_bias': 3, 'bert.encoder.layer.1.attention.self.value_quantize': True, 'bert.encoder.layer.1.attention.self.value_width': 16, 'bert.encoder.layer.1.attention.self.value_exp_width': 8, 'bert.encoder.layer.1.attention.self.value_exp_bias': 3, 'bert.encoder.layer.1.attention.output.dense_quantize': True, 'bert.encoder.layer.1.attention.output.dense_width': 8, 'bert.encoder.layer.1.attention.output.dense_exp_width': 8, 'bert.encoder.layer.1.attention.output.dense_exp_bias': 15, 'bert.encoder.layer.1.intermediate.dense_quantize': True, 'bert.encoder.layer.1.intermediate.dense_width': 32, 'bert.encoder.layer.1.intermediate.dense_exp_width': 4, 'bert.encoder.layer.1.intermediate.dense_exp_bias': 3, 'bert.encoder.layer.1.output.dense_quantize': True, 'bert.encoder.layer.1.output.dense_width': 16, 'bert.encoder.layer.1.output.dense_exp_width': 8, 'bert.encoder.layer.1.output.dense_exp_bias': 7, 'bert.pooler.dense_quantize': False, 'classifier_quantize': False}. Best is trial 1 with value: 0.85916.\n",
      "/vol/bitbucket/ug22/adls/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:39, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.605100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.428100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.403300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.376600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.357900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.365000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:22]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-02-04 14:44:32,791] Trial 11 finished with value: 0.8584 and parameters: {'bert.encoder.layer.0.attention.self.query_quantize': False, 'bert.encoder.layer.0.attention.self.key_quantize': False, 'bert.encoder.layer.0.attention.self.value_quantize': False, 'bert.encoder.layer.0.attention.output.dense_quantize': False, 'bert.encoder.layer.0.intermediate.dense_quantize': False, 'bert.encoder.layer.0.output.dense_quantize': False, 'bert.encoder.layer.1.attention.self.query_quantize': True, 'bert.encoder.layer.1.attention.self.query_width': 32, 'bert.encoder.layer.1.attention.self.query_exp_width': 2, 'bert.encoder.layer.1.attention.self.query_exp_bias': 15, 'bert.encoder.layer.1.attention.self.key_quantize': False, 'bert.encoder.layer.1.attention.self.value_quantize': True, 'bert.encoder.layer.1.attention.self.value_width': 32, 'bert.encoder.layer.1.attention.self.value_exp_width': 4, 'bert.encoder.layer.1.attention.self.value_exp_bias': 3, 'bert.encoder.layer.1.attention.output.dense_quantize': True, 'bert.encoder.layer.1.attention.output.dense_width': 16, 'bert.encoder.layer.1.attention.output.dense_exp_width': 2, 'bert.encoder.layer.1.attention.output.dense_exp_bias': 3, 'bert.encoder.layer.1.intermediate.dense_quantize': False, 'bert.encoder.layer.1.output.dense_quantize': True, 'bert.encoder.layer.1.output.dense_width': 32, 'bert.encoder.layer.1.output.dense_exp_width': 4, 'bert.encoder.layer.1.output.dense_exp_bias': 7, 'bert.pooler.dense_quantize': False, 'classifier_quantize': False}. Best is trial 1 with value: 0.85916.\n",
      "/vol/bitbucket/ug22/adls/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:42, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.605100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.428100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.403200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.376600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.358000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.365100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:25]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-02-04 14:45:42,502] Trial 12 finished with value: 0.85824 and parameters: {'bert.encoder.layer.0.attention.self.query_quantize': False, 'bert.encoder.layer.0.attention.self.key_quantize': False, 'bert.encoder.layer.0.attention.self.value_quantize': False, 'bert.encoder.layer.0.attention.output.dense_quantize': False, 'bert.encoder.layer.0.intermediate.dense_quantize': False, 'bert.encoder.layer.0.output.dense_quantize': False, 'bert.encoder.layer.1.attention.self.query_quantize': True, 'bert.encoder.layer.1.attention.self.query_width': 32, 'bert.encoder.layer.1.attention.self.query_exp_width': 2, 'bert.encoder.layer.1.attention.self.query_exp_bias': 15, 'bert.encoder.layer.1.attention.self.key_quantize': True, 'bert.encoder.layer.1.attention.self.key_width': 16, 'bert.encoder.layer.1.attention.self.key_exp_width': 2, 'bert.encoder.layer.1.attention.self.key_exp_bias': 3, 'bert.encoder.layer.1.attention.self.value_quantize': True, 'bert.encoder.layer.1.attention.self.value_width': 32, 'bert.encoder.layer.1.attention.self.value_exp_width': 4, 'bert.encoder.layer.1.attention.self.value_exp_bias': 3, 'bert.encoder.layer.1.attention.output.dense_quantize': True, 'bert.encoder.layer.1.attention.output.dense_width': 16, 'bert.encoder.layer.1.attention.output.dense_exp_width': 2, 'bert.encoder.layer.1.attention.output.dense_exp_bias': 3, 'bert.encoder.layer.1.intermediate.dense_quantize': False, 'bert.encoder.layer.1.output.dense_quantize': True, 'bert.encoder.layer.1.output.dense_width': 32, 'bert.encoder.layer.1.output.dense_exp_width': 4, 'bert.encoder.layer.1.output.dense_exp_bias': 7, 'bert.pooler.dense_quantize': False, 'classifier_quantize': False}. Best is trial 1 with value: 0.85916.\n",
      "/vol/bitbucket/ug22/adls/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:39, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.605900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.429100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.403400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.377100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.359300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.365600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:22]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-02-04 14:46:45,832] Trial 13 finished with value: 0.859 and parameters: {'bert.encoder.layer.0.attention.self.query_quantize': False, 'bert.encoder.layer.0.attention.self.key_quantize': False, 'bert.encoder.layer.0.attention.self.value_quantize': False, 'bert.encoder.layer.0.attention.output.dense_quantize': False, 'bert.encoder.layer.0.intermediate.dense_quantize': False, 'bert.encoder.layer.0.output.dense_quantize': False, 'bert.encoder.layer.1.attention.self.query_quantize': True, 'bert.encoder.layer.1.attention.self.query_width': 32, 'bert.encoder.layer.1.attention.self.query_exp_width': 2, 'bert.encoder.layer.1.attention.self.query_exp_bias': 15, 'bert.encoder.layer.1.attention.self.key_quantize': False, 'bert.encoder.layer.1.attention.self.value_quantize': True, 'bert.encoder.layer.1.attention.self.value_width': 32, 'bert.encoder.layer.1.attention.self.value_exp_width': 8, 'bert.encoder.layer.1.attention.self.value_exp_bias': 3, 'bert.encoder.layer.1.attention.output.dense_quantize': True, 'bert.encoder.layer.1.attention.output.dense_width': 16, 'bert.encoder.layer.1.attention.output.dense_exp_width': 2, 'bert.encoder.layer.1.attention.output.dense_exp_bias': 3, 'bert.encoder.layer.1.intermediate.dense_quantize': False, 'bert.encoder.layer.1.output.dense_quantize': True, 'bert.encoder.layer.1.output.dense_width': 8, 'bert.encoder.layer.1.output.dense_exp_width': 4, 'bert.encoder.layer.1.output.dense_exp_bias': 7, 'bert.pooler.dense_quantize': False, 'classifier_quantize': False}. Best is trial 1 with value: 0.85916.\n",
      "/vol/bitbucket/ug22/adls/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:45, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.683700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.685600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.697000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.690200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.689400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.689400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:28]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-02-04 14:48:01,226] Trial 14 finished with value: 0.56128 and parameters: {'bert.encoder.layer.0.attention.self.query_quantize': False, 'bert.encoder.layer.0.attention.self.key_quantize': True, 'bert.encoder.layer.0.attention.self.key_width': 32, 'bert.encoder.layer.0.attention.self.key_exp_width': 4, 'bert.encoder.layer.0.attention.self.key_exp_bias': 7, 'bert.encoder.layer.0.attention.self.value_quantize': False, 'bert.encoder.layer.0.attention.output.dense_quantize': True, 'bert.encoder.layer.0.attention.output.dense_width': 8, 'bert.encoder.layer.0.attention.output.dense_exp_width': 8, 'bert.encoder.layer.0.attention.output.dense_exp_bias': 15, 'bert.encoder.layer.0.intermediate.dense_quantize': False, 'bert.encoder.layer.0.output.dense_quantize': False, 'bert.encoder.layer.1.attention.self.query_quantize': True, 'bert.encoder.layer.1.attention.self.query_width': 16, 'bert.encoder.layer.1.attention.self.query_exp_width': 2, 'bert.encoder.layer.1.attention.self.query_exp_bias': 3, 'bert.encoder.layer.1.attention.self.key_quantize': False, 'bert.encoder.layer.1.attention.self.value_quantize': True, 'bert.encoder.layer.1.attention.self.value_width': 32, 'bert.encoder.layer.1.attention.self.value_exp_width': 8, 'bert.encoder.layer.1.attention.self.value_exp_bias': 3, 'bert.encoder.layer.1.attention.output.dense_quantize': True, 'bert.encoder.layer.1.attention.output.dense_width': 16, 'bert.encoder.layer.1.attention.output.dense_exp_width': 2, 'bert.encoder.layer.1.attention.output.dense_exp_bias': 3, 'bert.encoder.layer.1.intermediate.dense_quantize': False, 'bert.encoder.layer.1.output.dense_quantize': True, 'bert.encoder.layer.1.output.dense_width': 8, 'bert.encoder.layer.1.output.dense_exp_width': 4, 'bert.encoder.layer.1.output.dense_exp_bias': 15, 'bert.pooler.dense_quantize': False, 'classifier_quantize': False}. Best is trial 1 with value: 0.85916.\n",
      "/vol/bitbucket/ug22/adls/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:42, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.608900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.426200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.403800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.374400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.355900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.362800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:25]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-02-04 14:49:10,381] Trial 15 finished with value: 0.8588 and parameters: {'bert.encoder.layer.0.attention.self.query_quantize': False, 'bert.encoder.layer.0.attention.self.key_quantize': False, 'bert.encoder.layer.0.attention.self.value_quantize': False, 'bert.encoder.layer.0.attention.output.dense_quantize': False, 'bert.encoder.layer.0.intermediate.dense_quantize': False, 'bert.encoder.layer.0.output.dense_quantize': False, 'bert.encoder.layer.1.attention.self.query_quantize': True, 'bert.encoder.layer.1.attention.self.query_width': 32, 'bert.encoder.layer.1.attention.self.query_exp_width': 2, 'bert.encoder.layer.1.attention.self.query_exp_bias': 15, 'bert.encoder.layer.1.attention.self.key_quantize': True, 'bert.encoder.layer.1.attention.self.key_width': 8, 'bert.encoder.layer.1.attention.self.key_exp_width': 4, 'bert.encoder.layer.1.attention.self.key_exp_bias': 7, 'bert.encoder.layer.1.attention.self.value_quantize': True, 'bert.encoder.layer.1.attention.self.value_width': 16, 'bert.encoder.layer.1.attention.self.value_exp_width': 8, 'bert.encoder.layer.1.attention.self.value_exp_bias': 3, 'bert.encoder.layer.1.attention.output.dense_quantize': True, 'bert.encoder.layer.1.attention.output.dense_width': 8, 'bert.encoder.layer.1.attention.output.dense_exp_width': 4, 'bert.encoder.layer.1.attention.output.dense_exp_bias': 15, 'bert.encoder.layer.1.intermediate.dense_quantize': False, 'bert.encoder.layer.1.output.dense_quantize': True, 'bert.encoder.layer.1.output.dense_width': 8, 'bert.encoder.layer.1.output.dense_exp_width': 4, 'bert.encoder.layer.1.output.dense_exp_bias': 7, 'bert.pooler.dense_quantize': False, 'classifier_quantize': False}. Best is trial 1 with value: 0.85916.\n",
      "/vol/bitbucket/ug22/adls/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:45, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.721000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.713800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.717500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.703000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.732100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.704200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:28]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-02-04 14:50:25,517] Trial 16 finished with value: 0.49916 and parameters: {'bert.encoder.layer.0.attention.self.query_quantize': False, 'bert.encoder.layer.0.attention.self.key_quantize': True, 'bert.encoder.layer.0.attention.self.key_width': 32, 'bert.encoder.layer.0.attention.self.key_exp_width': 8, 'bert.encoder.layer.0.attention.self.key_exp_bias': 15, 'bert.encoder.layer.0.attention.self.value_quantize': False, 'bert.encoder.layer.0.attention.output.dense_quantize': False, 'bert.encoder.layer.0.intermediate.dense_quantize': False, 'bert.encoder.layer.0.output.dense_quantize': False, 'bert.encoder.layer.1.attention.self.query_quantize': True, 'bert.encoder.layer.1.attention.self.query_width': 32, 'bert.encoder.layer.1.attention.self.query_exp_width': 8, 'bert.encoder.layer.1.attention.self.query_exp_bias': 7, 'bert.encoder.layer.1.attention.self.key_quantize': True, 'bert.encoder.layer.1.attention.self.key_width': 16, 'bert.encoder.layer.1.attention.self.key_exp_width': 2, 'bert.encoder.layer.1.attention.self.key_exp_bias': 3, 'bert.encoder.layer.1.attention.self.value_quantize': True, 'bert.encoder.layer.1.attention.self.value_width': 16, 'bert.encoder.layer.1.attention.self.value_exp_width': 8, 'bert.encoder.layer.1.attention.self.value_exp_bias': 15, 'bert.encoder.layer.1.attention.output.dense_quantize': True, 'bert.encoder.layer.1.attention.output.dense_width': 16, 'bert.encoder.layer.1.attention.output.dense_exp_width': 2, 'bert.encoder.layer.1.attention.output.dense_exp_bias': 3, 'bert.encoder.layer.1.intermediate.dense_quantize': False, 'bert.encoder.layer.1.output.dense_quantize': True, 'bert.encoder.layer.1.output.dense_width': 8, 'bert.encoder.layer.1.output.dense_exp_width': 8, 'bert.encoder.layer.1.output.dense_exp_bias': 7, 'bert.pooler.dense_quantize': False, 'classifier_quantize': False}. Best is trial 1 with value: 0.85916.\n",
      "/vol/bitbucket/ug22/adls/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:39, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.625700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.431400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.393800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.370200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.353500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.355000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:22]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-02-04 14:51:29,043] Trial 17 finished with value: 0.86016 and parameters: {'bert.encoder.layer.0.attention.self.query_quantize': False, 'bert.encoder.layer.0.attention.self.key_quantize': False, 'bert.encoder.layer.0.attention.self.value_quantize': False, 'bert.encoder.layer.0.attention.output.dense_quantize': False, 'bert.encoder.layer.0.intermediate.dense_quantize': False, 'bert.encoder.layer.0.output.dense_quantize': False, 'bert.encoder.layer.1.attention.self.query_quantize': True, 'bert.encoder.layer.1.attention.self.query_width': 16, 'bert.encoder.layer.1.attention.self.query_exp_width': 4, 'bert.encoder.layer.1.attention.self.query_exp_bias': 15, 'bert.encoder.layer.1.attention.self.key_quantize': False, 'bert.encoder.layer.1.attention.self.value_quantize': True, 'bert.encoder.layer.1.attention.self.value_width': 32, 'bert.encoder.layer.1.attention.self.value_exp_width': 2, 'bert.encoder.layer.1.attention.self.value_exp_bias': 3, 'bert.encoder.layer.1.attention.output.dense_quantize': True, 'bert.encoder.layer.1.attention.output.dense_width': 16, 'bert.encoder.layer.1.attention.output.dense_exp_width': 4, 'bert.encoder.layer.1.attention.output.dense_exp_bias': 3, 'bert.encoder.layer.1.intermediate.dense_quantize': False, 'bert.encoder.layer.1.output.dense_quantize': True, 'bert.encoder.layer.1.output.dense_width': 8, 'bert.encoder.layer.1.output.dense_exp_width': 2, 'bert.encoder.layer.1.output.dense_exp_bias': 15, 'bert.pooler.dense_quantize': False, 'classifier_quantize': False}. Best is trial 17 with value: 0.86016.\n",
      "/vol/bitbucket/ug22/adls/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:57, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.639800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.451800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.400500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.376600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.361100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.366000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:39]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-02-04 14:53:07,318] Trial 18 finished with value: 0.85588 and parameters: {'bert.encoder.layer.0.attention.self.query_quantize': True, 'bert.encoder.layer.0.attention.self.query_width': 32, 'bert.encoder.layer.0.attention.self.query_exp_width': 8, 'bert.encoder.layer.0.attention.self.query_exp_bias': 7, 'bert.encoder.layer.0.attention.self.key_quantize': True, 'bert.encoder.layer.0.attention.self.key_width': 32, 'bert.encoder.layer.0.attention.self.key_exp_width': 8, 'bert.encoder.layer.0.attention.self.key_exp_bias': 15, 'bert.encoder.layer.0.attention.self.value_quantize': True, 'bert.encoder.layer.0.attention.self.value_width': 16, 'bert.encoder.layer.0.attention.self.value_exp_width': 8, 'bert.encoder.layer.0.attention.self.value_exp_bias': 7, 'bert.encoder.layer.0.attention.output.dense_quantize': True, 'bert.encoder.layer.0.attention.output.dense_width': 16, 'bert.encoder.layer.0.attention.output.dense_exp_width': 2, 'bert.encoder.layer.0.attention.output.dense_exp_bias': 3, 'bert.encoder.layer.0.intermediate.dense_quantize': False, 'bert.encoder.layer.0.output.dense_quantize': False, 'bert.encoder.layer.1.attention.self.query_quantize': True, 'bert.encoder.layer.1.attention.self.query_width': 16, 'bert.encoder.layer.1.attention.self.query_exp_width': 4, 'bert.encoder.layer.1.attention.self.query_exp_bias': 3, 'bert.encoder.layer.1.attention.self.key_quantize': True, 'bert.encoder.layer.1.attention.self.key_width': 16, 'bert.encoder.layer.1.attention.self.key_exp_width': 4, 'bert.encoder.layer.1.attention.self.key_exp_bias': 3, 'bert.encoder.layer.1.attention.self.value_quantize': True, 'bert.encoder.layer.1.attention.self.value_width': 32, 'bert.encoder.layer.1.attention.self.value_exp_width': 2, 'bert.encoder.layer.1.attention.self.value_exp_bias': 3, 'bert.encoder.layer.1.attention.output.dense_quantize': False, 'bert.encoder.layer.1.intermediate.dense_quantize': True, 'bert.encoder.layer.1.intermediate.dense_width': 16, 'bert.encoder.layer.1.intermediate.dense_exp_width': 8, 'bert.encoder.layer.1.intermediate.dense_exp_bias': 7, 'bert.encoder.layer.1.output.dense_quantize': True, 'bert.encoder.layer.1.output.dense_width': 16, 'bert.encoder.layer.1.output.dense_exp_width': 2, 'bert.encoder.layer.1.output.dense_exp_bias': 15, 'bert.pooler.dense_quantize': True, 'bert.pooler.dense_width': 16, 'bert.pooler.dense_exp_width': 2, 'bert.pooler.dense_exp_bias': 3, 'classifier_quantize': False}. Best is trial 17 with value: 0.86016.\n",
      "/vol/bitbucket/ug22/adls/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:39, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.625700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.431400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.393800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.370200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.353500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.355000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:22]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-02-04 14:54:12,179] Trial 19 finished with value: 0.86016 and parameters: {'bert.encoder.layer.0.attention.self.query_quantize': False, 'bert.encoder.layer.0.attention.self.key_quantize': False, 'bert.encoder.layer.0.attention.self.value_quantize': False, 'bert.encoder.layer.0.attention.output.dense_quantize': False, 'bert.encoder.layer.0.intermediate.dense_quantize': False, 'bert.encoder.layer.0.output.dense_quantize': False, 'bert.encoder.layer.1.attention.self.query_quantize': True, 'bert.encoder.layer.1.attention.self.query_width': 16, 'bert.encoder.layer.1.attention.self.query_exp_width': 4, 'bert.encoder.layer.1.attention.self.query_exp_bias': 15, 'bert.encoder.layer.1.attention.self.key_quantize': False, 'bert.encoder.layer.1.attention.self.value_quantize': True, 'bert.encoder.layer.1.attention.self.value_width': 16, 'bert.encoder.layer.1.attention.self.value_exp_width': 2, 'bert.encoder.layer.1.attention.self.value_exp_bias': 3, 'bert.encoder.layer.1.attention.output.dense_quantize': True, 'bert.encoder.layer.1.attention.output.dense_width': 16, 'bert.encoder.layer.1.attention.output.dense_exp_width': 4, 'bert.encoder.layer.1.attention.output.dense_exp_bias': 3, 'bert.encoder.layer.1.intermediate.dense_quantize': False, 'bert.encoder.layer.1.output.dense_quantize': True, 'bert.encoder.layer.1.output.dense_width': 8, 'bert.encoder.layer.1.output.dense_exp_width': 2, 'bert.encoder.layer.1.output.dense_exp_bias': 15, 'bert.pooler.dense_quantize': False, 'classifier_quantize': False}. Best is trial 17 with value: 0.86016.\n",
      "[I 2026-02-04 14:54:12,184] A new study created in memory with name: bert-MinifloatIEEE-search\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MinifloatDenorm - Best: 0.8602\n",
      "\n",
      "============================================================\n",
      "Searching: MinifloatIEEE\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vol/bitbucket/ug22/adls/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 01:03, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.636400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.460800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.409000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.382600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.363100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.373800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:46]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-02-04 14:56:02,848] Trial 0 finished with value: 0.8538 and parameters: {'bert.encoder.layer.0.attention.self.query_quantize': True, 'bert.encoder.layer.0.attention.self.query_width': 32, 'bert.encoder.layer.0.attention.self.query_exp_width': 2, 'bert.encoder.layer.0.attention.self.query_exp_bias': 3, 'bert.encoder.layer.0.attention.self.key_quantize': True, 'bert.encoder.layer.0.attention.self.key_width': 16, 'bert.encoder.layer.0.attention.self.key_exp_width': 8, 'bert.encoder.layer.0.attention.self.key_exp_bias': 7, 'bert.encoder.layer.0.attention.self.value_quantize': False, 'bert.encoder.layer.0.attention.output.dense_quantize': False, 'bert.encoder.layer.0.intermediate.dense_quantize': False, 'bert.encoder.layer.0.output.dense_quantize': True, 'bert.encoder.layer.0.output.dense_width': 8, 'bert.encoder.layer.0.output.dense_exp_width': 8, 'bert.encoder.layer.0.output.dense_exp_bias': 7, 'bert.encoder.layer.1.attention.self.query_quantize': False, 'bert.encoder.layer.1.attention.self.key_quantize': True, 'bert.encoder.layer.1.attention.self.key_width': 16, 'bert.encoder.layer.1.attention.self.key_exp_width': 8, 'bert.encoder.layer.1.attention.self.key_exp_bias': 7, 'bert.encoder.layer.1.attention.self.value_quantize': False, 'bert.encoder.layer.1.attention.output.dense_quantize': True, 'bert.encoder.layer.1.attention.output.dense_width': 32, 'bert.encoder.layer.1.attention.output.dense_exp_width': 2, 'bert.encoder.layer.1.attention.output.dense_exp_bias': 3, 'bert.encoder.layer.1.intermediate.dense_quantize': True, 'bert.encoder.layer.1.intermediate.dense_width': 32, 'bert.encoder.layer.1.intermediate.dense_exp_width': 8, 'bert.encoder.layer.1.intermediate.dense_exp_bias': 7, 'bert.encoder.layer.1.output.dense_quantize': True, 'bert.encoder.layer.1.output.dense_width': 8, 'bert.encoder.layer.1.output.dense_exp_width': 2, 'bert.encoder.layer.1.output.dense_exp_bias': 3, 'bert.pooler.dense_quantize': False, 'classifier_quantize': False}. Best is trial 0 with value: 0.8538.\n",
      "/vol/bitbucket/ug22/adls/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 01:08, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.662900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.517700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.432100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.393100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.373900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.388000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:50]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-02-04 14:58:03,113] Trial 1 finished with value: 0.84908 and parameters: {'bert.encoder.layer.0.attention.self.query_quantize': True, 'bert.encoder.layer.0.attention.self.query_width': 32, 'bert.encoder.layer.0.attention.self.query_exp_width': 2, 'bert.encoder.layer.0.attention.self.query_exp_bias': 7, 'bert.encoder.layer.0.attention.self.key_quantize': False, 'bert.encoder.layer.0.attention.self.value_quantize': True, 'bert.encoder.layer.0.attention.self.value_width': 16, 'bert.encoder.layer.0.attention.self.value_exp_width': 4, 'bert.encoder.layer.0.attention.self.value_exp_bias': 15, 'bert.encoder.layer.0.attention.output.dense_quantize': False, 'bert.encoder.layer.0.intermediate.dense_quantize': False, 'bert.encoder.layer.0.output.dense_quantize': True, 'bert.encoder.layer.0.output.dense_width': 8, 'bert.encoder.layer.0.output.dense_exp_width': 2, 'bert.encoder.layer.0.output.dense_exp_bias': 3, 'bert.encoder.layer.1.attention.self.query_quantize': False, 'bert.encoder.layer.1.attention.self.key_quantize': True, 'bert.encoder.layer.1.attention.self.key_width': 16, 'bert.encoder.layer.1.attention.self.key_exp_width': 8, 'bert.encoder.layer.1.attention.self.key_exp_bias': 3, 'bert.encoder.layer.1.attention.self.value_quantize': True, 'bert.encoder.layer.1.attention.self.value_width': 32, 'bert.encoder.layer.1.attention.self.value_exp_width': 2, 'bert.encoder.layer.1.attention.self.value_exp_bias': 7, 'bert.encoder.layer.1.attention.output.dense_quantize': True, 'bert.encoder.layer.1.attention.output.dense_width': 8, 'bert.encoder.layer.1.attention.output.dense_exp_width': 8, 'bert.encoder.layer.1.attention.output.dense_exp_bias': 7, 'bert.encoder.layer.1.intermediate.dense_quantize': False, 'bert.encoder.layer.1.output.dense_quantize': True, 'bert.encoder.layer.1.output.dense_width': 16, 'bert.encoder.layer.1.output.dense_exp_width': 2, 'bert.encoder.layer.1.output.dense_exp_bias': 7, 'bert.pooler.dense_quantize': False, 'classifier_quantize': True, 'classifier_width': 16, 'classifier_exp_width': 4, 'classifier_exp_bias': 3}. Best is trial 0 with value: 0.8538.\n",
      "/vol/bitbucket/ug22/adls/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 01:03, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.634300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.449000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.422700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.396800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.371000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.384900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:46]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-02-04 14:59:53,838] Trial 2 finished with value: 0.85124 and parameters: {'bert.encoder.layer.0.attention.self.query_quantize': True, 'bert.encoder.layer.0.attention.self.query_width': 16, 'bert.encoder.layer.0.attention.self.query_exp_width': 4, 'bert.encoder.layer.0.attention.self.query_exp_bias': 15, 'bert.encoder.layer.0.attention.self.key_quantize': False, 'bert.encoder.layer.0.attention.self.value_quantize': True, 'bert.encoder.layer.0.attention.self.value_width': 32, 'bert.encoder.layer.0.attention.self.value_exp_width': 8, 'bert.encoder.layer.0.attention.self.value_exp_bias': 7, 'bert.encoder.layer.0.attention.output.dense_quantize': True, 'bert.encoder.layer.0.attention.output.dense_width': 16, 'bert.encoder.layer.0.attention.output.dense_exp_width': 8, 'bert.encoder.layer.0.attention.output.dense_exp_bias': 7, 'bert.encoder.layer.0.intermediate.dense_quantize': False, 'bert.encoder.layer.0.output.dense_quantize': True, 'bert.encoder.layer.0.output.dense_width': 32, 'bert.encoder.layer.0.output.dense_exp_width': 2, 'bert.encoder.layer.0.output.dense_exp_bias': 7, 'bert.encoder.layer.1.attention.self.query_quantize': True, 'bert.encoder.layer.1.attention.self.query_width': 32, 'bert.encoder.layer.1.attention.self.query_exp_width': 2, 'bert.encoder.layer.1.attention.self.query_exp_bias': 7, 'bert.encoder.layer.1.attention.self.key_quantize': False, 'bert.encoder.layer.1.attention.self.value_quantize': False, 'bert.encoder.layer.1.attention.output.dense_quantize': True, 'bert.encoder.layer.1.attention.output.dense_width': 8, 'bert.encoder.layer.1.attention.output.dense_exp_width': 8, 'bert.encoder.layer.1.attention.output.dense_exp_bias': 7, 'bert.encoder.layer.1.intermediate.dense_quantize': False, 'bert.encoder.layer.1.output.dense_quantize': False, 'bert.pooler.dense_quantize': True, 'bert.pooler.dense_width': 8, 'bert.pooler.dense_exp_width': 2, 'bert.pooler.dense_exp_bias': 3, 'classifier_quantize': False}. Best is trial 0 with value: 0.8538.\n",
      "/vol/bitbucket/ug22/adls/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 01:08, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.629500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.434900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.395700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.372000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.356900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.363800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:50]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-02-04 15:01:54,749] Trial 3 finished with value: 0.85804 and parameters: {'bert.encoder.layer.0.attention.self.query_quantize': True, 'bert.encoder.layer.0.attention.self.query_width': 16, 'bert.encoder.layer.0.attention.self.query_exp_width': 8, 'bert.encoder.layer.0.attention.self.query_exp_bias': 3, 'bert.encoder.layer.0.attention.self.key_quantize': True, 'bert.encoder.layer.0.attention.self.key_width': 8, 'bert.encoder.layer.0.attention.self.key_exp_width': 4, 'bert.encoder.layer.0.attention.self.key_exp_bias': 7, 'bert.encoder.layer.0.attention.self.value_quantize': True, 'bert.encoder.layer.0.attention.self.value_width': 8, 'bert.encoder.layer.0.attention.self.value_exp_width': 4, 'bert.encoder.layer.0.attention.self.value_exp_bias': 3, 'bert.encoder.layer.0.attention.output.dense_quantize': False, 'bert.encoder.layer.0.intermediate.dense_quantize': True, 'bert.encoder.layer.0.intermediate.dense_width': 8, 'bert.encoder.layer.0.intermediate.dense_exp_width': 4, 'bert.encoder.layer.0.intermediate.dense_exp_bias': 7, 'bert.encoder.layer.0.output.dense_quantize': False, 'bert.encoder.layer.1.attention.self.query_quantize': False, 'bert.encoder.layer.1.attention.self.key_quantize': True, 'bert.encoder.layer.1.attention.self.key_width': 32, 'bert.encoder.layer.1.attention.self.key_exp_width': 2, 'bert.encoder.layer.1.attention.self.key_exp_bias': 3, 'bert.encoder.layer.1.attention.self.value_quantize': True, 'bert.encoder.layer.1.attention.self.value_width': 8, 'bert.encoder.layer.1.attention.self.value_exp_width': 2, 'bert.encoder.layer.1.attention.self.value_exp_bias': 3, 'bert.encoder.layer.1.attention.output.dense_quantize': True, 'bert.encoder.layer.1.attention.output.dense_width': 32, 'bert.encoder.layer.1.attention.output.dense_exp_width': 8, 'bert.encoder.layer.1.attention.output.dense_exp_bias': 3, 'bert.encoder.layer.1.intermediate.dense_quantize': True, 'bert.encoder.layer.1.intermediate.dense_width': 16, 'bert.encoder.layer.1.intermediate.dense_exp_width': 2, 'bert.encoder.layer.1.intermediate.dense_exp_bias': 7, 'bert.encoder.layer.1.output.dense_quantize': False, 'bert.pooler.dense_quantize': False, 'classifier_quantize': False}. Best is trial 3 with value: 0.85804.\n",
      "/vol/bitbucket/ug22/adls/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:58, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.625800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.452500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.403400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.376000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.356000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.367000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:41]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-02-04 15:03:34,932] Trial 4 finished with value: 0.8564 and parameters: {'bert.encoder.layer.0.attention.self.query_quantize': True, 'bert.encoder.layer.0.attention.self.query_width': 32, 'bert.encoder.layer.0.attention.self.query_exp_width': 2, 'bert.encoder.layer.0.attention.self.query_exp_bias': 3, 'bert.encoder.layer.0.attention.self.key_quantize': False, 'bert.encoder.layer.0.attention.self.value_quantize': True, 'bert.encoder.layer.0.attention.self.value_width': 8, 'bert.encoder.layer.0.attention.self.value_exp_width': 2, 'bert.encoder.layer.0.attention.self.value_exp_bias': 3, 'bert.encoder.layer.0.attention.output.dense_quantize': False, 'bert.encoder.layer.0.intermediate.dense_quantize': False, 'bert.encoder.layer.0.output.dense_quantize': False, 'bert.encoder.layer.1.attention.self.query_quantize': True, 'bert.encoder.layer.1.attention.self.query_width': 8, 'bert.encoder.layer.1.attention.self.query_exp_width': 4, 'bert.encoder.layer.1.attention.self.query_exp_bias': 7, 'bert.encoder.layer.1.attention.self.key_quantize': True, 'bert.encoder.layer.1.attention.self.key_width': 8, 'bert.encoder.layer.1.attention.self.key_exp_width': 4, 'bert.encoder.layer.1.attention.self.key_exp_bias': 7, 'bert.encoder.layer.1.attention.self.value_quantize': False, 'bert.encoder.layer.1.attention.output.dense_quantize': False, 'bert.encoder.layer.1.intermediate.dense_quantize': False, 'bert.encoder.layer.1.output.dense_quantize': True, 'bert.encoder.layer.1.output.dense_width': 8, 'bert.encoder.layer.1.output.dense_exp_width': 2, 'bert.encoder.layer.1.output.dense_exp_bias': 3, 'bert.pooler.dense_quantize': False, 'classifier_quantize': True, 'classifier_width': 8, 'classifier_exp_width': 2, 'classifier_exp_bias': 3}. Best is trial 3 with value: 0.85804.\n",
      "/vol/bitbucket/ug22/adls/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 01:04, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.669300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.567500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.520700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.467300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.433900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.424600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:46]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-02-04 15:05:26,267] Trial 5 finished with value: 0.82332 and parameters: {'bert.encoder.layer.0.attention.self.query_quantize': False, 'bert.encoder.layer.0.attention.self.key_quantize': True, 'bert.encoder.layer.0.attention.self.key_width': 32, 'bert.encoder.layer.0.attention.self.key_exp_width': 8, 'bert.encoder.layer.0.attention.self.key_exp_bias': 7, 'bert.encoder.layer.0.attention.self.value_quantize': True, 'bert.encoder.layer.0.attention.self.value_width': 16, 'bert.encoder.layer.0.attention.self.value_exp_width': 2, 'bert.encoder.layer.0.attention.self.value_exp_bias': 15, 'bert.encoder.layer.0.attention.output.dense_quantize': False, 'bert.encoder.layer.0.intermediate.dense_quantize': True, 'bert.encoder.layer.0.intermediate.dense_width': 8, 'bert.encoder.layer.0.intermediate.dense_exp_width': 2, 'bert.encoder.layer.0.intermediate.dense_exp_bias': 3, 'bert.encoder.layer.0.output.dense_quantize': True, 'bert.encoder.layer.0.output.dense_width': 32, 'bert.encoder.layer.0.output.dense_exp_width': 8, 'bert.encoder.layer.0.output.dense_exp_bias': 7, 'bert.encoder.layer.1.attention.self.query_quantize': False, 'bert.encoder.layer.1.attention.self.key_quantize': False, 'bert.encoder.layer.1.attention.self.value_quantize': False, 'bert.encoder.layer.1.attention.output.dense_quantize': True, 'bert.encoder.layer.1.attention.output.dense_width': 32, 'bert.encoder.layer.1.attention.output.dense_exp_width': 4, 'bert.encoder.layer.1.attention.output.dense_exp_bias': 15, 'bert.encoder.layer.1.intermediate.dense_quantize': True, 'bert.encoder.layer.1.intermediate.dense_width': 16, 'bert.encoder.layer.1.intermediate.dense_exp_width': 8, 'bert.encoder.layer.1.intermediate.dense_exp_bias': 7, 'bert.encoder.layer.1.output.dense_quantize': True, 'bert.encoder.layer.1.output.dense_width': 32, 'bert.encoder.layer.1.output.dense_exp_width': 2, 'bert.encoder.layer.1.output.dense_exp_bias': 3, 'bert.pooler.dense_quantize': False, 'classifier_quantize': False}. Best is trial 3 with value: 0.85804.\n",
      "/vol/bitbucket/ug22/adls/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:47, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.698100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.705100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.704800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.702900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.704800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.703000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:31]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-02-04 15:06:46,174] Trial 6 finished with value: 0.4618 and parameters: {'bert.encoder.layer.0.attention.self.query_quantize': True, 'bert.encoder.layer.0.attention.self.query_width': 16, 'bert.encoder.layer.0.attention.self.query_exp_width': 8, 'bert.encoder.layer.0.attention.self.query_exp_bias': 15, 'bert.encoder.layer.0.attention.self.key_quantize': False, 'bert.encoder.layer.0.attention.self.value_quantize': False, 'bert.encoder.layer.0.attention.output.dense_quantize': False, 'bert.encoder.layer.0.intermediate.dense_quantize': False, 'bert.encoder.layer.0.output.dense_quantize': False, 'bert.encoder.layer.1.attention.self.query_quantize': False, 'bert.encoder.layer.1.attention.self.key_quantize': True, 'bert.encoder.layer.1.attention.self.key_width': 32, 'bert.encoder.layer.1.attention.self.key_exp_width': 8, 'bert.encoder.layer.1.attention.self.key_exp_bias': 3, 'bert.encoder.layer.1.attention.self.value_quantize': True, 'bert.encoder.layer.1.attention.self.value_width': 8, 'bert.encoder.layer.1.attention.self.value_exp_width': 8, 'bert.encoder.layer.1.attention.self.value_exp_bias': 15, 'bert.encoder.layer.1.attention.output.dense_quantize': False, 'bert.encoder.layer.1.intermediate.dense_quantize': False, 'bert.encoder.layer.1.output.dense_quantize': False, 'bert.pooler.dense_quantize': True, 'bert.pooler.dense_width': 8, 'bert.pooler.dense_exp_width': 8, 'bert.pooler.dense_exp_bias': 3, 'classifier_quantize': False}. Best is trial 3 with value: 0.85804.\n",
      "/vol/bitbucket/ug22/adls/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 01:08, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.695700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.699800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.701000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.703300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.704600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.705100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:51]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-02-04 15:08:47,310] Trial 7 finished with value: 0.25008 and parameters: {'bert.encoder.layer.0.attention.self.query_quantize': True, 'bert.encoder.layer.0.attention.self.query_width': 32, 'bert.encoder.layer.0.attention.self.query_exp_width': 8, 'bert.encoder.layer.0.attention.self.query_exp_bias': 15, 'bert.encoder.layer.0.attention.self.key_quantize': False, 'bert.encoder.layer.0.attention.self.value_quantize': True, 'bert.encoder.layer.0.attention.self.value_width': 32, 'bert.encoder.layer.0.attention.self.value_exp_width': 2, 'bert.encoder.layer.0.attention.self.value_exp_bias': 15, 'bert.encoder.layer.0.attention.output.dense_quantize': True, 'bert.encoder.layer.0.attention.output.dense_width': 16, 'bert.encoder.layer.0.attention.output.dense_exp_width': 2, 'bert.encoder.layer.0.attention.output.dense_exp_bias': 15, 'bert.encoder.layer.0.intermediate.dense_quantize': True, 'bert.encoder.layer.0.intermediate.dense_width': 32, 'bert.encoder.layer.0.intermediate.dense_exp_width': 4, 'bert.encoder.layer.0.intermediate.dense_exp_bias': 15, 'bert.encoder.layer.0.output.dense_quantize': True, 'bert.encoder.layer.0.output.dense_width': 8, 'bert.encoder.layer.0.output.dense_exp_width': 4, 'bert.encoder.layer.0.output.dense_exp_bias': 3, 'bert.encoder.layer.1.attention.self.query_quantize': True, 'bert.encoder.layer.1.attention.self.query_width': 16, 'bert.encoder.layer.1.attention.self.query_exp_width': 2, 'bert.encoder.layer.1.attention.self.query_exp_bias': 15, 'bert.encoder.layer.1.attention.self.key_quantize': False, 'bert.encoder.layer.1.attention.self.value_quantize': False, 'bert.encoder.layer.1.attention.output.dense_quantize': False, 'bert.encoder.layer.1.intermediate.dense_quantize': True, 'bert.encoder.layer.1.intermediate.dense_width': 32, 'bert.encoder.layer.1.intermediate.dense_exp_width': 8, 'bert.encoder.layer.1.intermediate.dense_exp_bias': 7, 'bert.encoder.layer.1.output.dense_quantize': False, 'bert.pooler.dense_quantize': True, 'bert.pooler.dense_width': 8, 'bert.pooler.dense_exp_width': 8, 'bert.pooler.dense_exp_bias': 3, 'classifier_quantize': False}. Best is trial 3 with value: 0.85804.\n",
      "/vol/bitbucket/ug22/adls/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 01:03, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.636200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.463200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.409400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.380700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.362800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.370800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:46]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-02-04 15:10:38,213] Trial 8 finished with value: 0.85308 and parameters: {'bert.encoder.layer.0.attention.self.query_quantize': True, 'bert.encoder.layer.0.attention.self.query_width': 16, 'bert.encoder.layer.0.attention.self.query_exp_width': 2, 'bert.encoder.layer.0.attention.self.query_exp_bias': 3, 'bert.encoder.layer.0.attention.self.key_quantize': False, 'bert.encoder.layer.0.attention.self.value_quantize': True, 'bert.encoder.layer.0.attention.self.value_width': 16, 'bert.encoder.layer.0.attention.self.value_exp_width': 4, 'bert.encoder.layer.0.attention.self.value_exp_bias': 7, 'bert.encoder.layer.0.attention.output.dense_quantize': False, 'bert.encoder.layer.0.intermediate.dense_quantize': True, 'bert.encoder.layer.0.intermediate.dense_width': 8, 'bert.encoder.layer.0.intermediate.dense_exp_width': 4, 'bert.encoder.layer.0.intermediate.dense_exp_bias': 3, 'bert.encoder.layer.0.output.dense_quantize': True, 'bert.encoder.layer.0.output.dense_width': 16, 'bert.encoder.layer.0.output.dense_exp_width': 2, 'bert.encoder.layer.0.output.dense_exp_bias': 7, 'bert.encoder.layer.1.attention.self.query_quantize': False, 'bert.encoder.layer.1.attention.self.key_quantize': False, 'bert.encoder.layer.1.attention.self.value_quantize': True, 'bert.encoder.layer.1.attention.self.value_width': 32, 'bert.encoder.layer.1.attention.self.value_exp_width': 8, 'bert.encoder.layer.1.attention.self.value_exp_bias': 3, 'bert.encoder.layer.1.attention.output.dense_quantize': True, 'bert.encoder.layer.1.attention.output.dense_width': 16, 'bert.encoder.layer.1.attention.output.dense_exp_width': 4, 'bert.encoder.layer.1.attention.output.dense_exp_bias': 7, 'bert.encoder.layer.1.intermediate.dense_quantize': False, 'bert.encoder.layer.1.output.dense_quantize': False, 'bert.pooler.dense_quantize': True, 'bert.pooler.dense_width': 32, 'bert.pooler.dense_exp_width': 8, 'bert.pooler.dense_exp_bias': 15, 'classifier_quantize': False}. Best is trial 3 with value: 0.85804.\n",
      "/vol/bitbucket/ug22/adls/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:53, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.736300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.723700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.730900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.735400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.739600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.732800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:36]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-02-04 15:12:08,913] Trial 9 finished with value: 0.49688 and parameters: {'bert.encoder.layer.0.attention.self.query_quantize': False, 'bert.encoder.layer.0.attention.self.key_quantize': False, 'bert.encoder.layer.0.attention.self.value_quantize': True, 'bert.encoder.layer.0.attention.self.value_width': 8, 'bert.encoder.layer.0.attention.self.value_exp_width': 8, 'bert.encoder.layer.0.attention.self.value_exp_bias': 7, 'bert.encoder.layer.0.attention.output.dense_quantize': False, 'bert.encoder.layer.0.intermediate.dense_quantize': False, 'bert.encoder.layer.0.output.dense_quantize': False, 'bert.encoder.layer.1.attention.self.query_quantize': False, 'bert.encoder.layer.1.attention.self.key_quantize': True, 'bert.encoder.layer.1.attention.self.key_width': 8, 'bert.encoder.layer.1.attention.self.key_exp_width': 4, 'bert.encoder.layer.1.attention.self.key_exp_bias': 3, 'bert.encoder.layer.1.attention.self.value_quantize': True, 'bert.encoder.layer.1.attention.self.value_width': 32, 'bert.encoder.layer.1.attention.self.value_exp_width': 8, 'bert.encoder.layer.1.attention.self.value_exp_bias': 15, 'bert.encoder.layer.1.attention.output.dense_quantize': False, 'bert.encoder.layer.1.intermediate.dense_quantize': False, 'bert.encoder.layer.1.output.dense_quantize': True, 'bert.encoder.layer.1.output.dense_width': 32, 'bert.encoder.layer.1.output.dense_exp_width': 8, 'bert.encoder.layer.1.output.dense_exp_bias': 15, 'bert.pooler.dense_quantize': False, 'classifier_quantize': True, 'classifier_width': 8, 'classifier_exp_width': 8, 'classifier_exp_bias': 3}. Best is trial 3 with value: 0.85804.\n",
      "/vol/bitbucket/ug22/adls/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 01:14, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:56]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-02-04 15:14:20,223] Trial 10 finished with value: 0.5 and parameters: {'bert.encoder.layer.0.attention.self.query_quantize': False, 'bert.encoder.layer.0.attention.self.key_quantize': True, 'bert.encoder.layer.0.attention.self.key_width': 8, 'bert.encoder.layer.0.attention.self.key_exp_width': 4, 'bert.encoder.layer.0.attention.self.key_exp_bias': 3, 'bert.encoder.layer.0.attention.self.value_quantize': False, 'bert.encoder.layer.0.attention.output.dense_quantize': True, 'bert.encoder.layer.0.attention.output.dense_width': 8, 'bert.encoder.layer.0.attention.output.dense_exp_width': 4, 'bert.encoder.layer.0.attention.output.dense_exp_bias': 3, 'bert.encoder.layer.0.intermediate.dense_quantize': True, 'bert.encoder.layer.0.intermediate.dense_width': 16, 'bert.encoder.layer.0.intermediate.dense_exp_width': 8, 'bert.encoder.layer.0.intermediate.dense_exp_bias': 7, 'bert.encoder.layer.0.output.dense_quantize': False, 'bert.encoder.layer.1.attention.self.query_quantize': True, 'bert.encoder.layer.1.attention.self.query_width': 16, 'bert.encoder.layer.1.attention.self.query_exp_width': 8, 'bert.encoder.layer.1.attention.self.query_exp_bias': 3, 'bert.encoder.layer.1.attention.self.key_quantize': True, 'bert.encoder.layer.1.attention.self.key_width': 32, 'bert.encoder.layer.1.attention.self.key_exp_width': 2, 'bert.encoder.layer.1.attention.self.key_exp_bias': 15, 'bert.encoder.layer.1.attention.self.value_quantize': True, 'bert.encoder.layer.1.attention.self.value_width': 8, 'bert.encoder.layer.1.attention.self.value_exp_width': 2, 'bert.encoder.layer.1.attention.self.value_exp_bias': 3, 'bert.encoder.layer.1.attention.output.dense_quantize': True, 'bert.encoder.layer.1.attention.output.dense_width': 32, 'bert.encoder.layer.1.attention.output.dense_exp_width': 8, 'bert.encoder.layer.1.attention.output.dense_exp_bias': 3, 'bert.encoder.layer.1.intermediate.dense_quantize': True, 'bert.encoder.layer.1.intermediate.dense_width': 8, 'bert.encoder.layer.1.intermediate.dense_exp_width': 2, 'bert.encoder.layer.1.intermediate.dense_exp_bias': 15, 'bert.encoder.layer.1.output.dense_quantize': False, 'bert.pooler.dense_quantize': False, 'classifier_quantize': True, 'classifier_width': 32, 'classifier_exp_width': 2, 'classifier_exp_bias': 15}. Best is trial 3 with value: 0.85804.\n",
      "/vol/bitbucket/ug22/adls/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 01:14, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.675000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.621700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.581100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.543200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.516500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.507700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:55]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-02-04 15:16:31,197] Trial 11 finished with value: 0.8316 and parameters: {'bert.encoder.layer.0.attention.self.query_quantize': True, 'bert.encoder.layer.0.attention.self.query_width': 8, 'bert.encoder.layer.0.attention.self.query_exp_width': 8, 'bert.encoder.layer.0.attention.self.query_exp_bias': 3, 'bert.encoder.layer.0.attention.self.key_quantize': True, 'bert.encoder.layer.0.attention.self.key_width': 8, 'bert.encoder.layer.0.attention.self.key_exp_width': 4, 'bert.encoder.layer.0.attention.self.key_exp_bias': 15, 'bert.encoder.layer.0.attention.self.value_quantize': True, 'bert.encoder.layer.0.attention.self.value_width': 8, 'bert.encoder.layer.0.attention.self.value_exp_width': 2, 'bert.encoder.layer.0.attention.self.value_exp_bias': 3, 'bert.encoder.layer.0.attention.output.dense_quantize': False, 'bert.encoder.layer.0.intermediate.dense_quantize': True, 'bert.encoder.layer.0.intermediate.dense_width': 8, 'bert.encoder.layer.0.intermediate.dense_exp_width': 4, 'bert.encoder.layer.0.intermediate.dense_exp_bias': 7, 'bert.encoder.layer.0.output.dense_quantize': False, 'bert.encoder.layer.1.attention.self.query_quantize': True, 'bert.encoder.layer.1.attention.self.query_width': 8, 'bert.encoder.layer.1.attention.self.query_exp_width': 4, 'bert.encoder.layer.1.attention.self.query_exp_bias': 7, 'bert.encoder.layer.1.attention.self.key_quantize': True, 'bert.encoder.layer.1.attention.self.key_width': 8, 'bert.encoder.layer.1.attention.self.key_exp_width': 2, 'bert.encoder.layer.1.attention.self.key_exp_bias': 7, 'bert.encoder.layer.1.attention.self.value_quantize': False, 'bert.encoder.layer.1.attention.output.dense_quantize': False, 'bert.encoder.layer.1.intermediate.dense_quantize': True, 'bert.encoder.layer.1.intermediate.dense_width': 16, 'bert.encoder.layer.1.intermediate.dense_exp_width': 2, 'bert.encoder.layer.1.intermediate.dense_exp_bias': 3, 'bert.encoder.layer.1.output.dense_quantize': True, 'bert.encoder.layer.1.output.dense_width': 8, 'bert.encoder.layer.1.output.dense_exp_width': 4, 'bert.encoder.layer.1.output.dense_exp_bias': 3, 'bert.pooler.dense_quantize': False, 'classifier_quantize': True, 'classifier_width': 8, 'classifier_exp_width': 2, 'classifier_exp_bias': 7}. Best is trial 3 with value: 0.85804.\n",
      "/vol/bitbucket/ug22/adls/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 01:08, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.640500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.486100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.414800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.376900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.354500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.360400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:50]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-02-04 15:18:31,998] Trial 12 finished with value: 0.85036 and parameters: {'bert.encoder.layer.0.attention.self.query_quantize': True, 'bert.encoder.layer.0.attention.self.query_width': 8, 'bert.encoder.layer.0.attention.self.query_exp_width': 4, 'bert.encoder.layer.0.attention.self.query_exp_bias': 3, 'bert.encoder.layer.0.attention.self.key_quantize': True, 'bert.encoder.layer.0.attention.self.key_width': 8, 'bert.encoder.layer.0.attention.self.key_exp_width': 2, 'bert.encoder.layer.0.attention.self.key_exp_bias': 7, 'bert.encoder.layer.0.attention.self.value_quantize': True, 'bert.encoder.layer.0.attention.self.value_width': 8, 'bert.encoder.layer.0.attention.self.value_exp_width': 4, 'bert.encoder.layer.0.attention.self.value_exp_bias': 3, 'bert.encoder.layer.0.attention.output.dense_quantize': False, 'bert.encoder.layer.0.intermediate.dense_quantize': False, 'bert.encoder.layer.0.output.dense_quantize': False, 'bert.encoder.layer.1.attention.self.query_quantize': True, 'bert.encoder.layer.1.attention.self.query_width': 8, 'bert.encoder.layer.1.attention.self.query_exp_width': 4, 'bert.encoder.layer.1.attention.self.query_exp_bias': 7, 'bert.encoder.layer.1.attention.self.key_quantize': True, 'bert.encoder.layer.1.attention.self.key_width': 32, 'bert.encoder.layer.1.attention.self.key_exp_width': 4, 'bert.encoder.layer.1.attention.self.key_exp_bias': 7, 'bert.encoder.layer.1.attention.self.value_quantize': False, 'bert.encoder.layer.1.attention.output.dense_quantize': False, 'bert.encoder.layer.1.intermediate.dense_quantize': True, 'bert.encoder.layer.1.intermediate.dense_width': 16, 'bert.encoder.layer.1.intermediate.dense_exp_width': 4, 'bert.encoder.layer.1.intermediate.dense_exp_bias': 3, 'bert.encoder.layer.1.output.dense_quantize': True, 'bert.encoder.layer.1.output.dense_width': 8, 'bert.encoder.layer.1.output.dense_exp_width': 8, 'bert.encoder.layer.1.output.dense_exp_bias': 7, 'bert.pooler.dense_quantize': False, 'classifier_quantize': True, 'classifier_width': 8, 'classifier_exp_width': 2, 'classifier_exp_bias': 3}. Best is trial 3 with value: 0.85804.\n",
      "/vol/bitbucket/ug22/adls/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 01:08, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.624600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.448900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.404200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.381100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.358300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.370000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:50]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-02-04 15:20:32,485] Trial 13 finished with value: 0.85676 and parameters: {'bert.encoder.layer.0.attention.self.query_quantize': True, 'bert.encoder.layer.0.attention.self.query_width': 32, 'bert.encoder.layer.0.attention.self.query_exp_width': 2, 'bert.encoder.layer.0.attention.self.query_exp_bias': 3, 'bert.encoder.layer.0.attention.self.key_quantize': True, 'bert.encoder.layer.0.attention.self.key_width': 32, 'bert.encoder.layer.0.attention.self.key_exp_width': 4, 'bert.encoder.layer.0.attention.self.key_exp_bias': 3, 'bert.encoder.layer.0.attention.self.value_quantize': True, 'bert.encoder.layer.0.attention.self.value_width': 8, 'bert.encoder.layer.0.attention.self.value_exp_width': 4, 'bert.encoder.layer.0.attention.self.value_exp_bias': 3, 'bert.encoder.layer.0.attention.output.dense_quantize': False, 'bert.encoder.layer.0.intermediate.dense_quantize': True, 'bert.encoder.layer.0.intermediate.dense_width': 32, 'bert.encoder.layer.0.intermediate.dense_exp_width': 2, 'bert.encoder.layer.0.intermediate.dense_exp_bias': 7, 'bert.encoder.layer.0.output.dense_quantize': False, 'bert.encoder.layer.1.attention.self.query_quantize': True, 'bert.encoder.layer.1.attention.self.query_width': 8, 'bert.encoder.layer.1.attention.self.query_exp_width': 4, 'bert.encoder.layer.1.attention.self.query_exp_bias': 15, 'bert.encoder.layer.1.attention.self.key_quantize': True, 'bert.encoder.layer.1.attention.self.key_width': 8, 'bert.encoder.layer.1.attention.self.key_exp_width': 4, 'bert.encoder.layer.1.attention.self.key_exp_bias': 15, 'bert.encoder.layer.1.attention.self.value_quantize': True, 'bert.encoder.layer.1.attention.self.value_width': 16, 'bert.encoder.layer.1.attention.self.value_exp_width': 4, 'bert.encoder.layer.1.attention.self.value_exp_bias': 3, 'bert.encoder.layer.1.attention.output.dense_quantize': False, 'bert.encoder.layer.1.intermediate.dense_quantize': False, 'bert.encoder.layer.1.output.dense_quantize': False, 'bert.pooler.dense_quantize': False, 'classifier_quantize': True, 'classifier_width': 16, 'classifier_exp_width': 4, 'classifier_exp_bias': 15}. Best is trial 3 with value: 0.85804.\n",
      "/vol/bitbucket/ug22/adls/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:59, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.605400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.425400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.399300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.375900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.356800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.363500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:41]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-02-04 15:22:14,450] Trial 14 finished with value: 0.85832 and parameters: {'bert.encoder.layer.0.attention.self.query_quantize': False, 'bert.encoder.layer.0.attention.self.key_quantize': True, 'bert.encoder.layer.0.attention.self.key_width': 32, 'bert.encoder.layer.0.attention.self.key_exp_width': 4, 'bert.encoder.layer.0.attention.self.key_exp_bias': 3, 'bert.encoder.layer.0.attention.self.value_quantize': False, 'bert.encoder.layer.0.attention.output.dense_quantize': True, 'bert.encoder.layer.0.attention.output.dense_width': 32, 'bert.encoder.layer.0.attention.output.dense_exp_width': 4, 'bert.encoder.layer.0.attention.output.dense_exp_bias': 15, 'bert.encoder.layer.0.intermediate.dense_quantize': True, 'bert.encoder.layer.0.intermediate.dense_width': 32, 'bert.encoder.layer.0.intermediate.dense_exp_width': 2, 'bert.encoder.layer.0.intermediate.dense_exp_bias': 7, 'bert.encoder.layer.0.output.dense_quantize': False, 'bert.encoder.layer.1.attention.self.query_quantize': False, 'bert.encoder.layer.1.attention.self.key_quantize': True, 'bert.encoder.layer.1.attention.self.key_width': 8, 'bert.encoder.layer.1.attention.self.key_exp_width': 2, 'bert.encoder.layer.1.attention.self.key_exp_bias': 15, 'bert.encoder.layer.1.attention.self.value_quantize': True, 'bert.encoder.layer.1.attention.self.value_width': 16, 'bert.encoder.layer.1.attention.self.value_exp_width': 4, 'bert.encoder.layer.1.attention.self.value_exp_bias': 3, 'bert.encoder.layer.1.attention.output.dense_quantize': True, 'bert.encoder.layer.1.attention.output.dense_width': 16, 'bert.encoder.layer.1.attention.output.dense_exp_width': 2, 'bert.encoder.layer.1.attention.output.dense_exp_bias': 3, 'bert.encoder.layer.1.intermediate.dense_quantize': False, 'bert.encoder.layer.1.output.dense_quantize': False, 'bert.pooler.dense_quantize': False, 'classifier_quantize': False}. Best is trial 14 with value: 0.85832.\n",
      "/vol/bitbucket/ug22/adls/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 01:04, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.630100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.437400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.395000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.366500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.349500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.355700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:46]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-02-04 15:24:06,111] Trial 15 finished with value: 0.85668 and parameters: {'bert.encoder.layer.0.attention.self.query_quantize': False, 'bert.encoder.layer.0.attention.self.key_quantize': True, 'bert.encoder.layer.0.attention.self.key_width': 32, 'bert.encoder.layer.0.attention.self.key_exp_width': 4, 'bert.encoder.layer.0.attention.self.key_exp_bias': 3, 'bert.encoder.layer.0.attention.self.value_quantize': False, 'bert.encoder.layer.0.attention.output.dense_quantize': True, 'bert.encoder.layer.0.attention.output.dense_width': 32, 'bert.encoder.layer.0.attention.output.dense_exp_width': 4, 'bert.encoder.layer.0.attention.output.dense_exp_bias': 15, 'bert.encoder.layer.0.intermediate.dense_quantize': True, 'bert.encoder.layer.0.intermediate.dense_width': 32, 'bert.encoder.layer.0.intermediate.dense_exp_width': 2, 'bert.encoder.layer.0.intermediate.dense_exp_bias': 7, 'bert.encoder.layer.0.output.dense_quantize': False, 'bert.encoder.layer.1.attention.self.query_quantize': False, 'bert.encoder.layer.1.attention.self.key_quantize': True, 'bert.encoder.layer.1.attention.self.key_width': 32, 'bert.encoder.layer.1.attention.self.key_exp_width': 2, 'bert.encoder.layer.1.attention.self.key_exp_bias': 15, 'bert.encoder.layer.1.attention.self.value_quantize': True, 'bert.encoder.layer.1.attention.self.value_width': 16, 'bert.encoder.layer.1.attention.self.value_exp_width': 4, 'bert.encoder.layer.1.attention.self.value_exp_bias': 3, 'bert.encoder.layer.1.attention.output.dense_quantize': True, 'bert.encoder.layer.1.attention.output.dense_width': 16, 'bert.encoder.layer.1.attention.output.dense_exp_width': 2, 'bert.encoder.layer.1.attention.output.dense_exp_bias': 3, 'bert.encoder.layer.1.intermediate.dense_quantize': True, 'bert.encoder.layer.1.intermediate.dense_width': 8, 'bert.encoder.layer.1.intermediate.dense_exp_width': 2, 'bert.encoder.layer.1.intermediate.dense_exp_bias': 15, 'bert.encoder.layer.1.output.dense_quantize': False, 'bert.pooler.dense_quantize': False, 'classifier_quantize': False}. Best is trial 14 with value: 0.85832.\n",
      "/vol/bitbucket/ug22/adls/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 01:06, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.607600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.435500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.404300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.373200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.357000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.365200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:53]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-02-04 15:26:07,287] Trial 16 finished with value: 0.858 and parameters: {'bert.encoder.layer.0.attention.self.query_quantize': False, 'bert.encoder.layer.0.attention.self.key_quantize': True, 'bert.encoder.layer.0.attention.self.key_width': 16, 'bert.encoder.layer.0.attention.self.key_exp_width': 4, 'bert.encoder.layer.0.attention.self.key_exp_bias': 15, 'bert.encoder.layer.0.attention.self.value_quantize': False, 'bert.encoder.layer.0.attention.output.dense_quantize': True, 'bert.encoder.layer.0.attention.output.dense_width': 32, 'bert.encoder.layer.0.attention.output.dense_exp_width': 4, 'bert.encoder.layer.0.attention.output.dense_exp_bias': 15, 'bert.encoder.layer.0.intermediate.dense_quantize': True, 'bert.encoder.layer.0.intermediate.dense_width': 16, 'bert.encoder.layer.0.intermediate.dense_exp_width': 4, 'bert.encoder.layer.0.intermediate.dense_exp_bias': 7, 'bert.encoder.layer.0.output.dense_quantize': False, 'bert.encoder.layer.1.attention.self.query_quantize': False, 'bert.encoder.layer.1.attention.self.key_quantize': True, 'bert.encoder.layer.1.attention.self.key_width': 32, 'bert.encoder.layer.1.attention.self.key_exp_width': 2, 'bert.encoder.layer.1.attention.self.key_exp_bias': 15, 'bert.encoder.layer.1.attention.self.value_quantize': True, 'bert.encoder.layer.1.attention.self.value_width': 8, 'bert.encoder.layer.1.attention.self.value_exp_width': 2, 'bert.encoder.layer.1.attention.self.value_exp_bias': 3, 'bert.encoder.layer.1.attention.output.dense_quantize': True, 'bert.encoder.layer.1.attention.output.dense_width': 16, 'bert.encoder.layer.1.attention.output.dense_exp_width': 2, 'bert.encoder.layer.1.attention.output.dense_exp_bias': 3, 'bert.encoder.layer.1.intermediate.dense_quantize': True, 'bert.encoder.layer.1.intermediate.dense_width': 16, 'bert.encoder.layer.1.intermediate.dense_exp_width': 4, 'bert.encoder.layer.1.intermediate.dense_exp_bias': 7, 'bert.encoder.layer.1.output.dense_quantize': False, 'bert.pooler.dense_quantize': False, 'classifier_quantize': False}. Best is trial 14 with value: 0.85832.\n",
      "/vol/bitbucket/ug22/adls/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 01:09, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.634500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.466100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.413300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.383500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.362900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.374300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:42]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-02-04 15:27:59,842] Trial 17 finished with value: 0.85396 and parameters: {'bert.encoder.layer.0.attention.self.query_quantize': False, 'bert.encoder.layer.0.attention.self.key_quantize': True, 'bert.encoder.layer.0.attention.self.key_width': 8, 'bert.encoder.layer.0.attention.self.key_exp_width': 2, 'bert.encoder.layer.0.attention.self.key_exp_bias': 3, 'bert.encoder.layer.0.attention.self.value_quantize': False, 'bert.encoder.layer.0.attention.output.dense_quantize': True, 'bert.encoder.layer.0.attention.output.dense_width': 32, 'bert.encoder.layer.0.attention.output.dense_exp_width': 8, 'bert.encoder.layer.0.attention.output.dense_exp_bias': 3, 'bert.encoder.layer.0.intermediate.dense_quantize': True, 'bert.encoder.layer.0.intermediate.dense_width': 8, 'bert.encoder.layer.0.intermediate.dense_exp_width': 8, 'bert.encoder.layer.0.intermediate.dense_exp_bias': 15, 'bert.encoder.layer.0.output.dense_quantize': False, 'bert.encoder.layer.1.attention.self.query_quantize': False, 'bert.encoder.layer.1.attention.self.key_quantize': True, 'bert.encoder.layer.1.attention.self.key_width': 8, 'bert.encoder.layer.1.attention.self.key_exp_width': 2, 'bert.encoder.layer.1.attention.self.key_exp_bias': 3, 'bert.encoder.layer.1.attention.self.value_quantize': True, 'bert.encoder.layer.1.attention.self.value_width': 16, 'bert.encoder.layer.1.attention.self.value_exp_width': 4, 'bert.encoder.layer.1.attention.self.value_exp_bias': 7, 'bert.encoder.layer.1.attention.output.dense_quantize': True, 'bert.encoder.layer.1.attention.output.dense_width': 32, 'bert.encoder.layer.1.attention.output.dense_exp_width': 8, 'bert.encoder.layer.1.attention.output.dense_exp_bias': 3, 'bert.encoder.layer.1.intermediate.dense_quantize': False, 'bert.encoder.layer.1.output.dense_quantize': False, 'bert.pooler.dense_quantize': False, 'classifier_quantize': False}. Best is trial 14 with value: 0.85832.\n",
      "/vol/bitbucket/ug22/adls/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 01:04, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.693300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.666000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.626800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.596200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.571700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.568300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:46]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-02-04 15:29:51,853] Trial 18 finished with value: 0.76244 and parameters: {'bert.encoder.layer.0.attention.self.query_quantize': False, 'bert.encoder.layer.0.attention.self.key_quantize': True, 'bert.encoder.layer.0.attention.self.key_width': 32, 'bert.encoder.layer.0.attention.self.key_exp_width': 4, 'bert.encoder.layer.0.attention.self.key_exp_bias': 7, 'bert.encoder.layer.0.attention.self.value_quantize': False, 'bert.encoder.layer.0.attention.output.dense_quantize': True, 'bert.encoder.layer.0.attention.output.dense_width': 8, 'bert.encoder.layer.0.attention.output.dense_exp_width': 2, 'bert.encoder.layer.0.attention.output.dense_exp_bias': 7, 'bert.encoder.layer.0.intermediate.dense_quantize': True, 'bert.encoder.layer.0.intermediate.dense_width': 32, 'bert.encoder.layer.0.intermediate.dense_exp_width': 2, 'bert.encoder.layer.0.intermediate.dense_exp_bias': 7, 'bert.encoder.layer.0.output.dense_quantize': False, 'bert.encoder.layer.1.attention.self.query_quantize': False, 'bert.encoder.layer.1.attention.self.key_quantize': False, 'bert.encoder.layer.1.attention.self.value_quantize': True, 'bert.encoder.layer.1.attention.self.value_width': 8, 'bert.encoder.layer.1.attention.self.value_exp_width': 2, 'bert.encoder.layer.1.attention.self.value_exp_bias': 3, 'bert.encoder.layer.1.attention.output.dense_quantize': True, 'bert.encoder.layer.1.attention.output.dense_width': 16, 'bert.encoder.layer.1.attention.output.dense_exp_width': 2, 'bert.encoder.layer.1.attention.output.dense_exp_bias': 15, 'bert.encoder.layer.1.intermediate.dense_quantize': True, 'bert.encoder.layer.1.intermediate.dense_width': 16, 'bert.encoder.layer.1.intermediate.dense_exp_width': 2, 'bert.encoder.layer.1.intermediate.dense_exp_bias': 7, 'bert.encoder.layer.1.output.dense_quantize': False, 'bert.pooler.dense_quantize': True, 'bert.pooler.dense_width': 16, 'bert.pooler.dense_exp_width': 4, 'bert.pooler.dense_exp_bias': 7, 'classifier_quantize': False}. Best is trial 14 with value: 0.85832.\n",
      "/vol/bitbucket/ug22/adls/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:59, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.598600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.431300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.402500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.372600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.360100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.365900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:41]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-02-04 15:31:33,776] Trial 19 finished with value: 0.85992 and parameters: {'bert.encoder.layer.0.attention.self.query_quantize': False, 'bert.encoder.layer.0.attention.self.key_quantize': True, 'bert.encoder.layer.0.attention.self.key_width': 32, 'bert.encoder.layer.0.attention.self.key_exp_width': 4, 'bert.encoder.layer.0.attention.self.key_exp_bias': 3, 'bert.encoder.layer.0.attention.self.value_quantize': False, 'bert.encoder.layer.0.attention.output.dense_quantize': True, 'bert.encoder.layer.0.attention.output.dense_width': 32, 'bert.encoder.layer.0.attention.output.dense_exp_width': 4, 'bert.encoder.layer.0.attention.output.dense_exp_bias': 15, 'bert.encoder.layer.0.intermediate.dense_quantize': True, 'bert.encoder.layer.0.intermediate.dense_width': 8, 'bert.encoder.layer.0.intermediate.dense_exp_width': 4, 'bert.encoder.layer.0.intermediate.dense_exp_bias': 7, 'bert.encoder.layer.0.output.dense_quantize': False, 'bert.encoder.layer.1.attention.self.query_quantize': False, 'bert.encoder.layer.1.attention.self.key_quantize': True, 'bert.encoder.layer.1.attention.self.key_width': 16, 'bert.encoder.layer.1.attention.self.key_exp_width': 2, 'bert.encoder.layer.1.attention.self.key_exp_bias': 15, 'bert.encoder.layer.1.attention.self.value_quantize': True, 'bert.encoder.layer.1.attention.self.value_width': 16, 'bert.encoder.layer.1.attention.self.value_exp_width': 4, 'bert.encoder.layer.1.attention.self.value_exp_bias': 3, 'bert.encoder.layer.1.attention.output.dense_quantize': True, 'bert.encoder.layer.1.attention.output.dense_width': 32, 'bert.encoder.layer.1.attention.output.dense_exp_width': 8, 'bert.encoder.layer.1.attention.output.dense_exp_bias': 3, 'bert.encoder.layer.1.intermediate.dense_quantize': False, 'bert.encoder.layer.1.output.dense_quantize': False, 'bert.pooler.dense_quantize': False, 'classifier_quantize': False}. Best is trial 19 with value: 0.85992.\n",
      "[I 2026-02-04 15:31:33,782] A new study created in memory with name: bert-Log-search\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MinifloatIEEE - Best: 0.8599\n",
      "\n",
      "============================================================\n",
      "Searching: Log\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vol/bitbucket/ug22/adls/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:38, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.692800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.677000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.648500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.624300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.621900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.612200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:20]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-02-04 15:32:34,059] Trial 0 finished with value: 0.68764 and parameters: {'bert.encoder.layer.0.attention.self.query_quantize': True, 'bert.encoder.layer.0.attention.self.query_width': 16, 'bert.encoder.layer.0.attention.self.query_exp_bias': 3, 'bert.encoder.layer.0.attention.self.key_quantize': True, 'bert.encoder.layer.0.attention.self.key_width': 16, 'bert.encoder.layer.0.attention.self.key_exp_bias': 3, 'bert.encoder.layer.0.attention.self.value_quantize': True, 'bert.encoder.layer.0.attention.self.value_width': 8, 'bert.encoder.layer.0.attention.self.value_exp_bias': 3, 'bert.encoder.layer.0.attention.output.dense_quantize': False, 'bert.encoder.layer.0.intermediate.dense_quantize': True, 'bert.encoder.layer.0.intermediate.dense_width': 16, 'bert.encoder.layer.0.intermediate.dense_exp_bias': 3, 'bert.encoder.layer.0.output.dense_quantize': False, 'bert.encoder.layer.1.attention.self.query_quantize': True, 'bert.encoder.layer.1.attention.self.query_width': 16, 'bert.encoder.layer.1.attention.self.query_exp_bias': 3, 'bert.encoder.layer.1.attention.self.key_quantize': True, 'bert.encoder.layer.1.attention.self.key_width': 32, 'bert.encoder.layer.1.attention.self.key_exp_bias': 3, 'bert.encoder.layer.1.attention.self.value_quantize': False, 'bert.encoder.layer.1.attention.output.dense_quantize': False, 'bert.encoder.layer.1.intermediate.dense_quantize': True, 'bert.encoder.layer.1.intermediate.dense_width': 16, 'bert.encoder.layer.1.intermediate.dense_exp_bias': 7, 'bert.encoder.layer.1.output.dense_quantize': True, 'bert.encoder.layer.1.output.dense_width': 8, 'bert.encoder.layer.1.output.dense_exp_bias': 7, 'bert.pooler.dense_quantize': True, 'bert.pooler.dense_width': 32, 'bert.pooler.dense_exp_bias': 7, 'classifier_quantize': False}. Best is trial 0 with value: 0.68764.\n",
      "/vol/bitbucket/ug22/adls/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:34, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.645900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.492200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.435000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.401200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.383500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.379900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:17]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-02-04 15:33:27,699] Trial 1 finished with value: 0.84828 and parameters: {'bert.encoder.layer.0.attention.self.query_quantize': True, 'bert.encoder.layer.0.attention.self.query_width': 16, 'bert.encoder.layer.0.attention.self.query_exp_bias': 3, 'bert.encoder.layer.0.attention.self.key_quantize': True, 'bert.encoder.layer.0.attention.self.key_width': 8, 'bert.encoder.layer.0.attention.self.key_exp_bias': 15, 'bert.encoder.layer.0.attention.self.value_quantize': False, 'bert.encoder.layer.0.attention.output.dense_quantize': True, 'bert.encoder.layer.0.attention.output.dense_width': 32, 'bert.encoder.layer.0.attention.output.dense_exp_bias': 15, 'bert.encoder.layer.0.intermediate.dense_quantize': False, 'bert.encoder.layer.0.output.dense_quantize': False, 'bert.encoder.layer.1.attention.self.query_quantize': True, 'bert.encoder.layer.1.attention.self.query_width': 32, 'bert.encoder.layer.1.attention.self.query_exp_bias': 15, 'bert.encoder.layer.1.attention.self.key_quantize': True, 'bert.encoder.layer.1.attention.self.key_width': 8, 'bert.encoder.layer.1.attention.self.key_exp_bias': 7, 'bert.encoder.layer.1.attention.self.value_quantize': False, 'bert.encoder.layer.1.attention.output.dense_quantize': False, 'bert.encoder.layer.1.intermediate.dense_quantize': False, 'bert.encoder.layer.1.output.dense_quantize': True, 'bert.encoder.layer.1.output.dense_width': 32, 'bert.encoder.layer.1.output.dense_exp_bias': 15, 'bert.pooler.dense_quantize': False, 'classifier_quantize': False}. Best is trial 1 with value: 0.84828.\n",
      "/vol/bitbucket/ug22/adls/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:34, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.628800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.469200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.412800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.386100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.370500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.372300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:17]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-02-04 15:34:20,741] Trial 2 finished with value: 0.85376 and parameters: {'bert.encoder.layer.0.attention.self.query_quantize': True, 'bert.encoder.layer.0.attention.self.query_width': 16, 'bert.encoder.layer.0.attention.self.query_exp_bias': 15, 'bert.encoder.layer.0.attention.self.key_quantize': False, 'bert.encoder.layer.0.attention.self.value_quantize': False, 'bert.encoder.layer.0.attention.output.dense_quantize': True, 'bert.encoder.layer.0.attention.output.dense_width': 32, 'bert.encoder.layer.0.attention.output.dense_exp_bias': 15, 'bert.encoder.layer.0.intermediate.dense_quantize': False, 'bert.encoder.layer.0.output.dense_quantize': True, 'bert.encoder.layer.0.output.dense_width': 8, 'bert.encoder.layer.0.output.dense_exp_bias': 15, 'bert.encoder.layer.1.attention.self.query_quantize': False, 'bert.encoder.layer.1.attention.self.key_quantize': True, 'bert.encoder.layer.1.attention.self.key_width': 8, 'bert.encoder.layer.1.attention.self.key_exp_bias': 15, 'bert.encoder.layer.1.attention.self.value_quantize': False, 'bert.encoder.layer.1.attention.output.dense_quantize': False, 'bert.encoder.layer.1.intermediate.dense_quantize': True, 'bert.encoder.layer.1.intermediate.dense_width': 16, 'bert.encoder.layer.1.intermediate.dense_exp_bias': 15, 'bert.encoder.layer.1.output.dense_quantize': True, 'bert.encoder.layer.1.output.dense_width': 32, 'bert.encoder.layer.1.output.dense_exp_bias': 3, 'bert.pooler.dense_quantize': False, 'classifier_quantize': False}. Best is trial 2 with value: 0.85376.\n",
      "/vol/bitbucket/ug22/adls/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:38, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.678000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.584000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.489000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.432300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.405500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:20]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-02-04 15:35:20,418] Trial 3 finished with value: 0.8336 and parameters: {'bert.encoder.layer.0.attention.self.query_quantize': True, 'bert.encoder.layer.0.attention.self.query_width': 8, 'bert.encoder.layer.0.attention.self.query_exp_bias': 15, 'bert.encoder.layer.0.attention.self.key_quantize': True, 'bert.encoder.layer.0.attention.self.key_width': 8, 'bert.encoder.layer.0.attention.self.key_exp_bias': 15, 'bert.encoder.layer.0.attention.self.value_quantize': True, 'bert.encoder.layer.0.attention.self.value_width': 16, 'bert.encoder.layer.0.attention.self.value_exp_bias': 7, 'bert.encoder.layer.0.attention.output.dense_quantize': True, 'bert.encoder.layer.0.attention.output.dense_width': 32, 'bert.encoder.layer.0.attention.output.dense_exp_bias': 15, 'bert.encoder.layer.0.intermediate.dense_quantize': True, 'bert.encoder.layer.0.intermediate.dense_width': 16, 'bert.encoder.layer.0.intermediate.dense_exp_bias': 3, 'bert.encoder.layer.0.output.dense_quantize': True, 'bert.encoder.layer.0.output.dense_width': 16, 'bert.encoder.layer.0.output.dense_exp_bias': 7, 'bert.encoder.layer.1.attention.self.query_quantize': True, 'bert.encoder.layer.1.attention.self.query_width': 32, 'bert.encoder.layer.1.attention.self.query_exp_bias': 15, 'bert.encoder.layer.1.attention.self.key_quantize': True, 'bert.encoder.layer.1.attention.self.key_width': 32, 'bert.encoder.layer.1.attention.self.key_exp_bias': 3, 'bert.encoder.layer.1.attention.self.value_quantize': False, 'bert.encoder.layer.1.attention.output.dense_quantize': False, 'bert.encoder.layer.1.intermediate.dense_quantize': False, 'bert.encoder.layer.1.output.dense_quantize': True, 'bert.encoder.layer.1.output.dense_width': 32, 'bert.encoder.layer.1.output.dense_exp_bias': 7, 'bert.pooler.dense_quantize': False, 'classifier_quantize': False}. Best is trial 2 with value: 0.85376.\n",
      "/vol/bitbucket/ug22/adls/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:36, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.652400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.500200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.433100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.406600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.389200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.401000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:19]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-02-04 15:36:18,615] Trial 4 finished with value: 0.8404 and parameters: {'bert.encoder.layer.0.attention.self.query_quantize': False, 'bert.encoder.layer.0.attention.self.key_quantize': True, 'bert.encoder.layer.0.attention.self.key_width': 8, 'bert.encoder.layer.0.attention.self.key_exp_bias': 3, 'bert.encoder.layer.0.attention.self.value_quantize': True, 'bert.encoder.layer.0.attention.self.value_width': 8, 'bert.encoder.layer.0.attention.self.value_exp_bias': 7, 'bert.encoder.layer.0.attention.output.dense_quantize': False, 'bert.encoder.layer.0.intermediate.dense_quantize': True, 'bert.encoder.layer.0.intermediate.dense_width': 32, 'bert.encoder.layer.0.intermediate.dense_exp_bias': 7, 'bert.encoder.layer.0.output.dense_quantize': True, 'bert.encoder.layer.0.output.dense_width': 8, 'bert.encoder.layer.0.output.dense_exp_bias': 15, 'bert.encoder.layer.1.attention.self.query_quantize': False, 'bert.encoder.layer.1.attention.self.key_quantize': False, 'bert.encoder.layer.1.attention.self.value_quantize': True, 'bert.encoder.layer.1.attention.self.value_width': 16, 'bert.encoder.layer.1.attention.self.value_exp_bias': 3, 'bert.encoder.layer.1.attention.output.dense_quantize': False, 'bert.encoder.layer.1.intermediate.dense_quantize': False, 'bert.encoder.layer.1.output.dense_quantize': True, 'bert.encoder.layer.1.output.dense_width': 32, 'bert.encoder.layer.1.output.dense_exp_bias': 7, 'bert.pooler.dense_quantize': True, 'bert.pooler.dense_width': 8, 'bert.pooler.dense_exp_bias': 15, 'classifier_quantize': True, 'classifier_width': 16, 'classifier_exp_bias': 15}. Best is trial 2 with value: 0.85376.\n",
      "/vol/bitbucket/ug22/adls/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:35, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.665700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.531300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.452600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.412100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.392000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.390100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:18]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-02-04 15:37:14,215] Trial 5 finished with value: 0.84368 and parameters: {'bert.encoder.layer.0.attention.self.query_quantize': True, 'bert.encoder.layer.0.attention.self.query_width': 8, 'bert.encoder.layer.0.attention.self.query_exp_bias': 15, 'bert.encoder.layer.0.attention.self.key_quantize': True, 'bert.encoder.layer.0.attention.self.key_width': 16, 'bert.encoder.layer.0.attention.self.key_exp_bias': 15, 'bert.encoder.layer.0.attention.self.value_quantize': False, 'bert.encoder.layer.0.attention.output.dense_quantize': False, 'bert.encoder.layer.0.intermediate.dense_quantize': False, 'bert.encoder.layer.0.output.dense_quantize': True, 'bert.encoder.layer.0.output.dense_width': 16, 'bert.encoder.layer.0.output.dense_exp_bias': 3, 'bert.encoder.layer.1.attention.self.query_quantize': True, 'bert.encoder.layer.1.attention.self.query_width': 16, 'bert.encoder.layer.1.attention.self.query_exp_bias': 15, 'bert.encoder.layer.1.attention.self.key_quantize': False, 'bert.encoder.layer.1.attention.self.value_quantize': True, 'bert.encoder.layer.1.attention.self.value_width': 16, 'bert.encoder.layer.1.attention.self.value_exp_bias': 7, 'bert.encoder.layer.1.attention.output.dense_quantize': True, 'bert.encoder.layer.1.attention.output.dense_width': 8, 'bert.encoder.layer.1.attention.output.dense_exp_bias': 15, 'bert.encoder.layer.1.intermediate.dense_quantize': False, 'bert.encoder.layer.1.output.dense_quantize': True, 'bert.encoder.layer.1.output.dense_width': 32, 'bert.encoder.layer.1.output.dense_exp_bias': 15, 'bert.pooler.dense_quantize': False, 'classifier_quantize': False}. Best is trial 2 with value: 0.85376.\n",
      "/vol/bitbucket/ug22/adls/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:34, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.627000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.451700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.415800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.382100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.361000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.369100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:17]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-02-04 15:38:07,608] Trial 6 finished with value: 0.85432 and parameters: {'bert.encoder.layer.0.attention.self.query_quantize': True, 'bert.encoder.layer.0.attention.self.query_width': 32, 'bert.encoder.layer.0.attention.self.query_exp_bias': 7, 'bert.encoder.layer.0.attention.self.key_quantize': False, 'bert.encoder.layer.0.attention.self.value_quantize': False, 'bert.encoder.layer.0.attention.output.dense_quantize': True, 'bert.encoder.layer.0.attention.output.dense_width': 8, 'bert.encoder.layer.0.attention.output.dense_exp_bias': 15, 'bert.encoder.layer.0.intermediate.dense_quantize': True, 'bert.encoder.layer.0.intermediate.dense_width': 16, 'bert.encoder.layer.0.intermediate.dense_exp_bias': 7, 'bert.encoder.layer.0.output.dense_quantize': False, 'bert.encoder.layer.1.attention.self.query_quantize': True, 'bert.encoder.layer.1.attention.self.query_width': 16, 'bert.encoder.layer.1.attention.self.query_exp_bias': 7, 'bert.encoder.layer.1.attention.self.key_quantize': False, 'bert.encoder.layer.1.attention.self.value_quantize': False, 'bert.encoder.layer.1.attention.output.dense_quantize': False, 'bert.encoder.layer.1.intermediate.dense_quantize': True, 'bert.encoder.layer.1.intermediate.dense_width': 32, 'bert.encoder.layer.1.intermediate.dense_exp_bias': 7, 'bert.encoder.layer.1.output.dense_quantize': False, 'bert.pooler.dense_quantize': True, 'bert.pooler.dense_width': 16, 'bert.pooler.dense_exp_bias': 7, 'classifier_quantize': False}. Best is trial 6 with value: 0.85432.\n",
      "/vol/bitbucket/ug22/adls/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:31, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.654300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.510300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.434500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.393000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.379200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.375800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:14]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-02-04 15:38:54,200] Trial 7 finished with value: 0.84836 and parameters: {'bert.encoder.layer.0.attention.self.query_quantize': False, 'bert.encoder.layer.0.attention.self.key_quantize': False, 'bert.encoder.layer.0.attention.self.value_quantize': False, 'bert.encoder.layer.0.attention.output.dense_quantize': False, 'bert.encoder.layer.0.intermediate.dense_quantize': True, 'bert.encoder.layer.0.intermediate.dense_width': 8, 'bert.encoder.layer.0.intermediate.dense_exp_bias': 7, 'bert.encoder.layer.0.output.dense_quantize': False, 'bert.encoder.layer.1.attention.self.query_quantize': False, 'bert.encoder.layer.1.attention.self.key_quantize': True, 'bert.encoder.layer.1.attention.self.key_width': 8, 'bert.encoder.layer.1.attention.self.key_exp_bias': 3, 'bert.encoder.layer.1.attention.self.value_quantize': False, 'bert.encoder.layer.1.attention.output.dense_quantize': False, 'bert.encoder.layer.1.intermediate.dense_quantize': False, 'bert.encoder.layer.1.output.dense_quantize': True, 'bert.encoder.layer.1.output.dense_width': 16, 'bert.encoder.layer.1.output.dense_exp_bias': 3, 'bert.pooler.dense_quantize': False, 'classifier_quantize': False}. Best is trial 6 with value: 0.85432.\n",
      "/vol/bitbucket/ug22/adls/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:31, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.623200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.470800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.421400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.390300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.370700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.376900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:15]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-02-04 15:39:42,528] Trial 8 finished with value: 0.85284 and parameters: {'bert.encoder.layer.0.attention.self.query_quantize': True, 'bert.encoder.layer.0.attention.self.query_width': 16, 'bert.encoder.layer.0.attention.self.query_exp_bias': 15, 'bert.encoder.layer.0.attention.self.key_quantize': False, 'bert.encoder.layer.0.attention.self.value_quantize': False, 'bert.encoder.layer.0.attention.output.dense_quantize': False, 'bert.encoder.layer.0.intermediate.dense_quantize': False, 'bert.encoder.layer.0.output.dense_quantize': False, 'bert.encoder.layer.1.attention.self.query_quantize': False, 'bert.encoder.layer.1.attention.self.key_quantize': False, 'bert.encoder.layer.1.attention.self.value_quantize': True, 'bert.encoder.layer.1.attention.self.value_width': 32, 'bert.encoder.layer.1.attention.self.value_exp_bias': 3, 'bert.encoder.layer.1.attention.output.dense_quantize': False, 'bert.encoder.layer.1.intermediate.dense_quantize': True, 'bert.encoder.layer.1.intermediate.dense_width': 16, 'bert.encoder.layer.1.intermediate.dense_exp_bias': 7, 'bert.encoder.layer.1.output.dense_quantize': True, 'bert.encoder.layer.1.output.dense_width': 16, 'bert.encoder.layer.1.output.dense_exp_bias': 7, 'bert.pooler.dense_quantize': False, 'classifier_quantize': False}. Best is trial 6 with value: 0.85432.\n",
      "/vol/bitbucket/ug22/adls/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:35, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.682700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.617600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.508600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.449900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.428100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.435100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:18]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-02-04 15:40:37,579] Trial 9 finished with value: 0.83072 and parameters: {'bert.encoder.layer.0.attention.self.query_quantize': False, 'bert.encoder.layer.0.attention.self.key_quantize': False, 'bert.encoder.layer.0.attention.self.value_quantize': False, 'bert.encoder.layer.0.attention.output.dense_quantize': False, 'bert.encoder.layer.0.intermediate.dense_quantize': True, 'bert.encoder.layer.0.intermediate.dense_width': 8, 'bert.encoder.layer.0.intermediate.dense_exp_bias': 7, 'bert.encoder.layer.0.output.dense_quantize': True, 'bert.encoder.layer.0.output.dense_width': 32, 'bert.encoder.layer.0.output.dense_exp_bias': 3, 'bert.encoder.layer.1.attention.self.query_quantize': True, 'bert.encoder.layer.1.attention.self.query_width': 8, 'bert.encoder.layer.1.attention.self.query_exp_bias': 3, 'bert.encoder.layer.1.attention.self.key_quantize': True, 'bert.encoder.layer.1.attention.self.key_width': 32, 'bert.encoder.layer.1.attention.self.key_exp_bias': 15, 'bert.encoder.layer.1.attention.self.value_quantize': True, 'bert.encoder.layer.1.attention.self.value_width': 8, 'bert.encoder.layer.1.attention.self.value_exp_bias': 7, 'bert.encoder.layer.1.attention.output.dense_quantize': True, 'bert.encoder.layer.1.attention.output.dense_width': 32, 'bert.encoder.layer.1.attention.output.dense_exp_bias': 15, 'bert.encoder.layer.1.intermediate.dense_quantize': False, 'bert.encoder.layer.1.output.dense_quantize': False, 'bert.pooler.dense_quantize': False, 'classifier_quantize': True, 'classifier_width': 8, 'classifier_exp_bias': 15}. Best is trial 6 with value: 0.85432.\n",
      "/vol/bitbucket/ug22/adls/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:37, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.623900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.466900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.424800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.390000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.376700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.378500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:19]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-02-04 15:41:35,629] Trial 10 finished with value: 0.85436 and parameters: {'bert.encoder.layer.0.attention.self.query_quantize': False, 'bert.encoder.layer.0.attention.self.key_quantize': False, 'bert.encoder.layer.0.attention.self.value_quantize': True, 'bert.encoder.layer.0.attention.self.value_width': 32, 'bert.encoder.layer.0.attention.self.value_exp_bias': 15, 'bert.encoder.layer.0.attention.output.dense_quantize': True, 'bert.encoder.layer.0.attention.output.dense_width': 8, 'bert.encoder.layer.0.attention.output.dense_exp_bias': 7, 'bert.encoder.layer.0.intermediate.dense_quantize': True, 'bert.encoder.layer.0.intermediate.dense_width': 16, 'bert.encoder.layer.0.intermediate.dense_exp_bias': 15, 'bert.encoder.layer.0.output.dense_quantize': False, 'bert.encoder.layer.1.attention.self.query_quantize': True, 'bert.encoder.layer.1.attention.self.query_width': 16, 'bert.encoder.layer.1.attention.self.query_exp_bias': 7, 'bert.encoder.layer.1.attention.self.key_quantize': False, 'bert.encoder.layer.1.attention.self.value_quantize': False, 'bert.encoder.layer.1.attention.output.dense_quantize': True, 'bert.encoder.layer.1.attention.output.dense_width': 16, 'bert.encoder.layer.1.attention.output.dense_exp_bias': 3, 'bert.encoder.layer.1.intermediate.dense_quantize': True, 'bert.encoder.layer.1.intermediate.dense_width': 32, 'bert.encoder.layer.1.intermediate.dense_exp_bias': 3, 'bert.encoder.layer.1.output.dense_quantize': False, 'bert.pooler.dense_quantize': True, 'bert.pooler.dense_width': 16, 'bert.pooler.dense_exp_bias': 3, 'classifier_quantize': True, 'classifier_width': 32, 'classifier_exp_bias': 7}. Best is trial 10 with value: 0.85436.\n",
      "/vol/bitbucket/ug22/adls/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:37, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.623900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.466900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.424800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.390000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.376700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.378500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:19]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-02-04 15:42:33,197] Trial 11 finished with value: 0.85436 and parameters: {'bert.encoder.layer.0.attention.self.query_quantize': False, 'bert.encoder.layer.0.attention.self.key_quantize': False, 'bert.encoder.layer.0.attention.self.value_quantize': True, 'bert.encoder.layer.0.attention.self.value_width': 32, 'bert.encoder.layer.0.attention.self.value_exp_bias': 15, 'bert.encoder.layer.0.attention.output.dense_quantize': True, 'bert.encoder.layer.0.attention.output.dense_width': 8, 'bert.encoder.layer.0.attention.output.dense_exp_bias': 7, 'bert.encoder.layer.0.intermediate.dense_quantize': True, 'bert.encoder.layer.0.intermediate.dense_width': 16, 'bert.encoder.layer.0.intermediate.dense_exp_bias': 15, 'bert.encoder.layer.0.output.dense_quantize': False, 'bert.encoder.layer.1.attention.self.query_quantize': True, 'bert.encoder.layer.1.attention.self.query_width': 16, 'bert.encoder.layer.1.attention.self.query_exp_bias': 7, 'bert.encoder.layer.1.attention.self.key_quantize': False, 'bert.encoder.layer.1.attention.self.value_quantize': False, 'bert.encoder.layer.1.attention.output.dense_quantize': True, 'bert.encoder.layer.1.attention.output.dense_width': 16, 'bert.encoder.layer.1.attention.output.dense_exp_bias': 3, 'bert.encoder.layer.1.intermediate.dense_quantize': True, 'bert.encoder.layer.1.intermediate.dense_width': 32, 'bert.encoder.layer.1.intermediate.dense_exp_bias': 3, 'bert.encoder.layer.1.output.dense_quantize': False, 'bert.pooler.dense_quantize': True, 'bert.pooler.dense_width': 16, 'bert.pooler.dense_exp_bias': 3, 'classifier_quantize': True, 'classifier_width': 32, 'classifier_exp_bias': 7}. Best is trial 10 with value: 0.85436.\n",
      "/vol/bitbucket/ug22/adls/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:36, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.623900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.466900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.424800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.390000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.376700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.378500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:19]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-02-04 15:43:30,601] Trial 12 finished with value: 0.85436 and parameters: {'bert.encoder.layer.0.attention.self.query_quantize': False, 'bert.encoder.layer.0.attention.self.key_quantize': False, 'bert.encoder.layer.0.attention.self.value_quantize': True, 'bert.encoder.layer.0.attention.self.value_width': 32, 'bert.encoder.layer.0.attention.self.value_exp_bias': 15, 'bert.encoder.layer.0.attention.output.dense_quantize': True, 'bert.encoder.layer.0.attention.output.dense_width': 8, 'bert.encoder.layer.0.attention.output.dense_exp_bias': 7, 'bert.encoder.layer.0.intermediate.dense_quantize': True, 'bert.encoder.layer.0.intermediate.dense_width': 16, 'bert.encoder.layer.0.intermediate.dense_exp_bias': 15, 'bert.encoder.layer.0.output.dense_quantize': False, 'bert.encoder.layer.1.attention.self.query_quantize': True, 'bert.encoder.layer.1.attention.self.query_width': 16, 'bert.encoder.layer.1.attention.self.query_exp_bias': 7, 'bert.encoder.layer.1.attention.self.key_quantize': False, 'bert.encoder.layer.1.attention.self.value_quantize': False, 'bert.encoder.layer.1.attention.output.dense_quantize': True, 'bert.encoder.layer.1.attention.output.dense_width': 16, 'bert.encoder.layer.1.attention.output.dense_exp_bias': 3, 'bert.encoder.layer.1.intermediate.dense_quantize': True, 'bert.encoder.layer.1.intermediate.dense_width': 32, 'bert.encoder.layer.1.intermediate.dense_exp_bias': 3, 'bert.encoder.layer.1.output.dense_quantize': False, 'bert.pooler.dense_quantize': True, 'bert.pooler.dense_width': 16, 'bert.pooler.dense_exp_bias': 3, 'classifier_quantize': True, 'classifier_width': 32, 'classifier_exp_bias': 7}. Best is trial 10 with value: 0.85436.\n",
      "/vol/bitbucket/ug22/adls/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:37, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.623900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.466900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.424800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.390000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.376700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.378500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:19]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-02-04 15:44:28,072] Trial 13 finished with value: 0.85436 and parameters: {'bert.encoder.layer.0.attention.self.query_quantize': False, 'bert.encoder.layer.0.attention.self.key_quantize': False, 'bert.encoder.layer.0.attention.self.value_quantize': True, 'bert.encoder.layer.0.attention.self.value_width': 32, 'bert.encoder.layer.0.attention.self.value_exp_bias': 15, 'bert.encoder.layer.0.attention.output.dense_quantize': True, 'bert.encoder.layer.0.attention.output.dense_width': 16, 'bert.encoder.layer.0.attention.output.dense_exp_bias': 7, 'bert.encoder.layer.0.intermediate.dense_quantize': True, 'bert.encoder.layer.0.intermediate.dense_width': 16, 'bert.encoder.layer.0.intermediate.dense_exp_bias': 15, 'bert.encoder.layer.0.output.dense_quantize': False, 'bert.encoder.layer.1.attention.self.query_quantize': True, 'bert.encoder.layer.1.attention.self.query_width': 16, 'bert.encoder.layer.1.attention.self.query_exp_bias': 7, 'bert.encoder.layer.1.attention.self.key_quantize': False, 'bert.encoder.layer.1.attention.self.value_quantize': False, 'bert.encoder.layer.1.attention.output.dense_quantize': True, 'bert.encoder.layer.1.attention.output.dense_width': 16, 'bert.encoder.layer.1.attention.output.dense_exp_bias': 3, 'bert.encoder.layer.1.intermediate.dense_quantize': True, 'bert.encoder.layer.1.intermediate.dense_width': 32, 'bert.encoder.layer.1.intermediate.dense_exp_bias': 3, 'bert.encoder.layer.1.output.dense_quantize': False, 'bert.pooler.dense_quantize': True, 'bert.pooler.dense_width': 16, 'bert.pooler.dense_exp_bias': 3, 'classifier_quantize': True, 'classifier_width': 32, 'classifier_exp_bias': 7}. Best is trial 10 with value: 0.85436.\n",
      "/vol/bitbucket/ug22/adls/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:36, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.623900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.466900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.424800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.390000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.376700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.378500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:19]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-02-04 15:45:25,306] Trial 14 finished with value: 0.85436 and parameters: {'bert.encoder.layer.0.attention.self.query_quantize': False, 'bert.encoder.layer.0.attention.self.key_quantize': False, 'bert.encoder.layer.0.attention.self.value_quantize': True, 'bert.encoder.layer.0.attention.self.value_width': 32, 'bert.encoder.layer.0.attention.self.value_exp_bias': 15, 'bert.encoder.layer.0.attention.output.dense_quantize': True, 'bert.encoder.layer.0.attention.output.dense_width': 8, 'bert.encoder.layer.0.attention.output.dense_exp_bias': 7, 'bert.encoder.layer.0.intermediate.dense_quantize': True, 'bert.encoder.layer.0.intermediate.dense_width': 32, 'bert.encoder.layer.0.intermediate.dense_exp_bias': 15, 'bert.encoder.layer.0.output.dense_quantize': False, 'bert.encoder.layer.1.attention.self.query_quantize': True, 'bert.encoder.layer.1.attention.self.query_width': 8, 'bert.encoder.layer.1.attention.self.query_exp_bias': 7, 'bert.encoder.layer.1.attention.self.key_quantize': False, 'bert.encoder.layer.1.attention.self.value_quantize': False, 'bert.encoder.layer.1.attention.output.dense_quantize': True, 'bert.encoder.layer.1.attention.output.dense_width': 16, 'bert.encoder.layer.1.attention.output.dense_exp_bias': 3, 'bert.encoder.layer.1.intermediate.dense_quantize': True, 'bert.encoder.layer.1.intermediate.dense_width': 8, 'bert.encoder.layer.1.intermediate.dense_exp_bias': 3, 'bert.encoder.layer.1.output.dense_quantize': False, 'bert.pooler.dense_quantize': True, 'bert.pooler.dense_width': 16, 'bert.pooler.dense_exp_bias': 3, 'classifier_quantize': True, 'classifier_width': 32, 'classifier_exp_bias': 7}. Best is trial 10 with value: 0.85436.\n",
      "/vol/bitbucket/ug22/adls/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:37, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.669200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.526500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.457500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.420300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.393800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.403500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:19]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-02-04 15:46:22,918] Trial 15 finished with value: 0.84692 and parameters: {'bert.encoder.layer.0.attention.self.query_quantize': False, 'bert.encoder.layer.0.attention.self.key_quantize': False, 'bert.encoder.layer.0.attention.self.value_quantize': True, 'bert.encoder.layer.0.attention.self.value_width': 32, 'bert.encoder.layer.0.attention.self.value_exp_bias': 15, 'bert.encoder.layer.0.attention.output.dense_quantize': True, 'bert.encoder.layer.0.attention.output.dense_width': 8, 'bert.encoder.layer.0.attention.output.dense_exp_bias': 3, 'bert.encoder.layer.0.intermediate.dense_quantize': True, 'bert.encoder.layer.0.intermediate.dense_width': 16, 'bert.encoder.layer.0.intermediate.dense_exp_bias': 15, 'bert.encoder.layer.0.output.dense_quantize': False, 'bert.encoder.layer.1.attention.self.query_quantize': True, 'bert.encoder.layer.1.attention.self.query_width': 16, 'bert.encoder.layer.1.attention.self.query_exp_bias': 7, 'bert.encoder.layer.1.attention.self.key_quantize': False, 'bert.encoder.layer.1.attention.self.value_quantize': False, 'bert.encoder.layer.1.attention.output.dense_quantize': True, 'bert.encoder.layer.1.attention.output.dense_width': 16, 'bert.encoder.layer.1.attention.output.dense_exp_bias': 7, 'bert.encoder.layer.1.intermediate.dense_quantize': True, 'bert.encoder.layer.1.intermediate.dense_width': 32, 'bert.encoder.layer.1.intermediate.dense_exp_bias': 3, 'bert.encoder.layer.1.output.dense_quantize': False, 'bert.pooler.dense_quantize': True, 'bert.pooler.dense_width': 16, 'bert.pooler.dense_exp_bias': 3, 'classifier_quantize': True, 'classifier_width': 32, 'classifier_exp_bias': 3}. Best is trial 10 with value: 0.85436.\n",
      "/vol/bitbucket/ug22/adls/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:37, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.623900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.466900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.424800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.390000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.376700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.378500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:19]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-02-04 15:47:20,458] Trial 16 finished with value: 0.85436 and parameters: {'bert.encoder.layer.0.attention.self.query_quantize': False, 'bert.encoder.layer.0.attention.self.key_quantize': False, 'bert.encoder.layer.0.attention.self.value_quantize': True, 'bert.encoder.layer.0.attention.self.value_width': 32, 'bert.encoder.layer.0.attention.self.value_exp_bias': 15, 'bert.encoder.layer.0.attention.output.dense_quantize': True, 'bert.encoder.layer.0.attention.output.dense_width': 8, 'bert.encoder.layer.0.attention.output.dense_exp_bias': 7, 'bert.encoder.layer.0.intermediate.dense_quantize': True, 'bert.encoder.layer.0.intermediate.dense_width': 16, 'bert.encoder.layer.0.intermediate.dense_exp_bias': 15, 'bert.encoder.layer.0.output.dense_quantize': False, 'bert.encoder.layer.1.attention.self.query_quantize': True, 'bert.encoder.layer.1.attention.self.query_width': 16, 'bert.encoder.layer.1.attention.self.query_exp_bias': 7, 'bert.encoder.layer.1.attention.self.key_quantize': False, 'bert.encoder.layer.1.attention.self.value_quantize': False, 'bert.encoder.layer.1.attention.output.dense_quantize': True, 'bert.encoder.layer.1.attention.output.dense_width': 16, 'bert.encoder.layer.1.attention.output.dense_exp_bias': 3, 'bert.encoder.layer.1.intermediate.dense_quantize': True, 'bert.encoder.layer.1.intermediate.dense_width': 32, 'bert.encoder.layer.1.intermediate.dense_exp_bias': 3, 'bert.encoder.layer.1.output.dense_quantize': False, 'bert.pooler.dense_quantize': True, 'bert.pooler.dense_width': 8, 'bert.pooler.dense_exp_bias': 3, 'classifier_quantize': True, 'classifier_width': 32, 'classifier_exp_bias': 7}. Best is trial 10 with value: 0.85436.\n",
      "/vol/bitbucket/ug22/adls/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:36, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.638700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.483000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.426900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.385100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.379800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.382400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:19]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-02-04 15:48:17,640] Trial 17 finished with value: 0.8496 and parameters: {'bert.encoder.layer.0.attention.self.query_quantize': False, 'bert.encoder.layer.0.attention.self.key_quantize': False, 'bert.encoder.layer.0.attention.self.value_quantize': True, 'bert.encoder.layer.0.attention.self.value_width': 16, 'bert.encoder.layer.0.attention.self.value_exp_bias': 3, 'bert.encoder.layer.0.attention.output.dense_quantize': True, 'bert.encoder.layer.0.attention.output.dense_width': 16, 'bert.encoder.layer.0.attention.output.dense_exp_bias': 7, 'bert.encoder.layer.0.intermediate.dense_quantize': True, 'bert.encoder.layer.0.intermediate.dense_width': 16, 'bert.encoder.layer.0.intermediate.dense_exp_bias': 15, 'bert.encoder.layer.0.output.dense_quantize': False, 'bert.encoder.layer.1.attention.self.query_quantize': True, 'bert.encoder.layer.1.attention.self.query_width': 8, 'bert.encoder.layer.1.attention.self.query_exp_bias': 7, 'bert.encoder.layer.1.attention.self.key_quantize': False, 'bert.encoder.layer.1.attention.self.value_quantize': False, 'bert.encoder.layer.1.attention.output.dense_quantize': True, 'bert.encoder.layer.1.attention.output.dense_width': 32, 'bert.encoder.layer.1.attention.output.dense_exp_bias': 3, 'bert.encoder.layer.1.intermediate.dense_quantize': True, 'bert.encoder.layer.1.intermediate.dense_width': 8, 'bert.encoder.layer.1.intermediate.dense_exp_bias': 15, 'bert.encoder.layer.1.output.dense_quantize': False, 'bert.pooler.dense_quantize': True, 'bert.pooler.dense_width': 32, 'bert.pooler.dense_exp_bias': 15, 'classifier_quantize': True, 'classifier_width': 8, 'classifier_exp_bias': 7}. Best is trial 10 with value: 0.85436.\n",
      "/vol/bitbucket/ug22/adls/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:36, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.651800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.505600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.461000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.420000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.390800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.387700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:18]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-02-04 15:49:13,274] Trial 18 finished with value: 0.8486 and parameters: {'bert.encoder.layer.0.attention.self.query_quantize': False, 'bert.encoder.layer.0.attention.self.key_quantize': False, 'bert.encoder.layer.0.attention.self.value_quantize': True, 'bert.encoder.layer.0.attention.self.value_width': 32, 'bert.encoder.layer.0.attention.self.value_exp_bias': 15, 'bert.encoder.layer.0.attention.output.dense_quantize': True, 'bert.encoder.layer.0.attention.output.dense_width': 8, 'bert.encoder.layer.0.attention.output.dense_exp_bias': 3, 'bert.encoder.layer.0.intermediate.dense_quantize': False, 'bert.encoder.layer.0.output.dense_quantize': False, 'bert.encoder.layer.1.attention.self.query_quantize': False, 'bert.encoder.layer.1.attention.self.key_quantize': False, 'bert.encoder.layer.1.attention.self.value_quantize': True, 'bert.encoder.layer.1.attention.self.value_width': 32, 'bert.encoder.layer.1.attention.self.value_exp_bias': 15, 'bert.encoder.layer.1.attention.output.dense_quantize': True, 'bert.encoder.layer.1.attention.output.dense_width': 8, 'bert.encoder.layer.1.attention.output.dense_exp_bias': 7, 'bert.encoder.layer.1.intermediate.dense_quantize': True, 'bert.encoder.layer.1.intermediate.dense_width': 32, 'bert.encoder.layer.1.intermediate.dense_exp_bias': 3, 'bert.encoder.layer.1.output.dense_quantize': False, 'bert.pooler.dense_quantize': True, 'bert.pooler.dense_width': 16, 'bert.pooler.dense_exp_bias': 3, 'classifier_quantize': True, 'classifier_width': 16, 'classifier_exp_bias': 3}. Best is trial 10 with value: 0.85436.\n",
      "/vol/bitbucket/ug22/adls/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:37, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.623900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.466900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.424800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.390000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.376700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.378500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:19]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-02-04 15:50:10,870] Trial 19 finished with value: 0.85436 and parameters: {'bert.encoder.layer.0.attention.self.query_quantize': False, 'bert.encoder.layer.0.attention.self.key_quantize': False, 'bert.encoder.layer.0.attention.self.value_quantize': True, 'bert.encoder.layer.0.attention.self.value_width': 32, 'bert.encoder.layer.0.attention.self.value_exp_bias': 15, 'bert.encoder.layer.0.attention.output.dense_quantize': True, 'bert.encoder.layer.0.attention.output.dense_width': 8, 'bert.encoder.layer.0.attention.output.dense_exp_bias': 7, 'bert.encoder.layer.0.intermediate.dense_quantize': True, 'bert.encoder.layer.0.intermediate.dense_width': 32, 'bert.encoder.layer.0.intermediate.dense_exp_bias': 15, 'bert.encoder.layer.0.output.dense_quantize': False, 'bert.encoder.layer.1.attention.self.query_quantize': True, 'bert.encoder.layer.1.attention.self.query_width': 16, 'bert.encoder.layer.1.attention.self.query_exp_bias': 7, 'bert.encoder.layer.1.attention.self.key_quantize': False, 'bert.encoder.layer.1.attention.self.value_quantize': False, 'bert.encoder.layer.1.attention.output.dense_quantize': True, 'bert.encoder.layer.1.attention.output.dense_width': 16, 'bert.encoder.layer.1.attention.output.dense_exp_bias': 3, 'bert.encoder.layer.1.intermediate.dense_quantize': True, 'bert.encoder.layer.1.intermediate.dense_width': 32, 'bert.encoder.layer.1.intermediate.dense_exp_bias': 3, 'bert.encoder.layer.1.output.dense_quantize': False, 'bert.pooler.dense_quantize': True, 'bert.pooler.dense_width': 16, 'bert.pooler.dense_exp_bias': 3, 'classifier_quantize': True, 'classifier_width': 32, 'classifier_exp_bias': 7}. Best is trial 10 with value: 0.85436.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log - Best: 0.8544\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'task2_results/all_precision_results.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 177\u001b[39m\n\u001b[32m    174\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprecision_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m - Best: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstudy.best_trial.value\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    176\u001b[39m \u001b[38;5;66;03m# ============ SAVE RESULTS ============\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mOUTPUT_DIR\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mall_precision_results.json\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mw\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m    178\u001b[39m     json.dump(all_results, f, indent=\u001b[32m2\u001b[39m)\n\u001b[32m    180\u001b[39m \u001b[38;5;66;03m# ============ PLOT: ONE CURVE PER PRECISION ============\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/vol/bitbucket/ug22/adls/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py:344\u001b[39m, in \u001b[36m_modified_open\u001b[39m\u001b[34m(file, *args, **kwargs)\u001b[39m\n\u001b[32m    337\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m}:\n\u001b[32m    338\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    339\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPython won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m by default \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    340\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    341\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myou can use builtins\u001b[39m\u001b[33m'\u001b[39m\u001b[33m open.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    342\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m344\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'task2_results/all_precision_results.json'"
     ]
    }
   ],
   "source": [
    "# ============ TASK 2: SEPARATE SEARCH PER PRECISION TYPE ============\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from chop.nn.quantized.modules.linear import (\n",
    "    LinearInteger,\n",
    "    LinearMinifloatDenorm,\n",
    "    LinearMinifloatIEEE,\n",
    "    LinearLog,\n",
    ")\n",
    "from chop.tools.utils import deepsetattr\n",
    "from chop.tools import get_trainer\n",
    "from copy import deepcopy\n",
    "from optuna.samplers import TPESampler\n",
    "import optuna\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# ============ CONFIG ============\n",
    "N_TRIALS_PER_PRECISION = 20  # Fewer trials per type since we're running multiple\n",
    "OUTPUT_DIR = Path(\"task2_results\")\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Each precision type to test (excluding broken ones)\n",
    "PRECISION_CONFIGS = {\n",
    "    \"Integer\": {\n",
    "        \"class\": LinearInteger,\n",
    "        \"build_config\": lambda trial, name: {\n",
    "            \"data_in_width\": trial.suggest_categorical(f\"{name}_width\", [8, 16, 32]),\n",
    "            \"data_in_frac_width\": trial.suggest_categorical(f\"{name}_frac_width\", [2, 4, 8]),\n",
    "            \"weight_width\": trial.suggest_categorical(f\"{name}_width\", [8, 16, 32]),\n",
    "            \"weight_frac_width\": trial.suggest_categorical(f\"{name}_frac_width\", [2, 4, 8]),\n",
    "            \"bias_width\": trial.suggest_categorical(f\"{name}_width\", [8, 16, 32]),\n",
    "            \"bias_frac_width\": trial.suggest_categorical(f\"{name}_frac_width\", [2, 4, 8]),\n",
    "        }\n",
    "    },\n",
    "    \"MinifloatDenorm\": {\n",
    "        \"class\": LinearMinifloatDenorm,\n",
    "        \"build_config\": lambda trial, name: {\n",
    "            \"data_in_width\": trial.suggest_categorical(f\"{name}_width\", [8, 16, 32]),\n",
    "            \"data_in_exponent_width\": trial.suggest_categorical(f\"{name}_exp_width\", [2, 4, 8]),\n",
    "            \"data_in_exponent_bias\": trial.suggest_categorical(f\"{name}_exp_bias\", [3, 7, 15]),\n",
    "            \"weight_width\": trial.suggest_categorical(f\"{name}_width\", [8, 16, 32]),\n",
    "            \"weight_exponent_width\": trial.suggest_categorical(f\"{name}_exp_width\", [2, 4, 8]),\n",
    "            \"weight_exponent_bias\": trial.suggest_categorical(f\"{name}_exp_bias\", [3, 7, 15]),\n",
    "            \"bias_width\": trial.suggest_categorical(f\"{name}_width\", [8, 16, 32]),\n",
    "            \"bias_exponent_width\": trial.suggest_categorical(f\"{name}_exp_width\", [2, 4, 8]),\n",
    "            \"bias_exponent_bias\": trial.suggest_categorical(f\"{name}_exp_bias\", [3, 7, 15]),\n",
    "        }\n",
    "    },\n",
    "    \"MinifloatIEEE\": {\n",
    "        \"class\": LinearMinifloatIEEE,\n",
    "        \"build_config\": lambda trial, name: {\n",
    "            \"data_in_width\": trial.suggest_categorical(f\"{name}_width\", [8, 16, 32]),\n",
    "            \"data_in_exponent_width\": trial.suggest_categorical(f\"{name}_exp_width\", [2, 4, 8]),\n",
    "            \"data_in_exponent_bias\": trial.suggest_categorical(f\"{name}_exp_bias\", [3, 7, 15]),\n",
    "            \"weight_width\": trial.suggest_categorical(f\"{name}_width\", [8, 16, 32]),\n",
    "            \"weight_exponent_width\": trial.suggest_categorical(f\"{name}_exp_width\", [2, 4, 8]),\n",
    "            \"weight_exponent_bias\": trial.suggest_categorical(f\"{name}_exp_bias\", [3, 7, 15]),\n",
    "            \"bias_width\": trial.suggest_categorical(f\"{name}_width\", [8, 16, 32]),\n",
    "            \"bias_exponent_width\": trial.suggest_categorical(f\"{name}_exp_width\", [2, 4, 8]),\n",
    "            \"bias_exponent_bias\": trial.suggest_categorical(f\"{name}_exp_bias\", [3, 7, 15]),\n",
    "        }\n",
    "    },\n",
    "    \"Log\": {\n",
    "        \"class\": LinearLog,\n",
    "        \"build_config\": lambda trial, name: {\n",
    "            \"data_in_width\": trial.suggest_categorical(f\"{name}_width\", [8, 16, 32]),\n",
    "            \"data_in_exponent_bias\": trial.suggest_categorical(f\"{name}_exp_bias\", [3, 7, 15]),\n",
    "            \"weight_width\": trial.suggest_categorical(f\"{name}_width\", [8, 16, 32]),\n",
    "            \"weight_exponent_bias\": trial.suggest_categorical(f\"{name}_exp_bias\", [3, 7, 15]),\n",
    "            \"bias_width\": trial.suggest_categorical(f\"{name}_width\", [8, 16, 32]),\n",
    "            \"bias_exponent_bias\": trial.suggest_categorical(f\"{name}_exp_bias\", [3, 7, 15]),\n",
    "        }\n",
    "    },\n",
    "}\n",
    "\n",
    "# ============ MODEL CONSTRUCTOR FOR SINGLE PRECISION ============\n",
    "def make_construct_model(precision_name, precision_info):\n",
    "    \"\"\"Create a model constructor for a specific precision type.\"\"\"\n",
    "    \n",
    "    def construct_model(trial):\n",
    "        trial_model = deepcopy(base_model)\n",
    "        layer_cls = precision_info[\"class\"]\n",
    "        build_config = precision_info[\"build_config\"]\n",
    "        \n",
    "        for name, layer in trial_model.named_modules():\n",
    "            if isinstance(layer, torch.nn.Linear):\n",
    "                # For each layer: quantize or keep full precision\n",
    "                use_quantized = trial.suggest_categorical(f\"{name}_quantize\", [True, False])\n",
    "                \n",
    "                if not use_quantized:\n",
    "                    continue\n",
    "                \n",
    "                config = build_config(trial, name)\n",
    "                \n",
    "                new_layer = layer_cls(\n",
    "                    in_features=layer.in_features,\n",
    "                    out_features=layer.out_features,\n",
    "                    config=config\n",
    "                )\n",
    "                new_layer.weight.data = layer.weight.data\n",
    "                if layer.bias is not None and new_layer.bias is not None:\n",
    "                    new_layer.bias.data = layer.bias.data\n",
    "                \n",
    "                deepsetattr(trial_model, name, new_layer)\n",
    "        \n",
    "        return trial_model\n",
    "    \n",
    "    return construct_model\n",
    "\n",
    "# ============ OBJECTIVE FACTORY ============\n",
    "def make_objective(construct_model_fn):\n",
    "    def objective(trial):\n",
    "        model = construct_model_fn(trial)\n",
    "        \n",
    "        trainer = get_trainer(\n",
    "            model=model,\n",
    "            tokenized_dataset=dataset,\n",
    "            tokenizer=tokenizer,\n",
    "            evaluate_metric=\"accuracy\",\n",
    "            num_train_epochs=1,\n",
    "        )\n",
    "        \n",
    "        trainer.train()\n",
    "        eval_results = trainer.evaluate()\n",
    "        return eval_results[\"eval_accuracy\"]\n",
    "    \n",
    "    return objective\n",
    "\n",
    "# ============ RUN SEARCH FOR EACH PRECISION ============\n",
    "all_results = {}\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"TASK 2: Separate Search Per Precision Type\")\n",
    "print(f\"Trials per precision: {N_TRIALS_PER_PRECISION}\")\n",
    "print(f\"Precision types: {list(PRECISION_CONFIGS.keys())}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for precision_name, precision_info in PRECISION_CONFIGS.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Searching: {precision_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    construct_fn = make_construct_model(precision_name, precision_info)\n",
    "    objective_fn = make_objective(construct_fn)\n",
    "    \n",
    "    study = optuna.create_study(\n",
    "        direction=\"maximize\",\n",
    "        study_name=f\"bert-{precision_name}-search\",\n",
    "        sampler=TPESampler(),\n",
    "    )\n",
    "    \n",
    "    study.optimize(objective_fn, n_trials=N_TRIALS_PER_PRECISION)\n",
    "    \n",
    "    # Store results\n",
    "    accuracies = [t.value if t.value is not None else 0 for t in study.trials]\n",
    "    best_so_far = []\n",
    "    current_best = 0\n",
    "    for acc in accuracies:\n",
    "        if acc > current_best:\n",
    "            current_best = acc\n",
    "        best_so_far.append(current_best)\n",
    "    \n",
    "    all_results[precision_name] = {\n",
    "        \"accuracies\": accuracies,\n",
    "        \"best_so_far\": best_so_far,\n",
    "        \"best_accuracy\": study.best_trial.value,\n",
    "        \"best_params\": {k: str(v) for k, v in study.best_trial.params.items()},\n",
    "    }\n",
    "    \n",
    "    print(f\"{precision_name} - Best: {study.best_trial.value:.4f}\")\n",
    "\n",
    "# ============ SAVE RESULTS ============\n",
    "with open(OUTPUT_DIR / \"all_precision_results.json\", \"w\") as f:\n",
    "    json.dump(all_results, f, indent=2)\n",
    "\n",
    "# ============ PLOT: ONE CURVE PER PRECISION ============\n",
    "plt.figure(figsize=(12, 7))\n",
    "\n",
    "colors = ['#e74c3c', '#3498db', '#2ecc71', '#9b59b6', '#f39c12']\n",
    "markers = ['o', 's', '^', 'D', 'v']\n",
    "\n",
    "for i, (precision_name, results) in enumerate(all_results.items()):\n",
    "    best_so_far = results[\"best_so_far\"]\n",
    "    plt.plot(\n",
    "        range(1, len(best_so_far) + 1), \n",
    "        best_so_far, \n",
    "        marker=markers[i % len(markers)],\n",
    "        color=colors[i % len(colors)],\n",
    "        linewidth=2, \n",
    "        markersize=6,\n",
    "        label=f\"{precision_name} (best: {results['best_accuracy']:.4f})\"\n",
    "    )\n",
    "\n",
    "plt.xlabel(\"Trial Number\", fontsize=12)\n",
    "plt.ylabel(\"Best Accuracy So Far\", fontsize=12)\n",
    "plt.title(\"Mixed Precision Search: Comparison of Precision Types\", fontsize=14)\n",
    "plt.legend(loc='lower right', fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / \"precision_comparison.png\", dpi=150)\n",
    "plt.show()\n",
    "\n",
    "# ============ SUMMARY TABLE ============\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SUMMARY: Best Accuracy Per Precision Type\")\n",
    "print(\"=\"*60)\n",
    "for precision_name, results in sorted(all_results.items(), key=lambda x: -x[1][\"best_accuracy\"]):\n",
    "    print(f\"  {precision_name:20s}: {results['best_accuracy']:.4f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "SUMMARY: Best Accuracy Per Precision Type\n",
      "============================================================\n",
      "  MinifloatDenorm     : 0.8602\n",
      "  MinifloatIEEE       : 0.8599\n",
      "  Integer             : 0.8582\n",
      "  Log                 : 0.8544\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============ FIX: Create output directory and save results ============\n",
    "import os\n",
    "\n",
    "OUTPUT_DIR = Path(\"task2_results\")\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Save results\n",
    "with open(OUTPUT_DIR / \"all_precision_results.json\", \"w\") as f:\n",
    "    json.dump(all_results, f, indent=2)\n",
    "\n",
    "# ============ PLOT: ONE CURVE PER PRECISION ============\n",
    "plt.figure(figsize=(12, 7))\n",
    "\n",
    "colors = ['#e74c3c', '#3498db', '#2ecc71', '#9b59b6', '#f39c12']\n",
    "markers = ['o', 's', '^', 'D', 'v']\n",
    "\n",
    "for i, (precision_name, results) in enumerate(all_results.items()):\n",
    "    best_so_far = results[\"best_so_far\"]\n",
    "    plt.plot(\n",
    "        range(1, len(best_so_far) + 1), \n",
    "        best_so_far, \n",
    "        marker=markers[i % len(markers)],\n",
    "        color=colors[i % len(colors)],\n",
    "        linewidth=2, \n",
    "        markersize=6,\n",
    "        label=f\"{precision_name} (best: {results['best_accuracy']:.4f})\"\n",
    "    )\n",
    "\n",
    "plt.xlabel(\"Trial Number\", fontsize=12)\n",
    "plt.ylabel(\"Best Accuracy So Far\", fontsize=12)\n",
    "plt.title(\"Mixed Precision Search: Comparison of Precision Types\", fontsize=14)\n",
    "plt.legend(loc='lower right', fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / \"precision_comparison.png\", dpi=150)\n",
    "plt.show()\n",
    "\n",
    "# ============ SUMMARY TABLE ============\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SUMMARY: Best Accuracy Per Precision Type\")\n",
    "print(\"=\"*60)\n",
    "for precision_name, results in sorted(all_results.items(), key=lambda x: -x[1][\"best_accuracy\"]):\n",
    "    print(f\"  {precision_name:20s}: {results['best_accuracy']:.4f}\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mase",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
