{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 5: Neural Architecture Search (NAS) with Mase and Optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we'll see how Mase can be integrated with Optuna, the popular hyperparameter optimization framework, to search for a Bert model optimized for sequence classification on the IMDb dataset. We'll take the Optuna-generated model and import it into Mase, then run the CompressionPipeline to prepare the model for edge deployment by quantizing and pruning its weights.\n",
    "\n",
    "As we'll see, running Architecture Search with Mase/Optuna involves the following steps.\n",
    "\n",
    "1. **Define the search space**: this is a dictionary containing the range of values for each parameter at each layer in the model.\n",
    "\n",
    "2. **Write the model constructor**: this is a function which uses Optuna utilities to sample a model from the search space, and constructs the model using transformers from_config class method.\n",
    "\n",
    "3. **Write the objective function**: this function calls on the model constructor defined in Step 2 and defines the training/evaluation setup for each search iteration.\n",
    "\n",
    "4. **Go!** Choose an Optuna sampler, create a study and launch the search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"prajjwal1/bert-tiny\"\n",
    "tokenizer_checkpoint = \"bert-base-uncased\"\n",
    "dataset_name = \"imdb\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, fetch the dataset using the `get_tokenized_dataset` utility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vol/bitbucket/nr722/adls_group_14/.venv/lib/python3.11/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n",
      "/vol/bitbucket/nr722/adls_group_14/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mTokenizing dataset imdb with AutoTokenizer for bert-base-uncased.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from chop.tools import get_tokenized_dataset\n",
    "\n",
    "dataset, tokenizer = get_tokenized_dataset(\n",
    "    dataset=dataset_name,\n",
    "    checkpoint=tokenizer_checkpoint,\n",
    "    return_tokenizer=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Defining the Search Space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by defining a search space, i.e. enumerating the possible combinations of hyperparameters that Optuna can choose during search. We'll explore the following range of values for the model's hidden size, intermediate size, number of layers and number of heads, inspired by the [NAS-BERT paper](https://arxiv.org/abs/2105.14444)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from chop.nn.modules import Identity\n",
    "\n",
    "search_space = {\n",
    "    \"num_layers\": [2, 4, 8],\n",
    "    \"num_heads\": [2, 4, 8, 16],\n",
    "    # hidden size is the embedding dimension in transformers\n",
    "    \"hidden_size\": [128, 192, 256, 384, 512],\n",
    "    # intermediate size is the dimension of the feedforward layer in transformers\n",
    "    \"intermediate_size\": [512, 768, 1024, 1536, 2048],\n",
    "    \"linear_layer_choices\": [\n",
    "        \"Linear\",\n",
    "        \"Identity\",\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Writing a Model Constructor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the following function, which will get called in each iteration of the search process. The function is passed the `trial` argument, which is an Optuna object that comes with many functionalities - see the [Trial documentation](https://optuna.readthedocs.io/en/stable/reference/trial.html) for more details. Here, we use the `trial.suggest_int` and `trial.suggest_categorical` functions to trigger the chosen sampler to choose parameter choices and layer types. The suggested integer is the index into the search space for each parameter, which we defined in the previous cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, AutoModelForSequenceClassification\n",
    "from chop.tools.utils import deepsetattr\n",
    "\n",
    "def construct_model(trial):\n",
    "    config = AutoConfig.from_pretrained(checkpoint)\n",
    "\n",
    "    # Update the paramaters in the config\n",
    "    for param in [\n",
    "        \"num_layers\",\n",
    "        \"num_heads\",\n",
    "        \"hidden_size\",\n",
    "        \"intermediate_size\",\n",
    "    ]:\n",
    "        chosen_idx = trial.suggest_int(param, 0, len(search_space[param]) - 1)\n",
    "        setattr(config, param, search_space[param][chosen_idx])\n",
    "\n",
    "    trial_model = AutoModelForSequenceClassification.from_config(config)\n",
    "\n",
    "    # Map string choices to actual classes\n",
    "    layer_map = {\n",
    "        \"Linear\": nn.Linear,\n",
    "        \"Identity\": Identity\n",
    "    }\n",
    "\n",
    "    for name, layer in trial_model.named_modules():\n",
    "        if isinstance(layer, nn.Linear) and layer.in_features == layer.out_features:\n",
    "            # trial.suggest_categorical returns a string now (e.g. \"Linear\" or \"Identity\")\n",
    "            new_layer_name = trial.suggest_categorical(\n",
    "                f\"{name}_type\",\n",
    "                search_space[\"linear_layer_choices\"],\n",
    "            )\n",
    "\n",
    "            new_layer_cls = layer_map[new_layer_name]\n",
    "\n",
    "            if new_layer_cls == nn.Linear:\n",
    "                continue\n",
    "            elif new_layer_cls == Identity:\n",
    "                new_layer = Identity()\n",
    "                deepsetattr(trial_model, name, new_layer)\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown layer type: {new_layer_cls}\")\n",
    "\n",
    "    return trial_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Defining the Objective Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define the objective function for the search, which gets called on each trial. In each trial, we create a new model instace with chosen hyperparameters according to the defined sampler. We then use the `get_trainer` utility in Mase to run a training loop on the IMDb dataset for a number of epochs. Finally, we use `evaluate` to report back the classification accuracy on the test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chop.tools import get_trainer\n",
    "import torch\n",
    "\n",
    "def objective(trial):\n",
    "    \n",
    "    # Define the model and move to GPU\n",
    "    model = construct_model(trial)\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model.to(device)\n",
    "\n",
    "    trainer = get_trainer(\n",
    "        model=model,\n",
    "        tokenized_dataset=dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        evaluate_metric=\"accuracy\",\n",
    "        num_train_epochs=1,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    eval_results = trainer.evaluate()\n",
    "\n",
    "    # Move back to CPU for storage\n",
    "    trial.set_user_attr(\"model\", model.cpu())\n",
    "\n",
    "    return eval_results[\"eval_accuracy\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Launching the Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optuna provides a number of samplers, for example:\n",
    "\n",
    "* **GridSampler**: iterates through every possible combination of hyperparameters in the search space\n",
    "* **RandomSampler**: chooses a random combination of hyperparameters in each iteration\n",
    "* **TPESampler**: uses Tree-structured Parzen Estimator algorithm to choose hyperparameter values.\n",
    "\n",
    "You can define the chosen sampler by simply importing from `optuna.samplers` as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optuna.samplers import GridSampler, RandomSampler, TPESampler\n",
    "\n",
    "sampler = RandomSampler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all the pieces in place, we can launch the search as follows. The number of trials is set to 1 so you can go get a coffee for 10 minutes, then proceed with the tutorial. However, this will essentially be a random model - for better results, set this to 100 and leave it running overnight!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "study = optuna.create_study(\n",
    "    direction=\"maximize\",\n",
    "    study_name=\"bert-tiny-nas-study\",\n",
    "    sampler=sampler,\n",
    ")\n",
    "\n",
    "study.optimize(\n",
    "    objective,\n",
    "    n_trials=1,\n",
    "    timeout=60 * 60 * 24,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetch the model associated with the best trial as follows, and export to be used in future tutorials. In Tutorial 6, we'll see how to run mixed-precision quantization search on top of the model we've just found through NAS to further find the optimal quantization mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import dill\n",
    "\n",
    "model = study.best_trial.user_attrs[\"model\"].cpu()\n",
    "\n",
    "with open(f\"{Path.home()}/tutorial_5_best_model.pkl\", \"wb\") as f:\n",
    "    dill.dump(model, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying the Optimized Model with CompressionPipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can run the CompressionPipeline in Mase to run uniform quantization and pruning over the searched model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chop.pipelines import CompressionPipeline\n",
    "from chop import MaseGraph\n",
    "\n",
    "mg = MaseGraph(model)\n",
    "pipe = CompressionPipeline()\n",
    "\n",
    "quantization_config = {\n",
    "    \"by\": \"type\",\n",
    "    \"default\": {\n",
    "        \"config\": {\n",
    "            \"name\": None,\n",
    "        }\n",
    "    },\n",
    "    \"linear\": {\n",
    "        \"config\": {\n",
    "            \"name\": \"integer\",\n",
    "            # data\n",
    "            \"data_in_width\": 8,\n",
    "            \"data_in_frac_width\": 4,\n",
    "            # weight\n",
    "            \"weight_width\": 8,\n",
    "            \"weight_frac_width\": 4,\n",
    "            # bias\n",
    "            \"bias_width\": 8,\n",
    "            \"bias_frac_width\": 4,\n",
    "        }\n",
    "    },\n",
    "}\n",
    "\n",
    "pruning_config = {\n",
    "    \"weight\": {\n",
    "        \"sparsity\": 0.5,\n",
    "        \"method\": \"l1-norm\",\n",
    "        \"scope\": \"local\",\n",
    "    },\n",
    "    \"activation\": {\n",
    "        \"sparsity\": 0.5,\n",
    "        \"method\": \"l1-norm\",\n",
    "        \"scope\": \"local\",\n",
    "    },\n",
    "}\n",
    "\n",
    "mg, _ = pipe(\n",
    "    mg,\n",
    "    pass_args={\n",
    "        \"quantize_transform_pass\": quantization_config,\n",
    "        \"prune_transform_pass\": pruning_config,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, export the MaseGraph for the compressed checkpoint to be used in future tutorials for hardware generation and distributed deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mg.export(f\"{Path.home()}/tutorial_5_nas_compressed\", save_format=\"state_dict\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_space = {\n",
    "    \"num_layers\": [2, 4, 8],\n",
    "    \"num_heads\": [2, 4, 8, 16],\n",
    "    # hidden size is the embedding dimension in transformers\n",
    "    \"hidden_size\": [128, 192, 256, 384, 512],\n",
    "    # intermediate size is the dimension of the feedforward layer in transformers\n",
    "    \"intermediate_size\": [512, 768, 1024, 1536, 2048],\n",
    "    \"linear_layer_choices\": [\n",
    "        nn.Linear,\n",
    "        # Identity,\n",
    "    ],\n",
    "}\n",
    "# search_space = {\n",
    "#     \"num_layers\": [2, ],\n",
    "#     \"num_heads\": [2, 4],\n",
    "#     # hidden size is the embedding dimension in transformers\n",
    "#     \"hidden_size\": [128, 512],\n",
    "#     # intermediate size is the dimension of the feedforward layer in transformers\n",
    "#     \"intermediate_size\": [512],\n",
    "#     \"linear_layer_choices\": [\n",
    "#         nn.Linear,\n",
    "#         Identity,\n",
    "#     ],\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import optuna\n",
    "from optuna.samplers import RandomSampler, TPESampler, GridSampler\n",
    "import torch.nn as nn\n",
    "from chop.nn.modules import Identity\n",
    "import traceback\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"optuna.distributions\")\n",
    "\n",
    "# --- 1. FULL Search Space (for Random/TPE) ---\n",
    "search_space = {\n",
    "    \"num_layers\": [2, 4, 8],\n",
    "    \"num_heads\": [2, 4, 8, 16],\n",
    "    \"hidden_size\": [128, 192, 256, 384, 512],\n",
    "    \"intermediate_size\": [512, 768, 1024, 1536, 2048],\n",
    "    \"linear_layer_choices\": [\"Linear\", \"Identity\"],\n",
    "}\n",
    "\n",
    "# --- 2. FIXED Search Space (for GridSampler) ---\n",
    "# CRITICAL: construct_model uses `trial.suggest_int` which returns an index. \n",
    "# Therefore, GridSampler must search over INDICES (0, 1, 2...), not raw values (2, 4, 8...).\n",
    "grid_search_space = {\n",
    "    \"num_layers\": list(range(len(search_space[\"num_layers\"]))),\n",
    "    \"num_heads\": list(range(len(search_space[\"num_heads\"]))),\n",
    "    \"hidden_size\": list(range(len(search_space[\"hidden_size\"]))),\n",
    "    \"intermediate_size\": list(range(len(search_space[\"intermediate_size\"]))),\n",
    "}\n",
    "\n",
    "print(\"DEBUG: grid_search_space['num_layers'] is:\", grid_search_space['num_layers'])\n",
    "\n",
    "# We must pre-populate the grid with ALL possible layer types for ALL possible layers (up to max layers=8).\n",
    "max_layers = 8\n",
    "for i in range(max_layers):\n",
    "    for sublayer in [\"attention.self.query\", \"attention.self.key\", \"attention.self.value\", \n",
    "                     \"attention.output.dense\", \"intermediate.dense\", \"output.dense\"]:\n",
    "        # The key name must match exactly what `construct_model` asks for: e.g. \"bert.encoder.layer.0.output.dense_type\"\n",
    "        param_name = f\"bert.encoder.layer.{i}.{sublayer}_type\"\n",
    "        \n",
    "        # For GridSampler we fix this to just \"Linear\" to avoid combinatorial explosion\n",
    "        # grid_search_space[param_name] = [\"Linear\", \"Identity\"]\n",
    "        grid_search_space[param_name] = [\"Linear\"]\n",
    "\n",
    "# FIX: The BERT Pooler layer (bert.pooler.dense) is also a Linear layer with in_features == out_features.\n",
    "# It gets picked up by construct_model, so it MUST be in the grid.\n",
    "grid_search_space[\"bert.pooler.dense_type\"] = [\"Linear\"]\n",
    "\n",
    "\n",
    "# --- 3. Sampler Definition ---\n",
    "samplers = {\n",
    "    # We pass the fully-populated static grid to GridSampler\n",
    "    \"Grid Search\": GridSampler(grid_search_space),\n",
    "    \"Random Search\": RandomSampler(),\n",
    "    \"TPE (Bayesian)\": TPESampler(),\n",
    "}\n",
    "\n",
    "n_trials = 30\n",
    "sampler_history = {}\n",
    "best_overall_value = -float(\"inf\")\n",
    "best_overall_model = None\n",
    "\n",
    "# --- 4. Optimization Loop ---\n",
    "for name, sampler in samplers.items():\n",
    "    print(f\"--- Starting {name} ---\")\n",
    "    try:\n",
    "        # Standard Optuna setup\n",
    "        study = optuna.create_study(direction=\"maximize\", sampler=sampler)\n",
    "        study.optimize(objective, n_trials=n_trials)\n",
    "        \n",
    "        values = [t.value for t in study.trials if t.value is not None]\n",
    "        if values:\n",
    "            sampler_history[name] = np.maximum.accumulate(values)\n",
    "            \n",
    "            if study.best_value > best_overall_value:\n",
    "                best_overall_value = study.best_value\n",
    "                best_overall_model = study.best_trial.user_attrs[\"model\"]\n",
    "                \n",
    "    except Exception as e:\n",
    "        # traceback.print_exc()\n",
    "        print(f\"Sampler {name} encountered an error: {e}\")\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "for name, best_values in sampler_history.items():\n",
    "    if name in sampler_history:\n",
    "        plt.plot(range(1, len(sampler_history[name]) + 1), sampler_history[name], marker=\"o\", label=name)\n",
    "\n",
    "plt.xlabel(\"Number of Trials\")\n",
    "plt.ylabel(\"Maximum Accuracy Achieved\")\n",
    "plt.title(\"NAS Sampler Comparison (GPU-Accelerated)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Linear Layers in one BERT Encoder Block ---\n",
      "1. attention.self.query\n",
      "2. attention.self.key\n",
      "3. attention.self.value\n",
      "4. attention.output.dense\n",
      "5. intermediate.dense\n",
      "6. output.dense\n",
      "\n",
      "Verified Total: 6 Linear Layers per Block\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel\n",
    "import torch.nn as nn\n",
    "\n",
    "# Load the base model to inspect its architecture\n",
    "checkpoint = \"prajjwal1/bert-tiny\"\n",
    "model = AutoModel.from_pretrained(checkpoint)\n",
    "\n",
    "# Get the first encoder layer (BertLayer)\n",
    "encoder_block = model.encoder.layer[0]\n",
    "\n",
    "print(\"--- Linear Layers in one BERT Encoder Block ---\")\n",
    "count = 0\n",
    "for name, module in encoder_block.named_modules():\n",
    "    # Filter for Linear layers\n",
    "    if isinstance(module, nn.Linear):\n",
    "        # We print the name relative to the block\n",
    "        print(f\"{count+1}. {name}\")\n",
    "        count += 1\n",
    "\n",
    "print(f\"\\nVerified Total: {count} Linear Layers per Block\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-02-02 15:27:43,738] A new study created in memory with name: no-name-0b067e3a-9399-4b0e-a2e7-0ed4b7560354\n",
      "[I 2026-02-02 15:27:43,903] Trial 0 finished with value: 0.0 and parameters: {'num_layers': 0, 'num_heads': 1, 'hidden_size': 1, 'intermediate_size': 4, 'bert.encoder.layer.0.attention.self.query_type': 'Identity', 'bert.encoder.layer.0.attention.self.key_type': 'Identity', 'bert.encoder.layer.0.attention.self.value_type': 'Linear', 'bert.encoder.layer.0.attention.output.dense_type': 'Linear', 'bert.encoder.layer.1.attention.self.query_type': 'Identity', 'bert.encoder.layer.1.attention.self.key_type': 'Linear', 'bert.encoder.layer.1.attention.self.value_type': 'Linear', 'bert.encoder.layer.1.attention.output.dense_type': 'Identity', 'bert.pooler.dense_type': 'Linear'}. Best is trial 0 with value: 0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Verifying Sub-Layer Types in Randomly Sampled Models ---\n",
      "(Checking Layer 0 to confirm Identity/Linear mixing)\n",
      "\n",
      "Trial 0 Choices (Layer 0):\n",
      "  Query       : Identity\n",
      "  Key         : Identity\n",
      "  Value       : Linear\n",
      "  Attn Output : Linear\n",
      "  Intermediate: Linear\n",
      "  Output      : Linear\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-02-02 15:27:44,160] Trial 1 finished with value: 0.0 and parameters: {'num_layers': 2, 'num_heads': 2, 'hidden_size': 4, 'intermediate_size': 0, 'bert.encoder.layer.0.attention.self.query_type': 'Identity', 'bert.encoder.layer.0.attention.self.key_type': 'Identity', 'bert.encoder.layer.0.attention.self.value_type': 'Linear', 'bert.encoder.layer.0.attention.output.dense_type': 'Linear', 'bert.encoder.layer.0.intermediate.dense_type': 'Identity', 'bert.encoder.layer.0.output.dense_type': 'Identity', 'bert.encoder.layer.1.attention.self.query_type': 'Identity', 'bert.encoder.layer.1.attention.self.key_type': 'Identity', 'bert.encoder.layer.1.attention.self.value_type': 'Linear', 'bert.encoder.layer.1.attention.output.dense_type': 'Linear', 'bert.encoder.layer.1.intermediate.dense_type': 'Linear', 'bert.encoder.layer.1.output.dense_type': 'Linear', 'bert.pooler.dense_type': 'Identity'}. Best is trial 0 with value: 0.0.\n",
      "[I 2026-02-02 15:27:44,298] Trial 2 finished with value: 0.0 and parameters: {'num_layers': 1, 'num_heads': 3, 'hidden_size': 0, 'intermediate_size': 1, 'bert.encoder.layer.0.attention.self.query_type': 'Linear', 'bert.encoder.layer.0.attention.self.key_type': 'Identity', 'bert.encoder.layer.0.attention.self.value_type': 'Identity', 'bert.encoder.layer.0.attention.output.dense_type': 'Identity', 'bert.encoder.layer.1.attention.self.query_type': 'Linear', 'bert.encoder.layer.1.attention.self.key_type': 'Linear', 'bert.encoder.layer.1.attention.self.value_type': 'Identity', 'bert.encoder.layer.1.attention.output.dense_type': 'Linear', 'bert.pooler.dense_type': 'Identity'}. Best is trial 0 with value: 0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trial 1 Choices (Layer 0):\n",
      "  Query       : Identity\n",
      "  Key         : Identity\n",
      "  Value       : Linear\n",
      "  Attn Output : Linear\n",
      "  Intermediate: Identity\n",
      "  Output      : Identity\n",
      "\n",
      "Trial 2 Choices (Layer 0):\n",
      "  Query       : Linear\n",
      "  Key         : Identity\n",
      "  Value       : Identity\n",
      "  Attn Output : Identity\n",
      "  Intermediate: Linear\n",
      "  Output      : Linear\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "from optuna.samplers import RandomSampler\n",
    "import torch.nn as nn\n",
    "from chop.nn.modules import Identity\n",
    "\n",
    "print(\"--- Verifying Sub-Layer Types in Randomly Sampled Models ---\")\n",
    "print(\"(Checking Layer 0 to confirm Identity/Linear mixing)\")\n",
    "\n",
    "def verify_sublayers(trial):\n",
    "    # Construct a model from the search space\n",
    "    model = construct_model(trial)\n",
    "    \n",
    "    # Inspect Layer 0 of the encoder to see what types were chosen\n",
    "    if hasattr(model, 'bert'):\n",
    "        layer0 = model.bert.encoder.layer[0]\n",
    "    else:\n",
    "        # Fallback for models that might be wrapped differently or different architectures\n",
    "        layer0 = model.encoder.layer[0] \n",
    "    \n",
    "    # Dictionary of the 6 sub-layers we expect to be swappable\n",
    "    sublayers = {\n",
    "        \"Query\": layer0.attention.self.query,\n",
    "        \"Key\": layer0.attention.self.key,\n",
    "        \"Value\": layer0.attention.self.value,\n",
    "        \"Attn Output\": layer0.attention.output.dense,\n",
    "        \"Intermediate\": layer0.intermediate.dense,\n",
    "        \"Output\": layer0.output.dense\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nTrial {trial.number} Choices (Layer 0):\")\n",
    "    for name, module in sublayers.items():\n",
    "        # Identify the type\n",
    "        type_name = \"Identity\" if isinstance(module, Identity) else \"Linear\"\n",
    "        print(f\"  {name:12s}: {type_name}\")\n",
    "\n",
    "    return 0\n",
    "\n",
    "# We use RandomSampler because GridSampler is restricted to Linear-only in our setup\n",
    "study_check = optuna.create_study(sampler=RandomSampler())\n",
    "study_check.optimize(verify_sublayers, n_trials=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mase",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
