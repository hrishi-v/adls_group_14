{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 5: Neural Architecture Search (NAS) with Mase and Optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we'll see how Mase can be integrated with Optuna, the popular hyperparameter optimization framework, to search for a Bert model optimized for sequence classification on the IMDb dataset. We'll take the Optuna-generated model and import it into Mase, then run the CompressionPipeline to prepare the model for edge deployment by quantizing and pruning its weights.\n",
    "\n",
    "As we'll see, running Architecture Search with Mase/Optuna involves the following steps.\n",
    "\n",
    "1. **Define the search space**: this is a dictionary containing the range of values for each parameter at each layer in the model.\n",
    "\n",
    "2. **Write the model constructor**: this is a function which uses Optuna utilities to sample a model from the search space, and constructs the model using transformers from_config class method.\n",
    "\n",
    "3. **Write the objective function**: this function calls on the model constructor defined in Step 2 and defines the training/evaluation setup for each search iteration.\n",
    "\n",
    "4. **Go!** Choose an Optuna sampler, create a study and launch the search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"prajjwal1/bert-tiny\"\n",
    "tokenizer_checkpoint = \"bert-base-uncased\"\n",
    "dataset_name = \"imdb\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, fetch the dataset using the `get_tokenized_dataset` utility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vol/bitbucket/nr722/adls_group_14/.venv/lib/python3.11/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n",
      "/vol/bitbucket/nr722/adls_group_14/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mTokenizing dataset imdb with AutoTokenizer for bert-base-uncased.\u001b[0m\n",
      "Using the latest cached version of the dataset since imdb couldn't be found on the Hugging Face Hub\n",
      "WARNING:datasets.load:Using the latest cached version of the dataset since imdb couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'plain_text' at /homes/nr722/.cache/huggingface/datasets/imdb/plain_text/0.0.0/e6281661ce1c48d982bc483cf8a173c1bbeb5d31 (last modified on Sun Jan 25 09:19:15 2026).\n",
      "WARNING:datasets.packaged_modules.cache.cache:Found the latest cached dataset configuration 'plain_text' at /homes/nr722/.cache/huggingface/datasets/imdb/plain_text/0.0.0/e6281661ce1c48d982bc483cf8a173c1bbeb5d31 (last modified on Sun Jan 25 09:19:15 2026).\n"
     ]
    }
   ],
   "source": [
    "from chop.tools import get_tokenized_dataset\n",
    "\n",
    "dataset, tokenizer = get_tokenized_dataset(\n",
    "    dataset=dataset_name,\n",
    "    checkpoint=tokenizer_checkpoint,\n",
    "    return_tokenizer=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Defining the Search Space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by defining a search space, i.e. enumerating the possible combinations of hyperparameters that Optuna can choose during search. We'll explore the following range of values for the model's hidden size, intermediate size, number of layers and number of heads, inspired by the [NAS-BERT paper](https://arxiv.org/abs/2105.14444)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from chop.nn.modules import Identity\n",
    "\n",
    "search_space = {\n",
    "    \"num_layers\": [2, 4, 8],\n",
    "    \"num_heads\": [2, 4, 8, 16],\n",
    "    # hidden size is the embedding dimension in transformers\n",
    "    \"hidden_size\": [128, 192, 256, 384, 512],\n",
    "    # intermediate size is the dimension of the feedforward layer in transformers\n",
    "    \"intermediate_size\": [512, 768, 1024, 1536, 2048],\n",
    "    \"linear_layer_choices\": [\n",
    "        nn.Linear,\n",
    "        Identity,\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Writing a Model Constructor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the following function, which will get called in each iteration of the search process. The function is passed the `trial` argument, which is an Optuna object that comes with many functionalities - see the [Trial documentation](https://optuna.readthedocs.io/en/stable/reference/trial.html) for more details. Here, we use the `trial.suggest_int` and `trial.suggest_categorical` functions to trigger the chosen sampler to choose parameter choices and layer types. The suggested integer is the index into the search space for each parameter, which we defined in the previous cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, AutoModelForSequenceClassification\n",
    "from chop.tools.utils import deepsetattr\n",
    "\n",
    "\n",
    "def construct_model(trial):\n",
    "    config = AutoConfig.from_pretrained(checkpoint)\n",
    "\n",
    "    # Update the paramaters in the config\n",
    "    for param in [\n",
    "        \"num_layers\",\n",
    "        \"num_heads\",\n",
    "        \"hidden_size\",\n",
    "        \"intermediate_size\",\n",
    "    ]:\n",
    "        chosen_idx = trial.suggest_int(param, 0, len(search_space[param]) - 1)\n",
    "        setattr(config, param, search_space[param][chosen_idx])\n",
    "\n",
    "    trial_model = AutoModelForSequenceClassification.from_config(config)\n",
    "\n",
    "    for name, layer in trial_model.named_modules():\n",
    "        if isinstance(layer, nn.Linear) and layer.in_features == layer.out_features:\n",
    "            new_layer_cls = trial.suggest_categorical(\n",
    "                f\"{name}_type\",\n",
    "                search_space[\"linear_layer_choices\"],\n",
    "            )\n",
    "\n",
    "            if new_layer_cls == nn.Linear:\n",
    "                continue\n",
    "            elif new_layer_cls == Identity:\n",
    "                new_layer = Identity()\n",
    "                deepsetattr(trial_model, name, new_layer)\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown layer type: {new_layer_cls}\")\n",
    "\n",
    "    return trial_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Defining the Objective Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define the objective function for the search, which gets called on each trial. In each trial, we create a new model instace with chosen hyperparameters according to the defined sampler. We then use the `get_trainer` utility in Mase to run a training loop on the IMDb dataset for a number of epochs. Finally, we use `evaluate` to report back the classification accuracy on the test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chop.tools import get_trainer\n",
    "import torch\n",
    "\n",
    "def objective(trial):\n",
    "    \n",
    "    # Define the model and move to GPU\n",
    "    model = construct_model(trial)\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model.to(device)\n",
    "\n",
    "    trainer = get_trainer(\n",
    "        model=model,\n",
    "        tokenized_dataset=dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        evaluate_metric=\"accuracy\",\n",
    "        num_train_epochs=1,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    eval_results = trainer.evaluate()\n",
    "\n",
    "    # Move back to CPU for storage\n",
    "    trial.set_user_attr(\"model\", model.cpu())\n",
    "\n",
    "    return eval_results[\"eval_accuracy\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Launching the Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optuna provides a number of samplers, for example:\n",
    "\n",
    "* **GridSampler**: iterates through every possible combination of hyperparameters in the search space\n",
    "* **RandomSampler**: chooses a random combination of hyperparameters in each iteration\n",
    "* **TPESampler**: uses Tree-structured Parzen Estimator algorithm to choose hyperparameter values.\n",
    "\n",
    "You can define the chosen sampler by simply importing from `optuna.samplers` as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optuna.samplers import GridSampler, RandomSampler, TPESampler\n",
    "\n",
    "sampler = RandomSampler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all the pieces in place, we can launch the search as follows. The number of trials is set to 1 so you can go get a coffee for 10 minutes, then proceed with the tutorial. However, this will essentially be a random model - for better results, set this to 100 and leave it running overnight!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-01-23 11:42:20,204] A new study created in memory with name: bert-tiny-nas-study\n",
      "/home/neil/adls/adls_group_14/.venv/lib/python3.11/site-packages/optuna/distributions.py:502: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'torch.nn.modules.linear.Linear'> which is of type type.\n",
      "  warnings.warn(message)\n",
      "/home/neil/adls/adls_group_14/.venv/lib/python3.11/site-packages/optuna/distributions.py:502: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'chop.nn.modules.identity.Identity'> which is of type type.\n",
      "  warnings.warn(message)\n",
      "/home/neil/adls/adls_group_14/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 16:46, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.694000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.633000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.461100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.406400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.377400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.386500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 05:18]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-01-23 12:04:29,355] Trial 0 finished with value: 0.84664 and parameters: {'num_layers': 0, 'num_heads': 2, 'hidden_size': 0, 'intermediate_size': 4, 'bert.encoder.layer.0.attention.self.query_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.attention.self.key_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.attention.self.value_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.attention.output.dense_type': <class 'chop.nn.modules.identity.Identity'>, 'bert.encoder.layer.1.attention.self.query_type': <class 'chop.nn.modules.identity.Identity'>, 'bert.encoder.layer.1.attention.self.key_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.attention.self.value_type': <class 'chop.nn.modules.identity.Identity'>, 'bert.encoder.layer.1.attention.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.pooler.dense_type': <class 'torch.nn.modules.linear.Linear'>}. Best is trial 0 with value: 0.84664.\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "\n",
    "study = optuna.create_study(\n",
    "    direction=\"maximize\",\n",
    "    study_name=\"bert-tiny-nas-study\",\n",
    "    sampler=sampler,\n",
    ")\n",
    "\n",
    "study.optimize(\n",
    "    objective,\n",
    "    n_trials=1,\n",
    "    timeout=60 * 60 * 24,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetch the model associated with the best trial as follows, and export to be used in future tutorials. In Tutorial 6, we'll see how to run mixed-precision quantization search on top of the model we've just found through NAS to further find the optimal quantization mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import dill\n",
    "\n",
    "model = study.best_trial.user_attrs[\"model\"].cpu()\n",
    "\n",
    "with open(f\"{Path.home()}/tutorial_5_best_model.pkl\", \"wb\") as f:\n",
    "    dill.dump(model, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying the Optimized Model with CompressionPipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can run the CompressionPipeline in Mase to run uniform quantization and pruning over the searched model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`past_key_values` were not specified as input names, but model.config.use_cache = True. Setting model.config.use_cache = False.\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mGetting dummy input for prajjwal1/bert-tiny.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 101, 9932, 2089, 2202, 2058, 1996, 2088, 2028, 2154,  102],\n",
      "        [ 101, 2023, 2003, 2339, 2017, 2323, 4553, 4748, 4877,  102]])\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "tensor([[ 101, 9932, 2089, 2202, 2058, 1996, 2088, 2028, 2154,  102],\n",
      "        [ 101, 2023, 2003, 2339, 2017, 2323, 4553, 4748, 4877,  102]])\n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])\n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])\n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])\n",
      "tensor([[[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]]])\n",
      "tensor([[[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]]])\n",
      "tensor([[[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]]])\n",
      "tensor([[[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]]])\n",
      "tensor([[[-2.7848,  0.6903,  0.0748,  ...,  1.2979,  2.2950, -1.0034],\n",
      "         [-0.1152,  0.8773,  0.4429,  ...,  0.1657,  1.2624,  0.6311],\n",
      "         [ 1.2829,  0.7150,  0.5566,  ..., -0.1172,  1.0775, -0.4503],\n",
      "         ...,\n",
      "         [ 0.4918, -0.7880,  0.7300,  ...,  0.9211,  2.1163,  0.9393],\n",
      "         [ 0.8289,  0.7764,  0.0999,  ...,  0.8406,  1.9082,  0.4747],\n",
      "         [ 0.1125, -0.1866, -0.3279,  ...,  0.1215,  1.8005,  0.7874]],\n",
      "\n",
      "        [[-2.7848,  0.6903,  0.0748,  ...,  1.2979,  2.2950, -1.0034],\n",
      "         [-0.2906,  0.8570,  1.1283,  ..., -0.7093,  2.1347,  0.8213],\n",
      "         [ 2.0172,  0.0286, -0.0612,  ...,  1.1738,  1.3392,  0.2884],\n",
      "         ...,\n",
      "         [ 0.8422, -1.2634,  0.2478,  ...,  0.7810,  3.6310,  0.8491],\n",
      "         [-0.3479,  1.8140,  0.8316,  ...,  0.9450,  0.7788,  1.0316],\n",
      "         [ 0.1125, -0.1866, -0.3279,  ...,  0.1215,  1.8005,  0.7874]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[[-0.7412, -0.3804, -0.8460,  ..., -0.4371, -1.2164, -1.3986],\n",
      "         [-0.4100,  0.1004, -0.4044,  ..., -0.1055, -0.5008, -0.6416],\n",
      "         [-0.2607, -0.1195, -0.6848,  ..., -0.2281, -0.6319, -0.5957],\n",
      "         ...,\n",
      "         [-0.1480,  0.0111, -0.3225,  ..., -0.0623, -0.6074, -0.1067],\n",
      "         [-0.3695, -0.3713, -0.2849,  ..., -0.3794, -0.5888, -0.7569],\n",
      "         [-0.1104,  0.0506, -0.4374,  ..., -0.1904, -0.6011, -0.5660]],\n",
      "\n",
      "        [[-0.7412, -0.3804, -0.8460,  ..., -0.4371, -1.2164, -1.3986],\n",
      "         [-0.4088, -0.4402, -0.3554,  ..., -0.1222, -1.1231, -0.8788],\n",
      "         [-0.0330,  0.0906, -0.5880,  ..., -0.3124, -0.4530, -0.3067],\n",
      "         ...,\n",
      "         [-0.0388, -0.1259, -0.1314,  ...,  0.0194, -0.6458,  0.1440],\n",
      "         [-0.3649, -0.1592, -0.5686,  ..., -0.2025, -0.2002, -0.8242],\n",
      "         [-0.1104,  0.0506, -0.4374,  ..., -0.1904, -0.6011, -0.5660]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[-0.7412, -0.3804, -0.8460,  ..., -0.4371, -1.2164, -1.3986],\n",
      "         [-0.4100,  0.1004, -0.4044,  ..., -0.1055, -0.5008, -0.6416],\n",
      "         [-0.2607, -0.1195, -0.6848,  ..., -0.2281, -0.6319, -0.5957],\n",
      "         ...,\n",
      "         [-0.1480,  0.0111, -0.3225,  ..., -0.0623, -0.6074, -0.1067],\n",
      "         [-0.3695, -0.3713, -0.2849,  ..., -0.3794, -0.5888, -0.7569],\n",
      "         [-0.1104,  0.0506, -0.4374,  ..., -0.1904, -0.6011, -0.5660]],\n",
      "\n",
      "        [[-0.7412, -0.3804, -0.8460,  ..., -0.4371, -1.2164, -1.3986],\n",
      "         [-0.4088, -0.4402, -0.3554,  ..., -0.1222, -1.1231, -0.8788],\n",
      "         [-0.0330,  0.0906, -0.5880,  ..., -0.3124, -0.4530, -0.3067],\n",
      "         ...,\n",
      "         [-0.0388, -0.1259, -0.1314,  ...,  0.0194, -0.6458,  0.1440],\n",
      "         [-0.3649, -0.1592, -0.5686,  ..., -0.2025, -0.2002, -0.8242],\n",
      "         [-0.1104,  0.0506, -0.4374,  ..., -0.1904, -0.6011, -0.5660]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[[-0.7412, -0.3804, -0.8460,  ..., -1.4684, -1.0241, -0.2865],\n",
      "          [ 0.6948,  1.3182, -1.3075,  ..., -0.4371, -1.2164, -1.3986]],\n",
      "\n",
      "         [[-0.4100,  0.1004, -0.4044,  ..., -0.3182, -0.3609,  0.1743],\n",
      "          [ 0.5498,  0.3717, -0.4043,  ..., -0.1055, -0.5008, -0.6416]],\n",
      "\n",
      "         [[-0.2607, -0.1195, -0.6848,  ..., -0.7670, -0.4483, -0.0141],\n",
      "          [ 0.1719,  0.4026, -0.6285,  ..., -0.2281, -0.6319, -0.5957]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.1480,  0.0111, -0.3225,  ..., -0.3883, -0.1816,  0.0427],\n",
      "          [-0.1812,  0.4724, -0.6719,  ..., -0.0623, -0.6074, -0.1067]],\n",
      "\n",
      "         [[-0.3695, -0.3713, -0.2849,  ..., -0.9171, -0.7159,  0.2554],\n",
      "          [ 0.6924,  0.4391, -0.7644,  ..., -0.3794, -0.5888, -0.7569]],\n",
      "\n",
      "         [[-0.1104,  0.0506, -0.4374,  ..., -0.1900, -0.1476,  0.2160],\n",
      "          [ 0.3638,  0.5001, -0.2436,  ..., -0.1904, -0.6011, -0.5660]]],\n",
      "\n",
      "\n",
      "        [[[-0.7412, -0.3804, -0.8460,  ..., -1.4684, -1.0241, -0.2865],\n",
      "          [ 0.6948,  1.3182, -1.3075,  ..., -0.4371, -1.2164, -1.3986]],\n",
      "\n",
      "         [[-0.4088, -0.4402, -0.3554,  ..., -0.8156, -0.6480,  0.1249],\n",
      "          [ 0.3657,  0.5022, -0.4090,  ..., -0.1222, -1.1231, -0.8788]],\n",
      "\n",
      "         [[-0.0330,  0.0906, -0.5880,  ..., -0.6303, -0.4818, -0.1354],\n",
      "          [ 0.1159,  0.2703, -0.6950,  ..., -0.3124, -0.4530, -0.3067]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0388, -0.1259, -0.1314,  ..., -0.4360, -0.0777,  0.1085],\n",
      "          [ 0.1535,  0.3869, -0.4417,  ...,  0.0194, -0.6458,  0.1440]],\n",
      "\n",
      "         [[-0.3649, -0.1592, -0.5686,  ..., -0.5114, -0.6436, -0.0467],\n",
      "          [ 0.5243,  0.4281, -0.3406,  ..., -0.2025, -0.2002, -0.8242]],\n",
      "\n",
      "         [[-0.1104,  0.0506, -0.4374,  ..., -0.1900, -0.1476,  0.2160],\n",
      "          [ 0.3638,  0.5001, -0.2436,  ..., -0.1904, -0.6011, -0.5660]]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[-0.0974, -0.0419,  0.2999,  ...,  0.7525,  0.2172,  0.2374],\n",
      "         [-0.2668, -0.1748,  0.4762,  ...,  0.3301, -0.0036, -0.0977],\n",
      "         [-0.0156, -0.0499,  0.1641,  ...,  0.3254,  0.0016,  0.1563],\n",
      "         ...,\n",
      "         [-0.1696,  0.0340,  0.6925,  ...,  0.0798, -0.0088,  0.3424],\n",
      "         [-0.0981,  0.0194,  0.0728,  ...,  0.5733,  0.0337,  0.0745],\n",
      "         [ 0.0333,  0.2421,  1.0494,  ...,  0.3243,  0.0985,  0.3347]],\n",
      "\n",
      "        [[-0.0974, -0.0419,  0.2999,  ...,  0.7525,  0.2172,  0.2374],\n",
      "         [-0.2580,  0.0973,  0.3380,  ...,  0.1433,  0.1222,  0.4364],\n",
      "         [ 0.1091, -0.0317,  0.2976,  ...,  0.0798, -0.0255, -0.1473],\n",
      "         ...,\n",
      "         [-0.3029, -0.0208,  0.5885,  ..., -0.1004, -0.2212,  0.3332],\n",
      "         [-0.1652, -0.0959, -0.3040,  ...,  0.2133, -0.2691, -0.0334],\n",
      "         [ 0.0333,  0.2421,  1.0494,  ...,  0.3243,  0.0985,  0.3347]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[-0.0974, -0.0419,  0.2999,  ...,  0.7525,  0.2172,  0.2374],\n",
      "         [-0.2668, -0.1748,  0.4762,  ...,  0.3301, -0.0036, -0.0977],\n",
      "         [-0.0156, -0.0499,  0.1641,  ...,  0.3254,  0.0016,  0.1563],\n",
      "         ...,\n",
      "         [-0.1696,  0.0340,  0.6925,  ...,  0.0798, -0.0088,  0.3424],\n",
      "         [-0.0981,  0.0194,  0.0728,  ...,  0.5733,  0.0337,  0.0745],\n",
      "         [ 0.0333,  0.2421,  1.0494,  ...,  0.3243,  0.0985,  0.3347]],\n",
      "\n",
      "        [[-0.0974, -0.0419,  0.2999,  ...,  0.7525,  0.2172,  0.2374],\n",
      "         [-0.2580,  0.0973,  0.3380,  ...,  0.1433,  0.1222,  0.4364],\n",
      "         [ 0.1091, -0.0317,  0.2976,  ...,  0.0798, -0.0255, -0.1473],\n",
      "         ...,\n",
      "         [-0.3029, -0.0208,  0.5885,  ..., -0.1004, -0.2212,  0.3332],\n",
      "         [-0.1652, -0.0959, -0.3040,  ...,  0.2133, -0.2691, -0.0334],\n",
      "         [ 0.0333,  0.2421,  1.0494,  ...,  0.3243,  0.0985,  0.3347]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[[-0.0974, -0.0419,  0.2999,  ...,  0.3999,  0.0782,  0.0380],\n",
      "          [-0.3328,  0.1809,  0.5030,  ...,  0.7525,  0.2172,  0.2374]],\n",
      "\n",
      "         [[-0.2668, -0.1748,  0.4762,  ..., -0.0604, -0.3558, -0.0315],\n",
      "          [-0.3605,  0.4958, -0.1143,  ...,  0.3301, -0.0036, -0.0977]],\n",
      "\n",
      "         [[-0.0156, -0.0499,  0.1641,  ...,  0.0354,  0.1306, -0.0538],\n",
      "          [-0.3370,  0.2358,  0.1032,  ...,  0.3254,  0.0016,  0.1563]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.1696,  0.0340,  0.6925,  ...,  0.0983,  0.3532,  0.2897],\n",
      "          [-0.5079, -0.0419,  0.4354,  ...,  0.0798, -0.0088,  0.3424]],\n",
      "\n",
      "         [[-0.0981,  0.0194,  0.0728,  ...,  0.1028,  0.1641,  0.3848],\n",
      "          [-0.4029,  0.1176,  0.2699,  ...,  0.5733,  0.0337,  0.0745]],\n",
      "\n",
      "         [[ 0.0333,  0.2421,  1.0494,  ...,  0.2009,  0.2320,  0.3458],\n",
      "          [-0.2176, -0.2196, -0.0680,  ...,  0.3243,  0.0985,  0.3347]]],\n",
      "\n",
      "\n",
      "        [[[-0.0974, -0.0419,  0.2999,  ...,  0.3999,  0.0782,  0.0380],\n",
      "          [-0.3328,  0.1809,  0.5030,  ...,  0.7525,  0.2172,  0.2374]],\n",
      "\n",
      "         [[-0.2580,  0.0973,  0.3380,  ...,  0.3008, -0.2675,  0.3273],\n",
      "          [-0.4787,  0.2026,  0.3107,  ...,  0.1433,  0.1222,  0.4364]],\n",
      "\n",
      "         [[ 0.1091, -0.0317,  0.2976,  ...,  0.1426,  0.1900,  0.2218],\n",
      "          [-0.1310,  0.2211,  0.0468,  ...,  0.0798, -0.0255, -0.1473]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.3029, -0.0208,  0.5885,  ...,  0.4611,  0.0018,  0.2030],\n",
      "          [-0.1511,  0.0149,  0.4649,  ..., -0.1004, -0.2212,  0.3332]],\n",
      "\n",
      "         [[-0.1652, -0.0959, -0.3040,  ...,  0.1476,  0.0758,  0.5442],\n",
      "          [ 0.0185,  0.0567,  0.2480,  ...,  0.2133, -0.2691, -0.0334]],\n",
      "\n",
      "         [[ 0.0333,  0.2421,  1.0494,  ...,  0.2009,  0.2320,  0.3458],\n",
      "          [-0.2176, -0.2196, -0.0680,  ...,  0.3243,  0.0985,  0.3347]]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[-0.0708,  0.3246, -0.1697,  ...,  0.0190, -0.0840, -0.0289],\n",
      "         [ 0.0585,  0.0735,  0.0792,  ...,  0.0719, -0.0654,  0.0896],\n",
      "         [-0.1888,  0.0963,  0.2164,  ..., -0.2454, -0.1592,  0.0505],\n",
      "         ...,\n",
      "         [ 0.2679,  0.2498,  0.0784,  ..., -0.3999,  0.2649,  0.0094],\n",
      "         [ 0.2560, -0.3655,  0.3296,  ...,  0.0855,  0.3703,  0.1669],\n",
      "         [-0.0055, -0.0194,  0.5981,  ...,  0.0550,  0.2144,  0.5355]],\n",
      "\n",
      "        [[-0.0708,  0.3246, -0.1697,  ...,  0.0190, -0.0840, -0.0289],\n",
      "         [-0.0132,  0.0629, -0.4395,  ..., -0.0203,  0.1557, -0.1749],\n",
      "         [ 0.0988,  0.1147,  0.5807,  ...,  0.2224,  0.6068,  0.4321],\n",
      "         ...,\n",
      "         [ 0.2206,  0.3146,  0.1077,  ...,  0.0363,  0.6480,  0.2988],\n",
      "         [-0.0248, -0.2386,  0.1215,  ...,  0.4336,  0.2676,  0.1217],\n",
      "         [-0.0055, -0.0194,  0.5981,  ...,  0.0550,  0.2144,  0.5355]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[-0.0708,  0.3246, -0.1697,  ...,  0.0190, -0.0840, -0.0289],\n",
      "         [ 0.0585,  0.0735,  0.0792,  ...,  0.0719, -0.0654,  0.0896],\n",
      "         [-0.1888,  0.0963,  0.2164,  ..., -0.2454, -0.1592,  0.0505],\n",
      "         ...,\n",
      "         [ 0.2679,  0.2498,  0.0784,  ..., -0.3999,  0.2649,  0.0094],\n",
      "         [ 0.2560, -0.3655,  0.3296,  ...,  0.0855,  0.3703,  0.1669],\n",
      "         [-0.0055, -0.0194,  0.5981,  ...,  0.0550,  0.2144,  0.5355]],\n",
      "\n",
      "        [[-0.0708,  0.3246, -0.1697,  ...,  0.0190, -0.0840, -0.0289],\n",
      "         [-0.0132,  0.0629, -0.4395,  ..., -0.0203,  0.1557, -0.1749],\n",
      "         [ 0.0988,  0.1147,  0.5807,  ...,  0.2224,  0.6068,  0.4321],\n",
      "         ...,\n",
      "         [ 0.2206,  0.3146,  0.1077,  ...,  0.0363,  0.6480,  0.2988],\n",
      "         [-0.0248, -0.2386,  0.1215,  ...,  0.4336,  0.2676,  0.1217],\n",
      "         [-0.0055, -0.0194,  0.5981,  ...,  0.0550,  0.2144,  0.5355]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[[-0.0708,  0.3246, -0.1697,  ...,  0.2983, -0.1068, -0.0651],\n",
      "          [ 0.0789,  0.1643, -0.4012,  ...,  0.0190, -0.0840, -0.0289]],\n",
      "\n",
      "         [[ 0.0585,  0.0735,  0.0792,  ...,  0.1622,  0.0227, -0.3316],\n",
      "          [-0.0287, -0.4447,  0.0216,  ...,  0.0719, -0.0654,  0.0896]],\n",
      "\n",
      "         [[-0.1888,  0.0963,  0.2164,  ...,  0.0783, -0.2140, -0.0477],\n",
      "          [ 0.1991, -0.1073, -0.0446,  ..., -0.2454, -0.1592,  0.0505]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.2679,  0.2498,  0.0784,  ...,  0.0271, -0.1917, -0.1820],\n",
      "          [-0.1191,  0.1309, -0.2637,  ..., -0.3999,  0.2649,  0.0094]],\n",
      "\n",
      "         [[ 0.2560, -0.3655,  0.3296,  ..., -0.2423, -0.2244,  0.1101],\n",
      "          [-0.1970, -0.5031, -0.1664,  ...,  0.0855,  0.3703,  0.1669]],\n",
      "\n",
      "         [[-0.0055, -0.0194,  0.5981,  ...,  0.0521, -0.3181, -0.5965],\n",
      "          [-0.0684, -0.2745, -0.5289,  ...,  0.0550,  0.2144,  0.5355]]],\n",
      "\n",
      "\n",
      "        [[[-0.0708,  0.3246, -0.1697,  ...,  0.2983, -0.1068, -0.0651],\n",
      "          [ 0.0789,  0.1643, -0.4012,  ...,  0.0190, -0.0840, -0.0289]],\n",
      "\n",
      "         [[-0.0132,  0.0629, -0.4395,  ...,  0.1806, -0.0053,  0.1124],\n",
      "          [-0.2640,  0.2502,  0.0863,  ..., -0.0203,  0.1557, -0.1749]],\n",
      "\n",
      "         [[ 0.0988,  0.1147,  0.5807,  ..., -0.1693, -0.7897, -0.2951],\n",
      "          [ 0.3076, -0.0234, -0.4624,  ...,  0.2224,  0.6068,  0.4321]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.2206,  0.3146,  0.1077,  ..., -0.0184, -0.1327, -0.2403],\n",
      "          [ 0.0304,  0.0320, -0.4254,  ...,  0.0363,  0.6480,  0.2988]],\n",
      "\n",
      "         [[-0.0248, -0.2386,  0.1215,  ..., -0.1517,  0.0923, -0.1184],\n",
      "          [-0.1223, -0.2442, -0.0418,  ...,  0.4336,  0.2676,  0.1217]],\n",
      "\n",
      "         [[-0.0055, -0.0194,  0.5981,  ...,  0.0521, -0.3181, -0.5965],\n",
      "          [-0.0684, -0.2745, -0.5289,  ...,  0.0550,  0.2144,  0.5355]]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[[ 0.0900, -0.1697,  0.2524,  ...,  0.0447, -0.3236, -0.2939],\n",
      "          [ 0.0832, -0.0857,  0.2322,  ...,  0.0779, -0.2523, -0.2794],\n",
      "          [ 0.0818, -0.0786,  0.2281,  ...,  0.0825, -0.2449, -0.2820],\n",
      "          ...,\n",
      "          [ 0.0820, -0.0727,  0.2322,  ...,  0.0863, -0.2417, -0.2888],\n",
      "          [ 0.0829, -0.0911,  0.2318,  ...,  0.0759, -0.2563, -0.2838],\n",
      "          [ 0.0830, -0.0658,  0.2308,  ...,  0.0866, -0.2318, -0.2843]],\n",
      "\n",
      "         [[ 0.0787, -0.3700, -0.3788,  ..., -0.0089,  0.2339,  0.3308],\n",
      "          [ 0.0244, -0.2645, -0.3625,  ..., -0.0231,  0.1515,  0.2744],\n",
      "          [ 0.0307, -0.2713, -0.3558,  ..., -0.0250,  0.1512,  0.2730],\n",
      "          ...,\n",
      "          [ 0.0199, -0.2550, -0.3518,  ..., -0.0260,  0.1405,  0.2625],\n",
      "          [ 0.0270, -0.2724, -0.3508,  ..., -0.0234,  0.1482,  0.2705],\n",
      "          [ 0.0190, -0.2515, -0.3564,  ..., -0.0262,  0.1403,  0.2632]]],\n",
      "\n",
      "\n",
      "        [[[-0.1299,  0.0332,  0.0722,  ...,  0.0925,  0.0293, -0.1332],\n",
      "          [-0.0969,  0.0343,  0.0923,  ...,  0.0870, -0.0376, -0.1660],\n",
      "          [-0.0926,  0.0430,  0.0967,  ...,  0.0928, -0.0534, -0.1725],\n",
      "          ...,\n",
      "          [-0.0781,  0.0425,  0.1115,  ...,  0.0858, -0.0821, -0.1883],\n",
      "          [-0.0918,  0.0329,  0.0995,  ...,  0.0894, -0.0565, -0.1768],\n",
      "          [-0.0828,  0.0384,  0.1068,  ...,  0.0869, -0.0728, -0.1817]],\n",
      "\n",
      "         [[-0.1501,  0.0456, -0.1453,  ..., -0.0466,  0.1130,  0.0944],\n",
      "          [-0.0986,  0.0107, -0.1984,  ...,  0.0056,  0.1741,  0.1335],\n",
      "          [-0.0787, -0.0057, -0.2243,  ...,  0.0214,  0.2036,  0.1577],\n",
      "          ...,\n",
      "          [-0.0778, -0.0040, -0.2253,  ...,  0.0217,  0.2024,  0.1554],\n",
      "          [-0.0837, -0.0022, -0.2145,  ...,  0.0248,  0.1918,  0.1469],\n",
      "          [-0.0802, -0.0020, -0.2201,  ...,  0.0206,  0.1972,  0.1509]]]],\n",
      "       grad_fn=<ScaledDotProductFlashAttentionForCpuBackward0>)\n",
      "tensor([[[[ 0.0900, -0.1697,  0.2524,  ...,  0.0447, -0.3236, -0.2939],\n",
      "          [ 0.0787, -0.3700, -0.3788,  ..., -0.0089,  0.2339,  0.3308]],\n",
      "\n",
      "         [[ 0.0832, -0.0857,  0.2322,  ...,  0.0779, -0.2523, -0.2794],\n",
      "          [ 0.0244, -0.2645, -0.3625,  ..., -0.0231,  0.1515,  0.2744]],\n",
      "\n",
      "         [[ 0.0818, -0.0786,  0.2281,  ...,  0.0825, -0.2449, -0.2820],\n",
      "          [ 0.0307, -0.2713, -0.3558,  ..., -0.0250,  0.1512,  0.2730]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0820, -0.0727,  0.2322,  ...,  0.0863, -0.2417, -0.2888],\n",
      "          [ 0.0199, -0.2550, -0.3518,  ..., -0.0260,  0.1405,  0.2625]],\n",
      "\n",
      "         [[ 0.0829, -0.0911,  0.2318,  ...,  0.0759, -0.2563, -0.2838],\n",
      "          [ 0.0270, -0.2724, -0.3508,  ..., -0.0234,  0.1482,  0.2705]],\n",
      "\n",
      "         [[ 0.0830, -0.0658,  0.2308,  ...,  0.0866, -0.2318, -0.2843],\n",
      "          [ 0.0190, -0.2515, -0.3564,  ..., -0.0262,  0.1403,  0.2632]]],\n",
      "\n",
      "\n",
      "        [[[-0.1299,  0.0332,  0.0722,  ...,  0.0925,  0.0293, -0.1332],\n",
      "          [-0.1501,  0.0456, -0.1453,  ..., -0.0466,  0.1130,  0.0944]],\n",
      "\n",
      "         [[-0.0969,  0.0343,  0.0923,  ...,  0.0870, -0.0376, -0.1660],\n",
      "          [-0.0986,  0.0107, -0.1984,  ...,  0.0056,  0.1741,  0.1335]],\n",
      "\n",
      "         [[-0.0926,  0.0430,  0.0967,  ...,  0.0928, -0.0534, -0.1725],\n",
      "          [-0.0787, -0.0057, -0.2243,  ...,  0.0214,  0.2036,  0.1577]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0781,  0.0425,  0.1115,  ...,  0.0858, -0.0821, -0.1883],\n",
      "          [-0.0778, -0.0040, -0.2253,  ...,  0.0217,  0.2024,  0.1554]],\n",
      "\n",
      "         [[-0.0918,  0.0329,  0.0995,  ...,  0.0894, -0.0565, -0.1768],\n",
      "          [-0.0837, -0.0022, -0.2145,  ...,  0.0248,  0.1918,  0.1469]],\n",
      "\n",
      "         [[-0.0828,  0.0384,  0.1068,  ...,  0.0869, -0.0728, -0.1817],\n",
      "          [-0.0802, -0.0020, -0.2201,  ...,  0.0206,  0.1972,  0.1509]]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "tensor([[[-2.3082,  0.7651,  0.5504,  ...,  1.5358,  2.5475, -0.5222],\n",
      "         [ 0.1283,  0.9206,  0.6221,  ...,  0.3005,  1.4119,  0.8977],\n",
      "         [ 1.4134,  0.6622,  0.7189,  ...,  0.0411,  1.2354,  0.1287],\n",
      "         ...,\n",
      "         [ 0.6860, -0.8400,  0.8861,  ...,  1.1734,  2.3122,  1.3912],\n",
      "         [ 0.6597,  0.3663,  0.4045,  ...,  0.9252,  1.9168,  0.9830],\n",
      "         [ 0.3159, -0.3887, -0.1658,  ...,  0.6615,  1.9060,  1.3306]],\n",
      "\n",
      "        [[-2.7485,  1.1340,  0.2809,  ...,  1.4203,  2.3373, -1.0806],\n",
      "         [-0.3841,  1.2072,  0.9493,  ..., -0.8414,  1.8459,  0.7758],\n",
      "         [ 1.9556, -0.0805,  0.0745,  ...,  1.4761,  1.6672,  0.7070],\n",
      "         ...,\n",
      "         [ 0.9752, -1.2310,  0.2695,  ...,  1.0980,  3.8330,  1.0806],\n",
      "         [-0.3269,  1.4608,  1.0954,  ...,  1.0098,  1.0093,  1.4375],\n",
      "         [ 0.1361, -0.2443, -0.3630,  ...,  0.6604,  1.9624,  1.1708]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[[-2.3082,  0.7651,  0.5504,  ...,  1.5358,  2.5475, -0.5222],\n",
      "         [ 0.1283,  0.9206,  0.6221,  ...,  0.3005,  1.4119,  0.8977],\n",
      "         [ 1.4134,  0.6622,  0.7189,  ...,  0.0411,  1.2354,  0.1287],\n",
      "         ...,\n",
      "         [ 0.6860, -0.8400,  0.8861,  ...,  1.1734,  2.3122,  1.3912],\n",
      "         [ 0.6597,  0.3663,  0.4045,  ...,  0.9252,  1.9168,  0.9830],\n",
      "         [ 0.3159, -0.3887, -0.1658,  ...,  0.6615,  1.9060,  1.3306]],\n",
      "\n",
      "        [[-2.7485,  1.1340,  0.2809,  ...,  1.4203,  2.3373, -1.0806],\n",
      "         [-0.3841,  1.2072,  0.9493,  ..., -0.8414,  1.8459,  0.7758],\n",
      "         [ 1.9556, -0.0805,  0.0745,  ...,  1.4761,  1.6672,  0.7070],\n",
      "         ...,\n",
      "         [ 0.9752, -1.2310,  0.2695,  ...,  1.0980,  3.8330,  1.0806],\n",
      "         [-0.3269,  1.4608,  1.0954,  ...,  1.0098,  1.0093,  1.4375],\n",
      "         [ 0.1361, -0.2443, -0.3630,  ...,  0.6604,  1.9624,  1.1708]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[[-2.3082,  0.7651,  0.5504,  ...,  1.5358,  2.5475, -0.5222],\n",
      "         [ 0.1283,  0.9206,  0.6221,  ...,  0.3005,  1.4119,  0.8977],\n",
      "         [ 1.4134,  0.6622,  0.7189,  ...,  0.0411,  1.2354,  0.1287],\n",
      "         ...,\n",
      "         [ 0.6860, -0.8400,  0.8861,  ...,  1.1734,  2.3122,  1.3912],\n",
      "         [ 0.6597,  0.3663,  0.4045,  ...,  0.9252,  1.9168,  0.9830],\n",
      "         [ 0.3159, -0.3887, -0.1658,  ...,  0.6615,  1.9060,  1.3306]],\n",
      "\n",
      "        [[-2.7485,  1.1340,  0.2809,  ...,  1.4203,  2.3373, -1.0806],\n",
      "         [-0.3841,  1.2072,  0.9493,  ..., -0.8414,  1.8459,  0.7758],\n",
      "         [ 1.9556, -0.0805,  0.0745,  ...,  1.4761,  1.6672,  0.7070],\n",
      "         ...,\n",
      "         [ 0.9752, -1.2310,  0.2695,  ...,  1.0980,  3.8330,  1.0806],\n",
      "         [-0.3269,  1.4608,  1.0954,  ...,  1.0098,  1.0093,  1.4375],\n",
      "         [ 0.1361, -0.2443, -0.3630,  ...,  0.6604,  1.9624,  1.1708]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[[[-2.3082,  0.7651,  0.5504,  ...,  0.7859, -0.6492, -0.1826],\n",
      "          [-0.5673, -1.0926,  0.1068,  ...,  1.5358,  2.5475, -0.5222]],\n",
      "\n",
      "         [[ 0.1283,  0.9206,  0.6221,  ..., -1.9348,  1.4713, -0.1107],\n",
      "          [ 0.1369,  0.3139,  0.5301,  ...,  0.3005,  1.4119,  0.8977]],\n",
      "\n",
      "         [[ 1.4134,  0.6622,  0.7189,  ...,  0.3717, -0.1601, -0.8259],\n",
      "          [ 0.0255, -0.5125, -0.7821,  ...,  0.0411,  1.2354,  0.1287]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.6860, -0.8400,  0.8861,  ...,  0.8988,  0.5393,  0.6203],\n",
      "          [ 1.1425, -1.2861, -1.0775,  ...,  1.1734,  2.3122,  1.3912]],\n",
      "\n",
      "         [[ 0.6597,  0.3663,  0.4045,  ..., -0.4761, -1.2813, -1.1802],\n",
      "          [ 1.2211, -1.1852, -1.6645,  ...,  0.9252,  1.9168,  0.9830]],\n",
      "\n",
      "         [[ 0.3159, -0.3887, -0.1658,  ...,  0.4399,  0.0549,  0.4000],\n",
      "          [ 2.0346, -0.0538, -0.8923,  ...,  0.6615,  1.9060,  1.3306]]],\n",
      "\n",
      "\n",
      "        [[[-2.7485,  1.1340,  0.2809,  ...,  0.9466, -0.1298,  0.2705],\n",
      "          [-1.2361, -0.5180,  0.7297,  ...,  1.4203,  2.3373, -1.0806]],\n",
      "\n",
      "         [[-0.3841,  1.2072,  0.9493,  ..., -1.0432,  2.3208,  0.4396],\n",
      "          [-1.5662, -0.3035,  0.8778,  ..., -0.8414,  1.8459,  0.7758]],\n",
      "\n",
      "         [[ 1.9556, -0.0805,  0.0745,  ...,  1.0531, -0.0900, -0.2324],\n",
      "          [ 0.9524, -0.2288, -1.7176,  ...,  1.4761,  1.6672,  0.7070]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.9752, -1.2310,  0.2695,  ...,  1.2128, -1.0370,  0.1020],\n",
      "          [ 1.1809, -1.2216, -0.5878,  ...,  1.0980,  3.8330,  1.0806]],\n",
      "\n",
      "         [[-0.3269,  1.4608,  1.0954,  ..., -0.9398, -0.2387,  0.1908],\n",
      "          [-0.1161, -0.2387, -0.1029,  ...,  1.0098,  1.0093,  1.4375]],\n",
      "\n",
      "         [[ 0.1361, -0.2443, -0.3630,  ...,  0.5157,  0.3278,  0.6913],\n",
      "          [ 1.8884,  0.3167, -0.6189,  ...,  0.6604,  1.9624,  1.1708]]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[-0.1469, -0.6403, -0.4336,  ..., -0.4732, -0.1018,  0.0267],\n",
      "         [-0.0110,  0.0113, -0.2932,  ..., -0.0642, -0.2314,  0.2588],\n",
      "         [-0.0568, -0.1494, -0.1746,  ..., -0.3133, -0.1163, -0.1374],\n",
      "         ...,\n",
      "         [ 0.5269, -0.4526,  0.1725,  ..., -0.3702, -0.4076,  0.4215],\n",
      "         [ 0.0810, -0.2180,  0.1474,  ..., -0.2669, -0.1783,  0.2341],\n",
      "         [ 0.5613, -0.5525,  0.0366,  ..., -0.3497, -0.1890,  0.3354]],\n",
      "\n",
      "        [[-0.1598, -0.7371, -0.6059,  ..., -0.4508, -0.0422,  0.0565],\n",
      "         [ 0.3575, -0.6172, -0.4105,  ..., -0.3102, -0.3265,  0.3234],\n",
      "         [ 0.0735, -0.2081,  0.1281,  ..., -0.3561,  0.1196,  0.1379],\n",
      "         ...,\n",
      "         [ 0.3868, -0.7915, -0.0434,  ..., -0.1719, -0.0688,  0.1556],\n",
      "         [ 0.0793, -0.2826,  0.0975,  ..., -0.3293, -0.1849,  0.2143],\n",
      "         [ 0.6043, -0.6314, -0.0417,  ..., -0.3507, -0.1645,  0.3800]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[-0.1469, -0.6403, -0.4336,  ..., -0.4732, -0.1018,  0.0267],\n",
      "         [-0.0110,  0.0113, -0.2932,  ..., -0.0642, -0.2314,  0.2588],\n",
      "         [-0.0568, -0.1494, -0.1746,  ..., -0.3133, -0.1163, -0.1374],\n",
      "         ...,\n",
      "         [ 0.5269, -0.4526,  0.1725,  ..., -0.3702, -0.4076,  0.4215],\n",
      "         [ 0.0810, -0.2180,  0.1474,  ..., -0.2669, -0.1783,  0.2341],\n",
      "         [ 0.5613, -0.5525,  0.0366,  ..., -0.3497, -0.1890,  0.3354]],\n",
      "\n",
      "        [[-0.1598, -0.7371, -0.6059,  ..., -0.4508, -0.0422,  0.0565],\n",
      "         [ 0.3575, -0.6172, -0.4105,  ..., -0.3102, -0.3265,  0.3234],\n",
      "         [ 0.0735, -0.2081,  0.1281,  ..., -0.3561,  0.1196,  0.1379],\n",
      "         ...,\n",
      "         [ 0.3868, -0.7915, -0.0434,  ..., -0.1719, -0.0688,  0.1556],\n",
      "         [ 0.0793, -0.2826,  0.0975,  ..., -0.3293, -0.1849,  0.2143],\n",
      "         [ 0.6043, -0.6314, -0.0417,  ..., -0.3507, -0.1645,  0.3800]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[[-0.1469, -0.6403, -0.4336,  ..., -0.3482,  0.0808,  0.3196],\n",
      "          [ 0.4905, -0.2438, -0.2331,  ..., -0.4732, -0.1018,  0.0267]],\n",
      "\n",
      "         [[-0.0110,  0.0113, -0.2932,  ..., -0.1550,  0.2126,  0.2991],\n",
      "          [ 0.1962, -0.3690, -0.3832,  ..., -0.0642, -0.2314,  0.2588]],\n",
      "\n",
      "         [[-0.0568, -0.1494, -0.1746,  ..., -0.2490, -0.0245,  0.1575],\n",
      "          [-0.1326,  0.2628, -0.3845,  ..., -0.3133, -0.1163, -0.1374]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.5269, -0.4526,  0.1725,  ..., -0.4958, -0.0462,  0.2596],\n",
      "          [ 0.0955, -0.2266, -0.8404,  ..., -0.3702, -0.4076,  0.4215]],\n",
      "\n",
      "         [[ 0.0810, -0.2180,  0.1474,  ..., -0.6140, -0.2138, -0.2661],\n",
      "          [ 0.1962,  0.1977, -0.8744,  ..., -0.2669, -0.1783,  0.2341]],\n",
      "\n",
      "         [[ 0.5613, -0.5525,  0.0366,  ..., -0.5217,  0.3953,  0.1637],\n",
      "          [ 0.5370, -0.1999, -1.0452,  ..., -0.3497, -0.1890,  0.3354]]],\n",
      "\n",
      "\n",
      "        [[[-0.1598, -0.7371, -0.6059,  ..., -0.2130,  0.0618,  0.4512],\n",
      "          [ 0.5208, -0.0442, -0.0815,  ..., -0.4508, -0.0422,  0.0565]],\n",
      "\n",
      "         [[ 0.3575, -0.6172, -0.4105,  ..., -0.4724,  0.4848,  0.4376],\n",
      "          [ 0.1658,  0.3256, -0.5080,  ..., -0.3102, -0.3265,  0.3234]],\n",
      "\n",
      "         [[ 0.0735, -0.2081,  0.1281,  ..., -0.2016, -0.3360,  0.0404],\n",
      "          [-0.3349, -0.2328, -0.5280,  ..., -0.3561,  0.1196,  0.1379]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.3868, -0.7915, -0.0434,  ..., -0.1816,  0.0692, -0.0526],\n",
      "          [-0.2424, -0.4088, -0.8728,  ..., -0.1719, -0.0688,  0.1556]],\n",
      "\n",
      "         [[ 0.0793, -0.2826,  0.0975,  ..., -0.6438, -0.0507,  0.0322],\n",
      "          [ 0.3269,  0.0580, -0.5306,  ..., -0.3293, -0.1849,  0.2143]],\n",
      "\n",
      "         [[ 0.6043, -0.6314, -0.0417,  ..., -0.4747,  0.4116,  0.2329],\n",
      "          [ 0.5692, -0.1159, -1.0422,  ..., -0.3507, -0.1645,  0.3800]]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[-2.3082,  0.7651,  0.5504,  ...,  1.5358,  2.5475, -0.5222],\n",
      "         [ 0.1283,  0.9206,  0.6221,  ...,  0.3005,  1.4119,  0.8977],\n",
      "         [ 1.4134,  0.6622,  0.7189,  ...,  0.0411,  1.2354,  0.1287],\n",
      "         ...,\n",
      "         [ 0.6860, -0.8400,  0.8861,  ...,  1.1734,  2.3122,  1.3912],\n",
      "         [ 0.6597,  0.3663,  0.4045,  ...,  0.9252,  1.9168,  0.9830],\n",
      "         [ 0.3159, -0.3887, -0.1658,  ...,  0.6615,  1.9060,  1.3306]],\n",
      "\n",
      "        [[-2.7485,  1.1340,  0.2809,  ...,  1.4203,  2.3373, -1.0806],\n",
      "         [-0.3841,  1.2072,  0.9493,  ..., -0.8414,  1.8459,  0.7758],\n",
      "         [ 1.9556, -0.0805,  0.0745,  ...,  1.4761,  1.6672,  0.7070],\n",
      "         ...,\n",
      "         [ 0.9752, -1.2310,  0.2695,  ...,  1.0980,  3.8330,  1.0806],\n",
      "         [-0.3269,  1.4608,  1.0954,  ...,  1.0098,  1.0093,  1.4375],\n",
      "         [ 0.1361, -0.2443, -0.3630,  ...,  0.6604,  1.9624,  1.1708]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[[-2.3082,  0.7651,  0.5504,  ...,  1.5358,  2.5475, -0.5222],\n",
      "         [ 0.1283,  0.9206,  0.6221,  ...,  0.3005,  1.4119,  0.8977],\n",
      "         [ 1.4134,  0.6622,  0.7189,  ...,  0.0411,  1.2354,  0.1287],\n",
      "         ...,\n",
      "         [ 0.6860, -0.8400,  0.8861,  ...,  1.1734,  2.3122,  1.3912],\n",
      "         [ 0.6597,  0.3663,  0.4045,  ...,  0.9252,  1.9168,  0.9830],\n",
      "         [ 0.3159, -0.3887, -0.1658,  ...,  0.6615,  1.9060,  1.3306]],\n",
      "\n",
      "        [[-2.7485,  1.1340,  0.2809,  ...,  1.4203,  2.3373, -1.0806],\n",
      "         [-0.3841,  1.2072,  0.9493,  ..., -0.8414,  1.8459,  0.7758],\n",
      "         [ 1.9556, -0.0805,  0.0745,  ...,  1.4761,  1.6672,  0.7070],\n",
      "         ...,\n",
      "         [ 0.9752, -1.2310,  0.2695,  ...,  1.0980,  3.8330,  1.0806],\n",
      "         [-0.3269,  1.4608,  1.0954,  ...,  1.0098,  1.0093,  1.4375],\n",
      "         [ 0.1361, -0.2443, -0.3630,  ...,  0.6604,  1.9624,  1.1708]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[[[-2.3082,  0.7651,  0.5504,  ...,  0.7859, -0.6492, -0.1826],\n",
      "          [-0.5673, -1.0926,  0.1068,  ...,  1.5358,  2.5475, -0.5222]],\n",
      "\n",
      "         [[ 0.1283,  0.9206,  0.6221,  ..., -1.9348,  1.4713, -0.1107],\n",
      "          [ 0.1369,  0.3139,  0.5301,  ...,  0.3005,  1.4119,  0.8977]],\n",
      "\n",
      "         [[ 1.4134,  0.6622,  0.7189,  ...,  0.3717, -0.1601, -0.8259],\n",
      "          [ 0.0255, -0.5125, -0.7821,  ...,  0.0411,  1.2354,  0.1287]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.6860, -0.8400,  0.8861,  ...,  0.8988,  0.5393,  0.6203],\n",
      "          [ 1.1425, -1.2861, -1.0775,  ...,  1.1734,  2.3122,  1.3912]],\n",
      "\n",
      "         [[ 0.6597,  0.3663,  0.4045,  ..., -0.4761, -1.2813, -1.1802],\n",
      "          [ 1.2211, -1.1852, -1.6645,  ...,  0.9252,  1.9168,  0.9830]],\n",
      "\n",
      "         [[ 0.3159, -0.3887, -0.1658,  ...,  0.4399,  0.0549,  0.4000],\n",
      "          [ 2.0346, -0.0538, -0.8923,  ...,  0.6615,  1.9060,  1.3306]]],\n",
      "\n",
      "\n",
      "        [[[-2.7485,  1.1340,  0.2809,  ...,  0.9466, -0.1298,  0.2705],\n",
      "          [-1.2361, -0.5180,  0.7297,  ...,  1.4203,  2.3373, -1.0806]],\n",
      "\n",
      "         [[-0.3841,  1.2072,  0.9493,  ..., -1.0432,  2.3208,  0.4396],\n",
      "          [-1.5662, -0.3035,  0.8778,  ..., -0.8414,  1.8459,  0.7758]],\n",
      "\n",
      "         [[ 1.9556, -0.0805,  0.0745,  ...,  1.0531, -0.0900, -0.2324],\n",
      "          [ 0.9524, -0.2288, -1.7176,  ...,  1.4761,  1.6672,  0.7070]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.9752, -1.2310,  0.2695,  ...,  1.2128, -1.0370,  0.1020],\n",
      "          [ 1.1809, -1.2216, -0.5878,  ...,  1.0980,  3.8330,  1.0806]],\n",
      "\n",
      "         [[-0.3269,  1.4608,  1.0954,  ..., -0.9398, -0.2387,  0.1908],\n",
      "          [-0.1161, -0.2387, -0.1029,  ...,  1.0098,  1.0093,  1.4375]],\n",
      "\n",
      "         [[ 0.1361, -0.2443, -0.3630,  ...,  0.5157,  0.3278,  0.6913],\n",
      "          [ 1.8884,  0.3167, -0.6189,  ...,  0.6604,  1.9624,  1.1708]]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[[-0.2502,  0.1916,  0.6468,  ..., -0.2498, -0.1294, -0.5669],\n",
      "          [-0.1131,  0.1815,  0.5153,  ..., -0.0164, -0.2026, -0.4950],\n",
      "          [-0.1127,  0.2553,  0.4889,  ..., -0.0324, -0.1396, -0.4642],\n",
      "          ...,\n",
      "          [-0.0632,  0.2089,  0.4554,  ...,  0.0394, -0.1776, -0.4562],\n",
      "          [-0.0068,  0.1558,  0.5107,  ..., -0.0260, -0.1577, -0.5035],\n",
      "          [-0.0694,  0.1837,  0.5020,  ...,  0.0347, -0.1825, -0.4828]],\n",
      "\n",
      "         [[ 0.7569, -0.7743, -1.0517,  ...,  0.7334,  1.6199,  0.8744],\n",
      "          [ 0.8545, -0.7449, -0.9325,  ...,  0.6133,  1.6644,  0.8931],\n",
      "          [ 0.9055, -0.7429, -0.9926,  ...,  0.6479,  1.7060,  0.8781],\n",
      "          ...,\n",
      "          [ 0.9016, -0.7733, -0.9184,  ...,  0.6361,  1.7070,  0.8978],\n",
      "          [ 0.9676, -0.7355, -0.9783,  ...,  0.6122,  1.7108,  0.9099],\n",
      "          [ 0.8519, -0.7426, -0.8937,  ...,  0.6593,  1.7450,  0.8492]]],\n",
      "\n",
      "\n",
      "        [[[-0.1789,  0.8188, -0.3516,  ...,  0.2704,  0.6805,  0.5076],\n",
      "          [-0.2730,  0.6272,  0.0153,  ...,  0.2915,  0.4262,  0.3405],\n",
      "          [-0.1819,  0.4895,  0.1813,  ...,  0.4085,  0.1136,  0.1850],\n",
      "          ...,\n",
      "          [-0.1777,  0.4989,  0.1872,  ...,  0.4557,  0.0876,  0.1601],\n",
      "          [-0.0207,  0.2811,  0.1789,  ...,  0.4471, -0.0180,  0.0547],\n",
      "          [-0.0998,  0.3918,  0.2235,  ...,  0.4772, -0.0237,  0.0673]],\n",
      "\n",
      "         [[-0.6077, -0.6956, -0.2468,  ...,  0.4680,  1.1940, -0.3626],\n",
      "          [-0.0921, -0.6624, -0.4816,  ...,  0.8057,  1.6584,  0.3867],\n",
      "          [ 0.0743, -0.6579, -0.5267,  ...,  0.7932,  1.7956,  0.4678],\n",
      "          ...,\n",
      "          [-0.0617, -0.6459, -0.4916,  ...,  0.8121,  1.6992,  0.4797],\n",
      "          [ 0.2770, -0.6372, -0.5914,  ...,  0.8014,  1.7871,  0.5592],\n",
      "          [-0.1670, -0.6105, -0.3007,  ...,  0.7562,  1.7350,  0.3021]]]],\n",
      "       grad_fn=<ScaledDotProductFlashAttentionForCpuBackward0>)\n",
      "tensor([[[[-0.2502,  0.1916,  0.6468,  ..., -0.2498, -0.1294, -0.5669],\n",
      "          [ 0.7569, -0.7743, -1.0517,  ...,  0.7334,  1.6199,  0.8744]],\n",
      "\n",
      "         [[-0.1131,  0.1815,  0.5153,  ..., -0.0164, -0.2026, -0.4950],\n",
      "          [ 0.8545, -0.7449, -0.9325,  ...,  0.6133,  1.6644,  0.8931]],\n",
      "\n",
      "         [[-0.1127,  0.2553,  0.4889,  ..., -0.0324, -0.1396, -0.4642],\n",
      "          [ 0.9055, -0.7429, -0.9926,  ...,  0.6479,  1.7060,  0.8781]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0632,  0.2089,  0.4554,  ...,  0.0394, -0.1776, -0.4562],\n",
      "          [ 0.9016, -0.7733, -0.9184,  ...,  0.6361,  1.7070,  0.8978]],\n",
      "\n",
      "         [[-0.0068,  0.1558,  0.5107,  ..., -0.0260, -0.1577, -0.5035],\n",
      "          [ 0.9676, -0.7355, -0.9783,  ...,  0.6122,  1.7108,  0.9099]],\n",
      "\n",
      "         [[-0.0694,  0.1837,  0.5020,  ...,  0.0347, -0.1825, -0.4828],\n",
      "          [ 0.8519, -0.7426, -0.8937,  ...,  0.6593,  1.7450,  0.8492]]],\n",
      "\n",
      "\n",
      "        [[[-0.1789,  0.8188, -0.3516,  ...,  0.2704,  0.6805,  0.5076],\n",
      "          [-0.6077, -0.6956, -0.2468,  ...,  0.4680,  1.1940, -0.3626]],\n",
      "\n",
      "         [[-0.2730,  0.6272,  0.0153,  ...,  0.2915,  0.4262,  0.3405],\n",
      "          [-0.0921, -0.6624, -0.4816,  ...,  0.8057,  1.6584,  0.3867]],\n",
      "\n",
      "         [[-0.1819,  0.4895,  0.1813,  ...,  0.4085,  0.1136,  0.1850],\n",
      "          [ 0.0743, -0.6579, -0.5267,  ...,  0.7932,  1.7956,  0.4678]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.1777,  0.4989,  0.1872,  ...,  0.4557,  0.0876,  0.1601],\n",
      "          [-0.0617, -0.6459, -0.4916,  ...,  0.8121,  1.6992,  0.4797]],\n",
      "\n",
      "         [[-0.0207,  0.2811,  0.1789,  ...,  0.4471, -0.0180,  0.0547],\n",
      "          [ 0.2770, -0.6372, -0.5914,  ...,  0.8014,  1.7871,  0.5592]],\n",
      "\n",
      "         [[-0.0998,  0.3918,  0.2235,  ...,  0.4772, -0.0237,  0.0673],\n",
      "          [-0.1670, -0.6105, -0.3007,  ...,  0.7562,  1.7350,  0.3021]]]],\n",
      "       grad_fn=<TransposeBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO    \u001b[0m \u001b[34mPruning module: bert_encoder_layer_0_attention_self_query\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPruning module: bert_encoder_layer_0_attention_self_key\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPruning module: bert_encoder_layer_0_attention_self_value\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPruning module: bert_encoder_layer_0_intermediate_dense\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPruning module: bert_encoder_layer_0_output_dense\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPruning module: bert_encoder_layer_1_attention_self_key\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPruning module: bert_encoder_layer_1_attention_output_dense\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPruning module: bert_encoder_layer_1_intermediate_dense\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPruning module: bert_encoder_layer_1_output_dense\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPruning module: bert_pooler_dense\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPruning module: classifier\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from chop.pipelines import CompressionPipeline\n",
    "from chop import MaseGraph\n",
    "\n",
    "mg = MaseGraph(model)\n",
    "pipe = CompressionPipeline()\n",
    "\n",
    "quantization_config = {\n",
    "    \"by\": \"type\",\n",
    "    \"default\": {\n",
    "        \"config\": {\n",
    "            \"name\": None,\n",
    "        }\n",
    "    },\n",
    "    \"linear\": {\n",
    "        \"config\": {\n",
    "            \"name\": \"integer\",\n",
    "            # data\n",
    "            \"data_in_width\": 8,\n",
    "            \"data_in_frac_width\": 4,\n",
    "            # weight\n",
    "            \"weight_width\": 8,\n",
    "            \"weight_frac_width\": 4,\n",
    "            # bias\n",
    "            \"bias_width\": 8,\n",
    "            \"bias_frac_width\": 4,\n",
    "        }\n",
    "    },\n",
    "}\n",
    "\n",
    "pruning_config = {\n",
    "    \"weight\": {\n",
    "        \"sparsity\": 0.5,\n",
    "        \"method\": \"l1-norm\",\n",
    "        \"scope\": \"local\",\n",
    "    },\n",
    "    \"activation\": {\n",
    "        \"sparsity\": 0.5,\n",
    "        \"method\": \"l1-norm\",\n",
    "        \"scope\": \"local\",\n",
    "    },\n",
    "}\n",
    "\n",
    "mg, _ = pipe(\n",
    "    mg,\n",
    "    pass_args={\n",
    "        \"quantize_transform_pass\": quantization_config,\n",
    "        \"prune_transform_pass\": pruning_config,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, export the MaseGraph for the compressed checkpoint to be used in future tutorials for hardware generation and distributed deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO    \u001b[0m \u001b[34mExporting MaseGraph to /home/neil/tutorial_5_nas_compressed.pt, /home/neil/tutorial_5_nas_compressed.mz\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mExporting GraphModule to /home/neil/tutorial_5_nas_compressed.pt\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mSaving state_dict format\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mExporting MaseMetadata to /home/neil/tutorial_5_nas_compressed.mz\u001b[0m\n",
      "\u001b[33mWARNING \u001b[0m \u001b[34mFailed to pickle call_function node: finfo\u001b[0m\n",
      "\u001b[33mWARNING \u001b[0m \u001b[34mcannot pickle 'torch.finfo' object\u001b[0m\n",
      "\u001b[33mWARNING \u001b[0m \u001b[34mFailed to pickle call_function node: getattr_3\u001b[0m\n",
      "\u001b[33mWARNING \u001b[0m \u001b[34mcannot pickle 'torch.finfo' object\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "mg.export(f\"{Path.home()}/tutorial_5_nas_compressed\", save_format=\"state_dict\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "What changed:\n",
    "\n",
    "Strictly Static Search Space for GridSampler: Instead of per-layer decisions (which explode combinatorially), I added a global parameter layer_type_mode that switches all applicable linear layers to either \"All_Linear\" or \"All_Identity\".\n",
    "Values, Not Indices: As suggested, the search space now uses the actual values (integers for sizes, strings for modes) and construct_model uses trial.suggest_categorical to pick them directly. This is cleaner and avoids mapping indices.\n",
    "One Search Space for All: Now GridSampler, RandomSampler, and TPESampler all operate on the exact same search space definition, making the comparison scientifically fair.\n",
    "This reduces the total combinations to 600 (\n",
    "3\n",
    "\n",
    "4\n",
    "\n",
    "5\n",
    "\n",
    "5\n",
    "\n",
    "2\n",
    "34552), of which the code will sample 30. This ensures GridSampler works correctly without crashing on memory limits or dynamic parameter errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-01-27 11:39:28,729] A new study created in memory with name: no-name-5d2b3027-e297-4a28-823f-7192f2b1ed99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Grid Search ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2026-01-27 11:39:31,040] Trial 0 failed with parameters: {'num_layers': 8, 'num_heads': 2, 'hidden_size': 256, 'intermediate_size': 1536, 'layer_0_type': 'Identity', 'layer_1_type': 'Linear', 'layer_2_type': 'Identity', 'layer_3_type': 'Linear', 'layer_4_type': 'Identity', 'layer_5_type': 'Linear', 'layer_6_type': 'Linear', 'layer_7_type': 'Linear', 'pooler_type': 'Identity'} because of the following error: RuntimeError('CUDA error: out of memory\\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\\n').\n",
      "Traceback (most recent call last):\n",
      "  File \"/vol/bitbucket/nr722/adls_group_14/.venv/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 205, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_2647313/3502103930.py\", line 84, in objective\n",
      "    model.to(device)\n",
      "  File \"/vol/bitbucket/nr722/adls_group_14/.venv/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 3698, in to\n",
      "    return super().to(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/bitbucket/nr722/adls_group_14/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1343, in to\n",
      "    return self._apply(convert)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/bitbucket/nr722/adls_group_14/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 903, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/vol/bitbucket/nr722/adls_group_14/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 903, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/vol/bitbucket/nr722/adls_group_14/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 903, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/vol/bitbucket/nr722/adls_group_14/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 930, in _apply\n",
      "    param_applied = fn(param)\n",
      "                    ^^^^^^^^^\n",
      "  File \"/vol/bitbucket/nr722/adls_group_14/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1329, in convert\n",
      "    return t.to(\n",
      "           ^^^^^\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "[W 2026-01-27 11:39:31,058] Trial 0 failed with value None.\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_2647313/3502103930.py\", line 119, in <module>\n",
      "    study.optimize(objective, n_trials=n_trials)\n",
      "  File \"/vol/bitbucket/nr722/adls_group_14/.venv/lib/python3.11/site-packages/optuna/study/study.py\", line 490, in optimize\n",
      "    _optimize(\n",
      "  File \"/vol/bitbucket/nr722/adls_group_14/.venv/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 67, in _optimize\n",
      "    _optimize_sequential(\n",
      "  File \"/vol/bitbucket/nr722/adls_group_14/.venv/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 164, in _optimize_sequential\n",
      "    frozen_trial_id = _run_trial(study, func, catch)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/bitbucket/nr722/adls_group_14/.venv/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 262, in _run_trial\n",
      "    raise func_err\n",
      "  File \"/vol/bitbucket/nr722/adls_group_14/.venv/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 205, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_2647313/3502103930.py\", line 84, in objective\n",
      "    model.to(device)\n",
      "  File \"/vol/bitbucket/nr722/adls_group_14/.venv/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 3698, in to\n",
      "    return super().to(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/bitbucket/nr722/adls_group_14/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1343, in to\n",
      "    return self._apply(convert)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/bitbucket/nr722/adls_group_14/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 903, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/vol/bitbucket/nr722/adls_group_14/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 903, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/vol/bitbucket/nr722/adls_group_14/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 903, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/vol/bitbucket/nr722/adls_group_14/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 930, in _apply\n",
      "    param_applied = fn(param)\n",
      "                    ^^^^^^^^^\n",
      "  File \"/vol/bitbucket/nr722/adls_group_14/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1329, in convert\n",
      "    return t.to(\n",
      "           ^^^^^\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "[I 2026-01-27 11:39:31,062] A new study created in memory with name: no-name-c5b76628-52f6-4e5c-9f36-c642ef34c1e0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampler Grid Search encountered an error: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "--- Starting Random Search ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2026-01-27 11:39:31,381] Trial 0 failed with parameters: {'num_layers': 4, 'num_heads': 2, 'hidden_size': 384, 'intermediate_size': 1536, 'layer_0_type': 'Linear', 'layer_1_type': 'Linear', 'layer_2_type': 'Identity', 'layer_3_type': 'Linear', 'pooler_type': 'Identity'} because of the following error: RuntimeError('CUDA error: out of memory\\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\\n').\n",
      "Traceback (most recent call last):\n",
      "  File \"/vol/bitbucket/nr722/adls_group_14/.venv/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 205, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_2647313/3502103930.py\", line 84, in objective\n",
      "    model.to(device)\n",
      "  File \"/vol/bitbucket/nr722/adls_group_14/.venv/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 3698, in to\n",
      "    return super().to(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/bitbucket/nr722/adls_group_14/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1343, in to\n",
      "    return self._apply(convert)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/bitbucket/nr722/adls_group_14/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 903, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/vol/bitbucket/nr722/adls_group_14/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 903, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/vol/bitbucket/nr722/adls_group_14/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 903, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/vol/bitbucket/nr722/adls_group_14/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 930, in _apply\n",
      "    param_applied = fn(param)\n",
      "                    ^^^^^^^^^\n",
      "  File \"/vol/bitbucket/nr722/adls_group_14/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1329, in convert\n",
      "    return t.to(\n",
      "           ^^^^^\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "[W 2026-01-27 11:39:31,382] Trial 0 failed with value None.\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_2647313/3502103930.py\", line 119, in <module>\n",
      "    study.optimize(objective, n_trials=n_trials)\n",
      "  File \"/vol/bitbucket/nr722/adls_group_14/.venv/lib/python3.11/site-packages/optuna/study/study.py\", line 490, in optimize\n",
      "    _optimize(\n",
      "  File \"/vol/bitbucket/nr722/adls_group_14/.venv/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 67, in _optimize\n",
      "    _optimize_sequential(\n",
      "  File \"/vol/bitbucket/nr722/adls_group_14/.venv/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 164, in _optimize_sequential\n",
      "    frozen_trial_id = _run_trial(study, func, catch)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/bitbucket/nr722/adls_group_14/.venv/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 262, in _run_trial\n",
      "    raise func_err\n",
      "  File \"/vol/bitbucket/nr722/adls_group_14/.venv/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 205, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_2647313/3502103930.py\", line 84, in objective\n",
      "    model.to(device)\n",
      "  File \"/vol/bitbucket/nr722/adls_group_14/.venv/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 3698, in to\n",
      "    return super().to(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/bitbucket/nr722/adls_group_14/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1343, in to\n",
      "    return self._apply(convert)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/bitbucket/nr722/adls_group_14/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 903, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/vol/bitbucket/nr722/adls_group_14/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 903, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/vol/bitbucket/nr722/adls_group_14/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 903, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/vol/bitbucket/nr722/adls_group_14/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 930, in _apply\n",
      "    param_applied = fn(param)\n",
      "                    ^^^^^^^^^\n",
      "  File \"/vol/bitbucket/nr722/adls_group_14/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1329, in convert\n",
      "    return t.to(\n",
      "           ^^^^^\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "[I 2026-01-27 11:39:31,383] A new study created in memory with name: no-name-5343d915-6d2c-44b7-a20b-98808bc5ee78\n",
      "[W 2026-01-27 11:39:31,580] Trial 0 failed with parameters: {'num_layers': 2, 'num_heads': 4, 'hidden_size': 256, 'intermediate_size': 512, 'layer_0_type': 'Identity', 'layer_1_type': 'Identity', 'pooler_type': 'Identity'} because of the following error: RuntimeError('CUDA error: out of memory\\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\\n').\n",
      "Traceback (most recent call last):\n",
      "  File \"/vol/bitbucket/nr722/adls_group_14/.venv/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 205, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_2647313/3502103930.py\", line 84, in objective\n",
      "    model.to(device)\n",
      "  File \"/vol/bitbucket/nr722/adls_group_14/.venv/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 3698, in to\n",
      "    return super().to(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/bitbucket/nr722/adls_group_14/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1343, in to\n",
      "    return self._apply(convert)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/bitbucket/nr722/adls_group_14/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 903, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/vol/bitbucket/nr722/adls_group_14/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 903, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/vol/bitbucket/nr722/adls_group_14/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 903, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/vol/bitbucket/nr722/adls_group_14/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 930, in _apply\n",
      "    param_applied = fn(param)\n",
      "                    ^^^^^^^^^\n",
      "  File \"/vol/bitbucket/nr722/adls_group_14/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1329, in convert\n",
      "    return t.to(\n",
      "           ^^^^^\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "[W 2026-01-27 11:39:31,582] Trial 0 failed with value None.\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_2647313/3502103930.py\", line 119, in <module>\n",
      "    study.optimize(objective, n_trials=n_trials)\n",
      "  File \"/vol/bitbucket/nr722/adls_group_14/.venv/lib/python3.11/site-packages/optuna/study/study.py\", line 490, in optimize\n",
      "    _optimize(\n",
      "  File \"/vol/bitbucket/nr722/adls_group_14/.venv/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 67, in _optimize\n",
      "    _optimize_sequential(\n",
      "  File \"/vol/bitbucket/nr722/adls_group_14/.venv/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 164, in _optimize_sequential\n",
      "    frozen_trial_id = _run_trial(study, func, catch)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/bitbucket/nr722/adls_group_14/.venv/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 262, in _run_trial\n",
      "    raise func_err\n",
      "  File \"/vol/bitbucket/nr722/adls_group_14/.venv/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 205, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_2647313/3502103930.py\", line 84, in objective\n",
      "    model.to(device)\n",
      "  File \"/vol/bitbucket/nr722/adls_group_14/.venv/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 3698, in to\n",
      "    return super().to(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/bitbucket/nr722/adls_group_14/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1343, in to\n",
      "    return self._apply(convert)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/vol/bitbucket/nr722/adls_group_14/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 903, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/vol/bitbucket/nr722/adls_group_14/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 903, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/vol/bitbucket/nr722/adls_group_14/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 903, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/vol/bitbucket/nr722/adls_group_14/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 930, in _apply\n",
      "    param_applied = fn(param)\n",
      "                    ^^^^^^^^^\n",
      "  File \"/vol/bitbucket/nr722/adls_group_14/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1329, in convert\n",
      "    return t.to(\n",
      "           ^^^^^\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampler Random Search encountered an error: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "--- Starting TPE (Bayesian) ---\n",
      "Sampler TPE (Bayesian) encountered an error: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2647313/3502103930.py:142: UserWarning: No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n",
      "  plt.legend()\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import optuna\n",
    "from optuna.samplers import RandomSampler, TPESampler, GridSampler\n",
    "\n",
    "import torch.nn as nn\n",
    "from chop.nn.modules import Identity\n",
    "from transformers import AutoConfig, AutoModelForSequenceClassification\n",
    "from chop.tools.utils import deepsetattr\n",
    "from chop.tools import get_trainer\n",
    "import torch\n",
    "import traceback\n",
    "import re\n",
    "\n",
    "# 1. Redefine Search Space\n",
    "# To allow \"Per Layer\" search with GridSampler, we must define every possible parameter key upfront.\n",
    "# We define granularities at the \"Layer Block\" level (Layer 0, Layer 1...) rather than \"Per Module\" \n",
    "# to keep the grid size manageable (approx 30k combinations vs trillions).\n",
    "search_space = {\n",
    "    \"num_layers\": [2, 4, 8],\n",
    "    \"num_heads\": [2, 4, 8, 16],\n",
    "    \"hidden_size\": [128, 192, 256, 384, 512],\n",
    "    \"intermediate_size\": [512, 768, 1024, 1536, 2048],\n",
    "}\n",
    "# Add per-layer choices to the search space grid\n",
    "# Max layers is 8, so we prepare keys for layer 0 to 7\n",
    "for i in range(8):\n",
    "    search_space[f\"layer_{i}_type\"] = [\"Linear\", \"Identity\"]\n",
    "search_space[\"pooler_type\"] = [\"Linear\", \"Identity\"]\n",
    "\n",
    "\n",
    "# 2. construct_model\n",
    "def construct_model(trial):\n",
    "    config = AutoConfig.from_pretrained(checkpoint)\n",
    "\n",
    "    # 2a. Architecture Parameters\n",
    "    # Map from search spaces\n",
    "    num_layers = trial.suggest_categorical(\"num_layers\", search_space[\"num_layers\"])\n",
    "    num_heads = trial.suggest_categorical(\"num_heads\", search_space[\"num_heads\"])\n",
    "    hidden_size = trial.suggest_categorical(\"hidden_size\", search_space[\"hidden_size\"])\n",
    "    intermediate_size = trial.suggest_categorical(\"intermediate_size\", search_space[\"intermediate_size\"])\n",
    "\n",
    "    setattr(config, \"num_hidden_layers\", num_layers)\n",
    "    setattr(config, \"num_attention_heads\", num_heads)\n",
    "    setattr(config, \"hidden_size\", hidden_size)\n",
    "    setattr(config, \"intermediate_size\", intermediate_size)\n",
    "\n",
    "    trial_model = AutoModelForSequenceClassification.from_config(config)\n",
    "\n",
    "    # 2b. Per-Layer Type Decisions\n",
    "    # We apply the decision to all linear modules within that specific layer.\n",
    "    for name, layer in trial_model.named_modules():\n",
    "        if isinstance(layer, nn.Linear) and layer.in_features == layer.out_features:\n",
    "            \n",
    "            # Determine which \"Layer Block\" this module belongs to\n",
    "            # Pattern: bert.encoder.layer.0.attention...\n",
    "            match = re.search(r\"layer\\.(\\d+)\\.\", name)\n",
    "            \n",
    "            choice_key = None\n",
    "            if match:\n",
    "                layer_idx = int(match.group(1))\n",
    "                choice_key = f\"layer_{layer_idx}_type\"\n",
    "            elif \"pooler\" in name:\n",
    "                choice_key = \"pooler_type\"\n",
    "            \n",
    "            # If we found a valid control key for this module, ask the trial\n",
    "            if choice_key:\n",
    "                # Note: valid_values must define the choice. \n",
    "                # For GridSampler, this key simply MUST exist in search_space.\n",
    "                # If architecture has 2 layers, we won't ask for layer_7_type, which is fine.\n",
    "                layer_type = trial.suggest_categorical(choice_key, search_space[choice_key])\n",
    "                \n",
    "                if layer_type == \"Identity\":\n",
    "                    new_layer = Identity()\n",
    "                    deepsetattr(trial_model, name, new_layer)\n",
    "                # else \"Linear\" -> keep as is\n",
    "\n",
    "    return trial_model\n",
    "\n",
    "# 3. Objective Function\n",
    "def objective(trial):\n",
    "    model = construct_model(trial)\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model.to(device)\n",
    "\n",
    "    trainer = get_trainer(\n",
    "        model=model,\n",
    "        tokenized_dataset=dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        evaluate_metric=\"accuracy\",\n",
    "        num_train_epochs=1,\n",
    "    )\n",
    "    trainer.train()\n",
    "    eval_results = trainer.evaluate()\n",
    "    \n",
    "    # Storage\n",
    "    trial.set_user_attr(\"model\", model.cpu())\n",
    "    return eval_results[\"eval_accuracy\"]\n",
    "\n",
    "\n",
    "# 4. Sampler Setup\n",
    "# We use the full search_space for GridSampler.\n",
    "# Warning: This grid is large (~30,720 combinations), but feasible.\n",
    "samplers = {\n",
    "    \"Grid Search\": GridSampler(search_space),\n",
    "    \"Random Search\": RandomSampler(),\n",
    "    \"TPE (Bayesian)\": TPESampler(),\n",
    "}\n",
    "\n",
    "n_trials = 30\n",
    "sampler_history = {}\n",
    "best_overall_value = -float(\"inf\")\n",
    "best_overall_model = None\n",
    "\n",
    "for name, sampler in samplers.items():\n",
    "    print(f\"--- Starting {name} ---\")\n",
    "    try:\n",
    "        study = optuna.create_study(direction=\"maximize\", sampler=sampler)\n",
    "        study.optimize(objective, n_trials=n_trials)\n",
    "        \n",
    "        values = [t.value for t in study.trials if t.value is not None]\n",
    "        if values:\n",
    "            sampler_history[name] = np.maximum.accumulate(values)\n",
    "            \n",
    "            # Track the overall best model\n",
    "            if study.best_value > best_overall_value:\n",
    "                best_overall_value = study.best_value\n",
    "                best_overall_model = study.best_trial.user_attrs[\"model\"]\n",
    "                \n",
    "    except Exception as e:\n",
    "        traceback.print_exc()\n",
    "        print(f\"Sampler {name} encountered an error: {e}\")\n",
    "\n",
    "# Plotting the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "for name, best_values in sampler_history.items():\n",
    "    plt.plot(range(1, len(best_values) + 1), best_values, marker=\"o\", label=name)\n",
    "\n",
    "plt.xlabel(\"Number of Trials\")\n",
    "plt.ylabel(\"Maximum Accuracy Achieved\")\n",
    "plt.title(\"NAS Sampler Comparison (GPU-Accelerated)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Best Model\n",
    "Finally, we can save the best model found during the search for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No best model found to save.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import dill\n",
    "\n",
    "if best_overall_model is not None:\n",
    "    with open(f\"{Path.home()}/tutorial_5_loop_best_model.pkl\", \"wb\") as f:\n",
    "        dill.dump(best_overall_model, f)\n",
    "    print(f\"Saved the best overall model (Accuracy: {best_overall_value:.4f}) to {Path.home()}/tutorial_5_loop_best_model.pkl\")\n",
    "else:\n",
    "    print(\"No best model found to save.\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mase",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
