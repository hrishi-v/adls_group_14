{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 5: Neural Architecture Search (NAS) with Mase and Optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we'll see how Mase can be integrated with Optuna, the popular hyperparameter optimization framework, to search for a Bert model optimized for sequence classification on the IMDb dataset. We'll take the Optuna-generated model and import it into Mase, then run the CompressionPipeline to prepare the model for edge deployment by quantizing and pruning its weights.\n",
    "\n",
    "As we'll see, running Architecture Search with Mase/Optuna involves the following steps.\n",
    "\n",
    "1. **Define the search space**: this is a dictionary containing the range of values for each parameter at each layer in the model.\n",
    "\n",
    "2. **Write the model constructor**: this is a function which uses Optuna utilities to sample a model from the search space, and constructs the model using transformers from_config class method.\n",
    "\n",
    "3. **Write the objective function**: this function calls on the model constructor defined in Step 2 and defines the training/evaluation setup for each search iteration.\n",
    "\n",
    "4. **Go!** Choose an Optuna sampler, create a study and launch the search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"prajjwal1/bert-tiny\"\n",
    "tokenizer_checkpoint = \"bert-base-uncased\"\n",
    "dataset_name = \"imdb\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, fetch the dataset using the `get_tokenized_dataset` utility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/neil/adls/adls_group_14/.venv/lib/python3.11/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n",
      "/home/neil/adls/adls_group_14/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mTokenizing dataset imdb with AutoTokenizer for bert-base-uncased.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from chop.tools import get_tokenized_dataset\n",
    "\n",
    "dataset, tokenizer = get_tokenized_dataset(\n",
    "    dataset=dataset_name,\n",
    "    checkpoint=tokenizer_checkpoint,\n",
    "    return_tokenizer=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Defining the Search Space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by defining a search space, i.e. enumerating the possible combinations of hyperparameters that Optuna can choose during search. We'll explore the following range of values for the model's hidden size, intermediate size, number of layers and number of heads, inspired by the [NAS-BERT paper](https://arxiv.org/abs/2105.14444)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from chop.nn.modules import Identity\n",
    "\n",
    "search_space = {\n",
    "    \"num_layers\": [2, 4, 8],\n",
    "    \"num_heads\": [2, 4, 8, 16],\n",
    "    # hidden size is the embedding dimension in transformers\n",
    "    \"hidden_size\": [128, 192, 256, 384, 512],\n",
    "    # intermediate size is the dimension of the feedforward layer in transformers\n",
    "    \"intermediate_size\": [512, 768, 1024, 1536, 2048],\n",
    "    \"linear_layer_choices\": [\n",
    "        nn.Linear,\n",
    "        Identity,\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Writing a Model Constructor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the following function, which will get called in each iteration of the search process. The function is passed the `trial` argument, which is an Optuna object that comes with many functionalities - see the [Trial documentation](https://optuna.readthedocs.io/en/stable/reference/trial.html) for more details. Here, we use the `trial.suggest_int` and `trial.suggest_categorical` functions to trigger the chosen sampler to choose parameter choices and layer types. The suggested integer is the index into the search space for each parameter, which we defined in the previous cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, AutoModelForSequenceClassification\n",
    "from chop.tools.utils import deepsetattr\n",
    "\n",
    "\n",
    "def construct_model(trial):\n",
    "    config = AutoConfig.from_pretrained(checkpoint)\n",
    "\n",
    "    # Update the paramaters in the config\n",
    "    for param in [\n",
    "        \"num_layers\",\n",
    "        \"num_heads\",\n",
    "        \"hidden_size\",\n",
    "        \"intermediate_size\",\n",
    "    ]:\n",
    "        chosen_idx = trial.suggest_int(param, 0, len(search_space[param]) - 1)\n",
    "        setattr(config, param, search_space[param][chosen_idx])\n",
    "\n",
    "    trial_model = AutoModelForSequenceClassification.from_config(config)\n",
    "\n",
    "    for name, layer in trial_model.named_modules():\n",
    "        if isinstance(layer, nn.Linear) and layer.in_features == layer.out_features:\n",
    "            new_layer_cls = trial.suggest_categorical(\n",
    "                f\"{name}_type\",\n",
    "                search_space[\"linear_layer_choices\"],\n",
    "            )\n",
    "\n",
    "            if new_layer_cls == nn.Linear:\n",
    "                continue\n",
    "            elif new_layer_cls == Identity:\n",
    "                new_layer = Identity()\n",
    "                deepsetattr(trial_model, name, new_layer)\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown layer type: {new_layer_cls}\")\n",
    "\n",
    "    return trial_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Defining the Objective Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define the objective function for the search, which gets called on each trial. In each trial, we create a new model instace with chosen hyperparameters according to the defined sampler. We then use the `get_trainer` utility in Mase to run a training loop on the IMDb dataset for a number of epochs. Finally, we use `evaluate` to report back the classification accuracy on the test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chop.tools import get_trainer\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "\n",
    "    # Define the model\n",
    "    model = construct_model(trial)\n",
    "\n",
    "    trainer = get_trainer(\n",
    "        model=model,\n",
    "        tokenized_dataset=dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        evaluate_metric=\"accuracy\",\n",
    "        num_train_epochs=1,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    eval_results = trainer.evaluate()\n",
    "\n",
    "    # Set the model as an attribute so we can fetch it later\n",
    "    trial.set_user_attr(\"model\", model)\n",
    "\n",
    "    return eval_results[\"eval_accuracy\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Launching the Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optuna provides a number of samplers, for example:\n",
    "\n",
    "* **GridSampler**: iterates through every possible combination of hyperparameters in the search space\n",
    "* **RandomSampler**: chooses a random combination of hyperparameters in each iteration\n",
    "* **TPESampler**: uses Tree-structured Parzen Estimator algorithm to choose hyperparameter values.\n",
    "\n",
    "You can define the chosen sampler by simply importing from `optuna.samplers` as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optuna.samplers import GridSampler, RandomSampler, TPESampler\n",
    "\n",
    "sampler = RandomSampler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all the pieces in place, we can launch the search as follows. The number of trials is set to 1 so you can go get a coffee for 10 minutes, then proceed with the tutorial. However, this will essentially be a random model - for better results, set this to 100 and leave it running overnight!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-01-23 11:42:20,204] A new study created in memory with name: bert-tiny-nas-study\n",
      "/home/neil/adls/adls_group_14/.venv/lib/python3.11/site-packages/optuna/distributions.py:502: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'torch.nn.modules.linear.Linear'> which is of type type.\n",
      "  warnings.warn(message)\n",
      "/home/neil/adls/adls_group_14/.venv/lib/python3.11/site-packages/optuna/distributions.py:502: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'chop.nn.modules.identity.Identity'> which is of type type.\n",
      "  warnings.warn(message)\n",
      "/home/neil/adls/adls_group_14/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 16:46, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.694000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.633000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.461100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.406400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.377400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.386500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 05:18]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-01-23 12:04:29,355] Trial 0 finished with value: 0.84664 and parameters: {'num_layers': 0, 'num_heads': 2, 'hidden_size': 0, 'intermediate_size': 4, 'bert.encoder.layer.0.attention.self.query_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.attention.self.key_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.attention.self.value_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.attention.output.dense_type': <class 'chop.nn.modules.identity.Identity'>, 'bert.encoder.layer.1.attention.self.query_type': <class 'chop.nn.modules.identity.Identity'>, 'bert.encoder.layer.1.attention.self.key_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.attention.self.value_type': <class 'chop.nn.modules.identity.Identity'>, 'bert.encoder.layer.1.attention.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.pooler.dense_type': <class 'torch.nn.modules.linear.Linear'>}. Best is trial 0 with value: 0.84664.\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "\n",
    "study = optuna.create_study(\n",
    "    direction=\"maximize\",\n",
    "    study_name=\"bert-tiny-nas-study\",\n",
    "    sampler=sampler,\n",
    ")\n",
    "\n",
    "study.optimize(\n",
    "    objective,\n",
    "    n_trials=1,\n",
    "    timeout=60 * 60 * 24,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetch the model associated with the best trial as follows, and export to be used in future tutorials. In Tutorial 6, we'll see how to run mixed-precision quantization search on top of the model we've just found through NAS to further find the optimal quantization mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import dill\n",
    "\n",
    "model = study.best_trial.user_attrs[\"model\"].cpu()\n",
    "\n",
    "with open(f\"{Path.home()}/tutorial_5_best_model.pkl\", \"wb\") as f:\n",
    "    dill.dump(model, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying the Optimized Model with CompressionPipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can run the CompressionPipeline in Mase to run uniform quantization and pruning over the searched model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`past_key_values` were not specified as input names, but model.config.use_cache = True. Setting model.config.use_cache = False.\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mGetting dummy input for prajjwal1/bert-tiny.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 101, 9932, 2089, 2202, 2058, 1996, 2088, 2028, 2154,  102],\n",
      "        [ 101, 2023, 2003, 2339, 2017, 2323, 4553, 4748, 4877,  102]])\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "tensor([[ 101, 9932, 2089, 2202, 2058, 1996, 2088, 2028, 2154,  102],\n",
      "        [ 101, 2023, 2003, 2339, 2017, 2323, 4553, 4748, 4877,  102]])\n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])\n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])\n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])\n",
      "tensor([[[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]]])\n",
      "tensor([[[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]]])\n",
      "tensor([[[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]]])\n",
      "tensor([[[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]]])\n",
      "tensor([[[-2.7848,  0.6903,  0.0748,  ...,  1.2979,  2.2950, -1.0034],\n",
      "         [-0.1152,  0.8773,  0.4429,  ...,  0.1657,  1.2624,  0.6311],\n",
      "         [ 1.2829,  0.7150,  0.5566,  ..., -0.1172,  1.0775, -0.4503],\n",
      "         ...,\n",
      "         [ 0.4918, -0.7880,  0.7300,  ...,  0.9211,  2.1163,  0.9393],\n",
      "         [ 0.8289,  0.7764,  0.0999,  ...,  0.8406,  1.9082,  0.4747],\n",
      "         [ 0.1125, -0.1866, -0.3279,  ...,  0.1215,  1.8005,  0.7874]],\n",
      "\n",
      "        [[-2.7848,  0.6903,  0.0748,  ...,  1.2979,  2.2950, -1.0034],\n",
      "         [-0.2906,  0.8570,  1.1283,  ..., -0.7093,  2.1347,  0.8213],\n",
      "         [ 2.0172,  0.0286, -0.0612,  ...,  1.1738,  1.3392,  0.2884],\n",
      "         ...,\n",
      "         [ 0.8422, -1.2634,  0.2478,  ...,  0.7810,  3.6310,  0.8491],\n",
      "         [-0.3479,  1.8140,  0.8316,  ...,  0.9450,  0.7788,  1.0316],\n",
      "         [ 0.1125, -0.1866, -0.3279,  ...,  0.1215,  1.8005,  0.7874]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[[-0.7412, -0.3804, -0.8460,  ..., -0.4371, -1.2164, -1.3986],\n",
      "         [-0.4100,  0.1004, -0.4044,  ..., -0.1055, -0.5008, -0.6416],\n",
      "         [-0.2607, -0.1195, -0.6848,  ..., -0.2281, -0.6319, -0.5957],\n",
      "         ...,\n",
      "         [-0.1480,  0.0111, -0.3225,  ..., -0.0623, -0.6074, -0.1067],\n",
      "         [-0.3695, -0.3713, -0.2849,  ..., -0.3794, -0.5888, -0.7569],\n",
      "         [-0.1104,  0.0506, -0.4374,  ..., -0.1904, -0.6011, -0.5660]],\n",
      "\n",
      "        [[-0.7412, -0.3804, -0.8460,  ..., -0.4371, -1.2164, -1.3986],\n",
      "         [-0.4088, -0.4402, -0.3554,  ..., -0.1222, -1.1231, -0.8788],\n",
      "         [-0.0330,  0.0906, -0.5880,  ..., -0.3124, -0.4530, -0.3067],\n",
      "         ...,\n",
      "         [-0.0388, -0.1259, -0.1314,  ...,  0.0194, -0.6458,  0.1440],\n",
      "         [-0.3649, -0.1592, -0.5686,  ..., -0.2025, -0.2002, -0.8242],\n",
      "         [-0.1104,  0.0506, -0.4374,  ..., -0.1904, -0.6011, -0.5660]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[-0.7412, -0.3804, -0.8460,  ..., -0.4371, -1.2164, -1.3986],\n",
      "         [-0.4100,  0.1004, -0.4044,  ..., -0.1055, -0.5008, -0.6416],\n",
      "         [-0.2607, -0.1195, -0.6848,  ..., -0.2281, -0.6319, -0.5957],\n",
      "         ...,\n",
      "         [-0.1480,  0.0111, -0.3225,  ..., -0.0623, -0.6074, -0.1067],\n",
      "         [-0.3695, -0.3713, -0.2849,  ..., -0.3794, -0.5888, -0.7569],\n",
      "         [-0.1104,  0.0506, -0.4374,  ..., -0.1904, -0.6011, -0.5660]],\n",
      "\n",
      "        [[-0.7412, -0.3804, -0.8460,  ..., -0.4371, -1.2164, -1.3986],\n",
      "         [-0.4088, -0.4402, -0.3554,  ..., -0.1222, -1.1231, -0.8788],\n",
      "         [-0.0330,  0.0906, -0.5880,  ..., -0.3124, -0.4530, -0.3067],\n",
      "         ...,\n",
      "         [-0.0388, -0.1259, -0.1314,  ...,  0.0194, -0.6458,  0.1440],\n",
      "         [-0.3649, -0.1592, -0.5686,  ..., -0.2025, -0.2002, -0.8242],\n",
      "         [-0.1104,  0.0506, -0.4374,  ..., -0.1904, -0.6011, -0.5660]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[[-0.7412, -0.3804, -0.8460,  ..., -1.4684, -1.0241, -0.2865],\n",
      "          [ 0.6948,  1.3182, -1.3075,  ..., -0.4371, -1.2164, -1.3986]],\n",
      "\n",
      "         [[-0.4100,  0.1004, -0.4044,  ..., -0.3182, -0.3609,  0.1743],\n",
      "          [ 0.5498,  0.3717, -0.4043,  ..., -0.1055, -0.5008, -0.6416]],\n",
      "\n",
      "         [[-0.2607, -0.1195, -0.6848,  ..., -0.7670, -0.4483, -0.0141],\n",
      "          [ 0.1719,  0.4026, -0.6285,  ..., -0.2281, -0.6319, -0.5957]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.1480,  0.0111, -0.3225,  ..., -0.3883, -0.1816,  0.0427],\n",
      "          [-0.1812,  0.4724, -0.6719,  ..., -0.0623, -0.6074, -0.1067]],\n",
      "\n",
      "         [[-0.3695, -0.3713, -0.2849,  ..., -0.9171, -0.7159,  0.2554],\n",
      "          [ 0.6924,  0.4391, -0.7644,  ..., -0.3794, -0.5888, -0.7569]],\n",
      "\n",
      "         [[-0.1104,  0.0506, -0.4374,  ..., -0.1900, -0.1476,  0.2160],\n",
      "          [ 0.3638,  0.5001, -0.2436,  ..., -0.1904, -0.6011, -0.5660]]],\n",
      "\n",
      "\n",
      "        [[[-0.7412, -0.3804, -0.8460,  ..., -1.4684, -1.0241, -0.2865],\n",
      "          [ 0.6948,  1.3182, -1.3075,  ..., -0.4371, -1.2164, -1.3986]],\n",
      "\n",
      "         [[-0.4088, -0.4402, -0.3554,  ..., -0.8156, -0.6480,  0.1249],\n",
      "          [ 0.3657,  0.5022, -0.4090,  ..., -0.1222, -1.1231, -0.8788]],\n",
      "\n",
      "         [[-0.0330,  0.0906, -0.5880,  ..., -0.6303, -0.4818, -0.1354],\n",
      "          [ 0.1159,  0.2703, -0.6950,  ..., -0.3124, -0.4530, -0.3067]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0388, -0.1259, -0.1314,  ..., -0.4360, -0.0777,  0.1085],\n",
      "          [ 0.1535,  0.3869, -0.4417,  ...,  0.0194, -0.6458,  0.1440]],\n",
      "\n",
      "         [[-0.3649, -0.1592, -0.5686,  ..., -0.5114, -0.6436, -0.0467],\n",
      "          [ 0.5243,  0.4281, -0.3406,  ..., -0.2025, -0.2002, -0.8242]],\n",
      "\n",
      "         [[-0.1104,  0.0506, -0.4374,  ..., -0.1900, -0.1476,  0.2160],\n",
      "          [ 0.3638,  0.5001, -0.2436,  ..., -0.1904, -0.6011, -0.5660]]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[-0.0974, -0.0419,  0.2999,  ...,  0.7525,  0.2172,  0.2374],\n",
      "         [-0.2668, -0.1748,  0.4762,  ...,  0.3301, -0.0036, -0.0977],\n",
      "         [-0.0156, -0.0499,  0.1641,  ...,  0.3254,  0.0016,  0.1563],\n",
      "         ...,\n",
      "         [-0.1696,  0.0340,  0.6925,  ...,  0.0798, -0.0088,  0.3424],\n",
      "         [-0.0981,  0.0194,  0.0728,  ...,  0.5733,  0.0337,  0.0745],\n",
      "         [ 0.0333,  0.2421,  1.0494,  ...,  0.3243,  0.0985,  0.3347]],\n",
      "\n",
      "        [[-0.0974, -0.0419,  0.2999,  ...,  0.7525,  0.2172,  0.2374],\n",
      "         [-0.2580,  0.0973,  0.3380,  ...,  0.1433,  0.1222,  0.4364],\n",
      "         [ 0.1091, -0.0317,  0.2976,  ...,  0.0798, -0.0255, -0.1473],\n",
      "         ...,\n",
      "         [-0.3029, -0.0208,  0.5885,  ..., -0.1004, -0.2212,  0.3332],\n",
      "         [-0.1652, -0.0959, -0.3040,  ...,  0.2133, -0.2691, -0.0334],\n",
      "         [ 0.0333,  0.2421,  1.0494,  ...,  0.3243,  0.0985,  0.3347]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[-0.0974, -0.0419,  0.2999,  ...,  0.7525,  0.2172,  0.2374],\n",
      "         [-0.2668, -0.1748,  0.4762,  ...,  0.3301, -0.0036, -0.0977],\n",
      "         [-0.0156, -0.0499,  0.1641,  ...,  0.3254,  0.0016,  0.1563],\n",
      "         ...,\n",
      "         [-0.1696,  0.0340,  0.6925,  ...,  0.0798, -0.0088,  0.3424],\n",
      "         [-0.0981,  0.0194,  0.0728,  ...,  0.5733,  0.0337,  0.0745],\n",
      "         [ 0.0333,  0.2421,  1.0494,  ...,  0.3243,  0.0985,  0.3347]],\n",
      "\n",
      "        [[-0.0974, -0.0419,  0.2999,  ...,  0.7525,  0.2172,  0.2374],\n",
      "         [-0.2580,  0.0973,  0.3380,  ...,  0.1433,  0.1222,  0.4364],\n",
      "         [ 0.1091, -0.0317,  0.2976,  ...,  0.0798, -0.0255, -0.1473],\n",
      "         ...,\n",
      "         [-0.3029, -0.0208,  0.5885,  ..., -0.1004, -0.2212,  0.3332],\n",
      "         [-0.1652, -0.0959, -0.3040,  ...,  0.2133, -0.2691, -0.0334],\n",
      "         [ 0.0333,  0.2421,  1.0494,  ...,  0.3243,  0.0985,  0.3347]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[[-0.0974, -0.0419,  0.2999,  ...,  0.3999,  0.0782,  0.0380],\n",
      "          [-0.3328,  0.1809,  0.5030,  ...,  0.7525,  0.2172,  0.2374]],\n",
      "\n",
      "         [[-0.2668, -0.1748,  0.4762,  ..., -0.0604, -0.3558, -0.0315],\n",
      "          [-0.3605,  0.4958, -0.1143,  ...,  0.3301, -0.0036, -0.0977]],\n",
      "\n",
      "         [[-0.0156, -0.0499,  0.1641,  ...,  0.0354,  0.1306, -0.0538],\n",
      "          [-0.3370,  0.2358,  0.1032,  ...,  0.3254,  0.0016,  0.1563]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.1696,  0.0340,  0.6925,  ...,  0.0983,  0.3532,  0.2897],\n",
      "          [-0.5079, -0.0419,  0.4354,  ...,  0.0798, -0.0088,  0.3424]],\n",
      "\n",
      "         [[-0.0981,  0.0194,  0.0728,  ...,  0.1028,  0.1641,  0.3848],\n",
      "          [-0.4029,  0.1176,  0.2699,  ...,  0.5733,  0.0337,  0.0745]],\n",
      "\n",
      "         [[ 0.0333,  0.2421,  1.0494,  ...,  0.2009,  0.2320,  0.3458],\n",
      "          [-0.2176, -0.2196, -0.0680,  ...,  0.3243,  0.0985,  0.3347]]],\n",
      "\n",
      "\n",
      "        [[[-0.0974, -0.0419,  0.2999,  ...,  0.3999,  0.0782,  0.0380],\n",
      "          [-0.3328,  0.1809,  0.5030,  ...,  0.7525,  0.2172,  0.2374]],\n",
      "\n",
      "         [[-0.2580,  0.0973,  0.3380,  ...,  0.3008, -0.2675,  0.3273],\n",
      "          [-0.4787,  0.2026,  0.3107,  ...,  0.1433,  0.1222,  0.4364]],\n",
      "\n",
      "         [[ 0.1091, -0.0317,  0.2976,  ...,  0.1426,  0.1900,  0.2218],\n",
      "          [-0.1310,  0.2211,  0.0468,  ...,  0.0798, -0.0255, -0.1473]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.3029, -0.0208,  0.5885,  ...,  0.4611,  0.0018,  0.2030],\n",
      "          [-0.1511,  0.0149,  0.4649,  ..., -0.1004, -0.2212,  0.3332]],\n",
      "\n",
      "         [[-0.1652, -0.0959, -0.3040,  ...,  0.1476,  0.0758,  0.5442],\n",
      "          [ 0.0185,  0.0567,  0.2480,  ...,  0.2133, -0.2691, -0.0334]],\n",
      "\n",
      "         [[ 0.0333,  0.2421,  1.0494,  ...,  0.2009,  0.2320,  0.3458],\n",
      "          [-0.2176, -0.2196, -0.0680,  ...,  0.3243,  0.0985,  0.3347]]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[-0.0708,  0.3246, -0.1697,  ...,  0.0190, -0.0840, -0.0289],\n",
      "         [ 0.0585,  0.0735,  0.0792,  ...,  0.0719, -0.0654,  0.0896],\n",
      "         [-0.1888,  0.0963,  0.2164,  ..., -0.2454, -0.1592,  0.0505],\n",
      "         ...,\n",
      "         [ 0.2679,  0.2498,  0.0784,  ..., -0.3999,  0.2649,  0.0094],\n",
      "         [ 0.2560, -0.3655,  0.3296,  ...,  0.0855,  0.3703,  0.1669],\n",
      "         [-0.0055, -0.0194,  0.5981,  ...,  0.0550,  0.2144,  0.5355]],\n",
      "\n",
      "        [[-0.0708,  0.3246, -0.1697,  ...,  0.0190, -0.0840, -0.0289],\n",
      "         [-0.0132,  0.0629, -0.4395,  ..., -0.0203,  0.1557, -0.1749],\n",
      "         [ 0.0988,  0.1147,  0.5807,  ...,  0.2224,  0.6068,  0.4321],\n",
      "         ...,\n",
      "         [ 0.2206,  0.3146,  0.1077,  ...,  0.0363,  0.6480,  0.2988],\n",
      "         [-0.0248, -0.2386,  0.1215,  ...,  0.4336,  0.2676,  0.1217],\n",
      "         [-0.0055, -0.0194,  0.5981,  ...,  0.0550,  0.2144,  0.5355]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[-0.0708,  0.3246, -0.1697,  ...,  0.0190, -0.0840, -0.0289],\n",
      "         [ 0.0585,  0.0735,  0.0792,  ...,  0.0719, -0.0654,  0.0896],\n",
      "         [-0.1888,  0.0963,  0.2164,  ..., -0.2454, -0.1592,  0.0505],\n",
      "         ...,\n",
      "         [ 0.2679,  0.2498,  0.0784,  ..., -0.3999,  0.2649,  0.0094],\n",
      "         [ 0.2560, -0.3655,  0.3296,  ...,  0.0855,  0.3703,  0.1669],\n",
      "         [-0.0055, -0.0194,  0.5981,  ...,  0.0550,  0.2144,  0.5355]],\n",
      "\n",
      "        [[-0.0708,  0.3246, -0.1697,  ...,  0.0190, -0.0840, -0.0289],\n",
      "         [-0.0132,  0.0629, -0.4395,  ..., -0.0203,  0.1557, -0.1749],\n",
      "         [ 0.0988,  0.1147,  0.5807,  ...,  0.2224,  0.6068,  0.4321],\n",
      "         ...,\n",
      "         [ 0.2206,  0.3146,  0.1077,  ...,  0.0363,  0.6480,  0.2988],\n",
      "         [-0.0248, -0.2386,  0.1215,  ...,  0.4336,  0.2676,  0.1217],\n",
      "         [-0.0055, -0.0194,  0.5981,  ...,  0.0550,  0.2144,  0.5355]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[[-0.0708,  0.3246, -0.1697,  ...,  0.2983, -0.1068, -0.0651],\n",
      "          [ 0.0789,  0.1643, -0.4012,  ...,  0.0190, -0.0840, -0.0289]],\n",
      "\n",
      "         [[ 0.0585,  0.0735,  0.0792,  ...,  0.1622,  0.0227, -0.3316],\n",
      "          [-0.0287, -0.4447,  0.0216,  ...,  0.0719, -0.0654,  0.0896]],\n",
      "\n",
      "         [[-0.1888,  0.0963,  0.2164,  ...,  0.0783, -0.2140, -0.0477],\n",
      "          [ 0.1991, -0.1073, -0.0446,  ..., -0.2454, -0.1592,  0.0505]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.2679,  0.2498,  0.0784,  ...,  0.0271, -0.1917, -0.1820],\n",
      "          [-0.1191,  0.1309, -0.2637,  ..., -0.3999,  0.2649,  0.0094]],\n",
      "\n",
      "         [[ 0.2560, -0.3655,  0.3296,  ..., -0.2423, -0.2244,  0.1101],\n",
      "          [-0.1970, -0.5031, -0.1664,  ...,  0.0855,  0.3703,  0.1669]],\n",
      "\n",
      "         [[-0.0055, -0.0194,  0.5981,  ...,  0.0521, -0.3181, -0.5965],\n",
      "          [-0.0684, -0.2745, -0.5289,  ...,  0.0550,  0.2144,  0.5355]]],\n",
      "\n",
      "\n",
      "        [[[-0.0708,  0.3246, -0.1697,  ...,  0.2983, -0.1068, -0.0651],\n",
      "          [ 0.0789,  0.1643, -0.4012,  ...,  0.0190, -0.0840, -0.0289]],\n",
      "\n",
      "         [[-0.0132,  0.0629, -0.4395,  ...,  0.1806, -0.0053,  0.1124],\n",
      "          [-0.2640,  0.2502,  0.0863,  ..., -0.0203,  0.1557, -0.1749]],\n",
      "\n",
      "         [[ 0.0988,  0.1147,  0.5807,  ..., -0.1693, -0.7897, -0.2951],\n",
      "          [ 0.3076, -0.0234, -0.4624,  ...,  0.2224,  0.6068,  0.4321]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.2206,  0.3146,  0.1077,  ..., -0.0184, -0.1327, -0.2403],\n",
      "          [ 0.0304,  0.0320, -0.4254,  ...,  0.0363,  0.6480,  0.2988]],\n",
      "\n",
      "         [[-0.0248, -0.2386,  0.1215,  ..., -0.1517,  0.0923, -0.1184],\n",
      "          [-0.1223, -0.2442, -0.0418,  ...,  0.4336,  0.2676,  0.1217]],\n",
      "\n",
      "         [[-0.0055, -0.0194,  0.5981,  ...,  0.0521, -0.3181, -0.5965],\n",
      "          [-0.0684, -0.2745, -0.5289,  ...,  0.0550,  0.2144,  0.5355]]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[[ 0.0900, -0.1697,  0.2524,  ...,  0.0447, -0.3236, -0.2939],\n",
      "          [ 0.0832, -0.0857,  0.2322,  ...,  0.0779, -0.2523, -0.2794],\n",
      "          [ 0.0818, -0.0786,  0.2281,  ...,  0.0825, -0.2449, -0.2820],\n",
      "          ...,\n",
      "          [ 0.0820, -0.0727,  0.2322,  ...,  0.0863, -0.2417, -0.2888],\n",
      "          [ 0.0829, -0.0911,  0.2318,  ...,  0.0759, -0.2563, -0.2838],\n",
      "          [ 0.0830, -0.0658,  0.2308,  ...,  0.0866, -0.2318, -0.2843]],\n",
      "\n",
      "         [[ 0.0787, -0.3700, -0.3788,  ..., -0.0089,  0.2339,  0.3308],\n",
      "          [ 0.0244, -0.2645, -0.3625,  ..., -0.0231,  0.1515,  0.2744],\n",
      "          [ 0.0307, -0.2713, -0.3558,  ..., -0.0250,  0.1512,  0.2730],\n",
      "          ...,\n",
      "          [ 0.0199, -0.2550, -0.3518,  ..., -0.0260,  0.1405,  0.2625],\n",
      "          [ 0.0270, -0.2724, -0.3508,  ..., -0.0234,  0.1482,  0.2705],\n",
      "          [ 0.0190, -0.2515, -0.3564,  ..., -0.0262,  0.1403,  0.2632]]],\n",
      "\n",
      "\n",
      "        [[[-0.1299,  0.0332,  0.0722,  ...,  0.0925,  0.0293, -0.1332],\n",
      "          [-0.0969,  0.0343,  0.0923,  ...,  0.0870, -0.0376, -0.1660],\n",
      "          [-0.0926,  0.0430,  0.0967,  ...,  0.0928, -0.0534, -0.1725],\n",
      "          ...,\n",
      "          [-0.0781,  0.0425,  0.1115,  ...,  0.0858, -0.0821, -0.1883],\n",
      "          [-0.0918,  0.0329,  0.0995,  ...,  0.0894, -0.0565, -0.1768],\n",
      "          [-0.0828,  0.0384,  0.1068,  ...,  0.0869, -0.0728, -0.1817]],\n",
      "\n",
      "         [[-0.1501,  0.0456, -0.1453,  ..., -0.0466,  0.1130,  0.0944],\n",
      "          [-0.0986,  0.0107, -0.1984,  ...,  0.0056,  0.1741,  0.1335],\n",
      "          [-0.0787, -0.0057, -0.2243,  ...,  0.0214,  0.2036,  0.1577],\n",
      "          ...,\n",
      "          [-0.0778, -0.0040, -0.2253,  ...,  0.0217,  0.2024,  0.1554],\n",
      "          [-0.0837, -0.0022, -0.2145,  ...,  0.0248,  0.1918,  0.1469],\n",
      "          [-0.0802, -0.0020, -0.2201,  ...,  0.0206,  0.1972,  0.1509]]]],\n",
      "       grad_fn=<ScaledDotProductFlashAttentionForCpuBackward0>)\n",
      "tensor([[[[ 0.0900, -0.1697,  0.2524,  ...,  0.0447, -0.3236, -0.2939],\n",
      "          [ 0.0787, -0.3700, -0.3788,  ..., -0.0089,  0.2339,  0.3308]],\n",
      "\n",
      "         [[ 0.0832, -0.0857,  0.2322,  ...,  0.0779, -0.2523, -0.2794],\n",
      "          [ 0.0244, -0.2645, -0.3625,  ..., -0.0231,  0.1515,  0.2744]],\n",
      "\n",
      "         [[ 0.0818, -0.0786,  0.2281,  ...,  0.0825, -0.2449, -0.2820],\n",
      "          [ 0.0307, -0.2713, -0.3558,  ..., -0.0250,  0.1512,  0.2730]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0820, -0.0727,  0.2322,  ...,  0.0863, -0.2417, -0.2888],\n",
      "          [ 0.0199, -0.2550, -0.3518,  ..., -0.0260,  0.1405,  0.2625]],\n",
      "\n",
      "         [[ 0.0829, -0.0911,  0.2318,  ...,  0.0759, -0.2563, -0.2838],\n",
      "          [ 0.0270, -0.2724, -0.3508,  ..., -0.0234,  0.1482,  0.2705]],\n",
      "\n",
      "         [[ 0.0830, -0.0658,  0.2308,  ...,  0.0866, -0.2318, -0.2843],\n",
      "          [ 0.0190, -0.2515, -0.3564,  ..., -0.0262,  0.1403,  0.2632]]],\n",
      "\n",
      "\n",
      "        [[[-0.1299,  0.0332,  0.0722,  ...,  0.0925,  0.0293, -0.1332],\n",
      "          [-0.1501,  0.0456, -0.1453,  ..., -0.0466,  0.1130,  0.0944]],\n",
      "\n",
      "         [[-0.0969,  0.0343,  0.0923,  ...,  0.0870, -0.0376, -0.1660],\n",
      "          [-0.0986,  0.0107, -0.1984,  ...,  0.0056,  0.1741,  0.1335]],\n",
      "\n",
      "         [[-0.0926,  0.0430,  0.0967,  ...,  0.0928, -0.0534, -0.1725],\n",
      "          [-0.0787, -0.0057, -0.2243,  ...,  0.0214,  0.2036,  0.1577]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0781,  0.0425,  0.1115,  ...,  0.0858, -0.0821, -0.1883],\n",
      "          [-0.0778, -0.0040, -0.2253,  ...,  0.0217,  0.2024,  0.1554]],\n",
      "\n",
      "         [[-0.0918,  0.0329,  0.0995,  ...,  0.0894, -0.0565, -0.1768],\n",
      "          [-0.0837, -0.0022, -0.2145,  ...,  0.0248,  0.1918,  0.1469]],\n",
      "\n",
      "         [[-0.0828,  0.0384,  0.1068,  ...,  0.0869, -0.0728, -0.1817],\n",
      "          [-0.0802, -0.0020, -0.2201,  ...,  0.0206,  0.1972,  0.1509]]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "tensor([[[-2.3082,  0.7651,  0.5504,  ...,  1.5358,  2.5475, -0.5222],\n",
      "         [ 0.1283,  0.9206,  0.6221,  ...,  0.3005,  1.4119,  0.8977],\n",
      "         [ 1.4134,  0.6622,  0.7189,  ...,  0.0411,  1.2354,  0.1287],\n",
      "         ...,\n",
      "         [ 0.6860, -0.8400,  0.8861,  ...,  1.1734,  2.3122,  1.3912],\n",
      "         [ 0.6597,  0.3663,  0.4045,  ...,  0.9252,  1.9168,  0.9830],\n",
      "         [ 0.3159, -0.3887, -0.1658,  ...,  0.6615,  1.9060,  1.3306]],\n",
      "\n",
      "        [[-2.7485,  1.1340,  0.2809,  ...,  1.4203,  2.3373, -1.0806],\n",
      "         [-0.3841,  1.2072,  0.9493,  ..., -0.8414,  1.8459,  0.7758],\n",
      "         [ 1.9556, -0.0805,  0.0745,  ...,  1.4761,  1.6672,  0.7070],\n",
      "         ...,\n",
      "         [ 0.9752, -1.2310,  0.2695,  ...,  1.0980,  3.8330,  1.0806],\n",
      "         [-0.3269,  1.4608,  1.0954,  ...,  1.0098,  1.0093,  1.4375],\n",
      "         [ 0.1361, -0.2443, -0.3630,  ...,  0.6604,  1.9624,  1.1708]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[[-2.3082,  0.7651,  0.5504,  ...,  1.5358,  2.5475, -0.5222],\n",
      "         [ 0.1283,  0.9206,  0.6221,  ...,  0.3005,  1.4119,  0.8977],\n",
      "         [ 1.4134,  0.6622,  0.7189,  ...,  0.0411,  1.2354,  0.1287],\n",
      "         ...,\n",
      "         [ 0.6860, -0.8400,  0.8861,  ...,  1.1734,  2.3122,  1.3912],\n",
      "         [ 0.6597,  0.3663,  0.4045,  ...,  0.9252,  1.9168,  0.9830],\n",
      "         [ 0.3159, -0.3887, -0.1658,  ...,  0.6615,  1.9060,  1.3306]],\n",
      "\n",
      "        [[-2.7485,  1.1340,  0.2809,  ...,  1.4203,  2.3373, -1.0806],\n",
      "         [-0.3841,  1.2072,  0.9493,  ..., -0.8414,  1.8459,  0.7758],\n",
      "         [ 1.9556, -0.0805,  0.0745,  ...,  1.4761,  1.6672,  0.7070],\n",
      "         ...,\n",
      "         [ 0.9752, -1.2310,  0.2695,  ...,  1.0980,  3.8330,  1.0806],\n",
      "         [-0.3269,  1.4608,  1.0954,  ...,  1.0098,  1.0093,  1.4375],\n",
      "         [ 0.1361, -0.2443, -0.3630,  ...,  0.6604,  1.9624,  1.1708]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[[-2.3082,  0.7651,  0.5504,  ...,  1.5358,  2.5475, -0.5222],\n",
      "         [ 0.1283,  0.9206,  0.6221,  ...,  0.3005,  1.4119,  0.8977],\n",
      "         [ 1.4134,  0.6622,  0.7189,  ...,  0.0411,  1.2354,  0.1287],\n",
      "         ...,\n",
      "         [ 0.6860, -0.8400,  0.8861,  ...,  1.1734,  2.3122,  1.3912],\n",
      "         [ 0.6597,  0.3663,  0.4045,  ...,  0.9252,  1.9168,  0.9830],\n",
      "         [ 0.3159, -0.3887, -0.1658,  ...,  0.6615,  1.9060,  1.3306]],\n",
      "\n",
      "        [[-2.7485,  1.1340,  0.2809,  ...,  1.4203,  2.3373, -1.0806],\n",
      "         [-0.3841,  1.2072,  0.9493,  ..., -0.8414,  1.8459,  0.7758],\n",
      "         [ 1.9556, -0.0805,  0.0745,  ...,  1.4761,  1.6672,  0.7070],\n",
      "         ...,\n",
      "         [ 0.9752, -1.2310,  0.2695,  ...,  1.0980,  3.8330,  1.0806],\n",
      "         [-0.3269,  1.4608,  1.0954,  ...,  1.0098,  1.0093,  1.4375],\n",
      "         [ 0.1361, -0.2443, -0.3630,  ...,  0.6604,  1.9624,  1.1708]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[[[-2.3082,  0.7651,  0.5504,  ...,  0.7859, -0.6492, -0.1826],\n",
      "          [-0.5673, -1.0926,  0.1068,  ...,  1.5358,  2.5475, -0.5222]],\n",
      "\n",
      "         [[ 0.1283,  0.9206,  0.6221,  ..., -1.9348,  1.4713, -0.1107],\n",
      "          [ 0.1369,  0.3139,  0.5301,  ...,  0.3005,  1.4119,  0.8977]],\n",
      "\n",
      "         [[ 1.4134,  0.6622,  0.7189,  ...,  0.3717, -0.1601, -0.8259],\n",
      "          [ 0.0255, -0.5125, -0.7821,  ...,  0.0411,  1.2354,  0.1287]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.6860, -0.8400,  0.8861,  ...,  0.8988,  0.5393,  0.6203],\n",
      "          [ 1.1425, -1.2861, -1.0775,  ...,  1.1734,  2.3122,  1.3912]],\n",
      "\n",
      "         [[ 0.6597,  0.3663,  0.4045,  ..., -0.4761, -1.2813, -1.1802],\n",
      "          [ 1.2211, -1.1852, -1.6645,  ...,  0.9252,  1.9168,  0.9830]],\n",
      "\n",
      "         [[ 0.3159, -0.3887, -0.1658,  ...,  0.4399,  0.0549,  0.4000],\n",
      "          [ 2.0346, -0.0538, -0.8923,  ...,  0.6615,  1.9060,  1.3306]]],\n",
      "\n",
      "\n",
      "        [[[-2.7485,  1.1340,  0.2809,  ...,  0.9466, -0.1298,  0.2705],\n",
      "          [-1.2361, -0.5180,  0.7297,  ...,  1.4203,  2.3373, -1.0806]],\n",
      "\n",
      "         [[-0.3841,  1.2072,  0.9493,  ..., -1.0432,  2.3208,  0.4396],\n",
      "          [-1.5662, -0.3035,  0.8778,  ..., -0.8414,  1.8459,  0.7758]],\n",
      "\n",
      "         [[ 1.9556, -0.0805,  0.0745,  ...,  1.0531, -0.0900, -0.2324],\n",
      "          [ 0.9524, -0.2288, -1.7176,  ...,  1.4761,  1.6672,  0.7070]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.9752, -1.2310,  0.2695,  ...,  1.2128, -1.0370,  0.1020],\n",
      "          [ 1.1809, -1.2216, -0.5878,  ...,  1.0980,  3.8330,  1.0806]],\n",
      "\n",
      "         [[-0.3269,  1.4608,  1.0954,  ..., -0.9398, -0.2387,  0.1908],\n",
      "          [-0.1161, -0.2387, -0.1029,  ...,  1.0098,  1.0093,  1.4375]],\n",
      "\n",
      "         [[ 0.1361, -0.2443, -0.3630,  ...,  0.5157,  0.3278,  0.6913],\n",
      "          [ 1.8884,  0.3167, -0.6189,  ...,  0.6604,  1.9624,  1.1708]]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[-0.1469, -0.6403, -0.4336,  ..., -0.4732, -0.1018,  0.0267],\n",
      "         [-0.0110,  0.0113, -0.2932,  ..., -0.0642, -0.2314,  0.2588],\n",
      "         [-0.0568, -0.1494, -0.1746,  ..., -0.3133, -0.1163, -0.1374],\n",
      "         ...,\n",
      "         [ 0.5269, -0.4526,  0.1725,  ..., -0.3702, -0.4076,  0.4215],\n",
      "         [ 0.0810, -0.2180,  0.1474,  ..., -0.2669, -0.1783,  0.2341],\n",
      "         [ 0.5613, -0.5525,  0.0366,  ..., -0.3497, -0.1890,  0.3354]],\n",
      "\n",
      "        [[-0.1598, -0.7371, -0.6059,  ..., -0.4508, -0.0422,  0.0565],\n",
      "         [ 0.3575, -0.6172, -0.4105,  ..., -0.3102, -0.3265,  0.3234],\n",
      "         [ 0.0735, -0.2081,  0.1281,  ..., -0.3561,  0.1196,  0.1379],\n",
      "         ...,\n",
      "         [ 0.3868, -0.7915, -0.0434,  ..., -0.1719, -0.0688,  0.1556],\n",
      "         [ 0.0793, -0.2826,  0.0975,  ..., -0.3293, -0.1849,  0.2143],\n",
      "         [ 0.6043, -0.6314, -0.0417,  ..., -0.3507, -0.1645,  0.3800]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[-0.1469, -0.6403, -0.4336,  ..., -0.4732, -0.1018,  0.0267],\n",
      "         [-0.0110,  0.0113, -0.2932,  ..., -0.0642, -0.2314,  0.2588],\n",
      "         [-0.0568, -0.1494, -0.1746,  ..., -0.3133, -0.1163, -0.1374],\n",
      "         ...,\n",
      "         [ 0.5269, -0.4526,  0.1725,  ..., -0.3702, -0.4076,  0.4215],\n",
      "         [ 0.0810, -0.2180,  0.1474,  ..., -0.2669, -0.1783,  0.2341],\n",
      "         [ 0.5613, -0.5525,  0.0366,  ..., -0.3497, -0.1890,  0.3354]],\n",
      "\n",
      "        [[-0.1598, -0.7371, -0.6059,  ..., -0.4508, -0.0422,  0.0565],\n",
      "         [ 0.3575, -0.6172, -0.4105,  ..., -0.3102, -0.3265,  0.3234],\n",
      "         [ 0.0735, -0.2081,  0.1281,  ..., -0.3561,  0.1196,  0.1379],\n",
      "         ...,\n",
      "         [ 0.3868, -0.7915, -0.0434,  ..., -0.1719, -0.0688,  0.1556],\n",
      "         [ 0.0793, -0.2826,  0.0975,  ..., -0.3293, -0.1849,  0.2143],\n",
      "         [ 0.6043, -0.6314, -0.0417,  ..., -0.3507, -0.1645,  0.3800]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[[-0.1469, -0.6403, -0.4336,  ..., -0.3482,  0.0808,  0.3196],\n",
      "          [ 0.4905, -0.2438, -0.2331,  ..., -0.4732, -0.1018,  0.0267]],\n",
      "\n",
      "         [[-0.0110,  0.0113, -0.2932,  ..., -0.1550,  0.2126,  0.2991],\n",
      "          [ 0.1962, -0.3690, -0.3832,  ..., -0.0642, -0.2314,  0.2588]],\n",
      "\n",
      "         [[-0.0568, -0.1494, -0.1746,  ..., -0.2490, -0.0245,  0.1575],\n",
      "          [-0.1326,  0.2628, -0.3845,  ..., -0.3133, -0.1163, -0.1374]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.5269, -0.4526,  0.1725,  ..., -0.4958, -0.0462,  0.2596],\n",
      "          [ 0.0955, -0.2266, -0.8404,  ..., -0.3702, -0.4076,  0.4215]],\n",
      "\n",
      "         [[ 0.0810, -0.2180,  0.1474,  ..., -0.6140, -0.2138, -0.2661],\n",
      "          [ 0.1962,  0.1977, -0.8744,  ..., -0.2669, -0.1783,  0.2341]],\n",
      "\n",
      "         [[ 0.5613, -0.5525,  0.0366,  ..., -0.5217,  0.3953,  0.1637],\n",
      "          [ 0.5370, -0.1999, -1.0452,  ..., -0.3497, -0.1890,  0.3354]]],\n",
      "\n",
      "\n",
      "        [[[-0.1598, -0.7371, -0.6059,  ..., -0.2130,  0.0618,  0.4512],\n",
      "          [ 0.5208, -0.0442, -0.0815,  ..., -0.4508, -0.0422,  0.0565]],\n",
      "\n",
      "         [[ 0.3575, -0.6172, -0.4105,  ..., -0.4724,  0.4848,  0.4376],\n",
      "          [ 0.1658,  0.3256, -0.5080,  ..., -0.3102, -0.3265,  0.3234]],\n",
      "\n",
      "         [[ 0.0735, -0.2081,  0.1281,  ..., -0.2016, -0.3360,  0.0404],\n",
      "          [-0.3349, -0.2328, -0.5280,  ..., -0.3561,  0.1196,  0.1379]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.3868, -0.7915, -0.0434,  ..., -0.1816,  0.0692, -0.0526],\n",
      "          [-0.2424, -0.4088, -0.8728,  ..., -0.1719, -0.0688,  0.1556]],\n",
      "\n",
      "         [[ 0.0793, -0.2826,  0.0975,  ..., -0.6438, -0.0507,  0.0322],\n",
      "          [ 0.3269,  0.0580, -0.5306,  ..., -0.3293, -0.1849,  0.2143]],\n",
      "\n",
      "         [[ 0.6043, -0.6314, -0.0417,  ..., -0.4747,  0.4116,  0.2329],\n",
      "          [ 0.5692, -0.1159, -1.0422,  ..., -0.3507, -0.1645,  0.3800]]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[-2.3082,  0.7651,  0.5504,  ...,  1.5358,  2.5475, -0.5222],\n",
      "         [ 0.1283,  0.9206,  0.6221,  ...,  0.3005,  1.4119,  0.8977],\n",
      "         [ 1.4134,  0.6622,  0.7189,  ...,  0.0411,  1.2354,  0.1287],\n",
      "         ...,\n",
      "         [ 0.6860, -0.8400,  0.8861,  ...,  1.1734,  2.3122,  1.3912],\n",
      "         [ 0.6597,  0.3663,  0.4045,  ...,  0.9252,  1.9168,  0.9830],\n",
      "         [ 0.3159, -0.3887, -0.1658,  ...,  0.6615,  1.9060,  1.3306]],\n",
      "\n",
      "        [[-2.7485,  1.1340,  0.2809,  ...,  1.4203,  2.3373, -1.0806],\n",
      "         [-0.3841,  1.2072,  0.9493,  ..., -0.8414,  1.8459,  0.7758],\n",
      "         [ 1.9556, -0.0805,  0.0745,  ...,  1.4761,  1.6672,  0.7070],\n",
      "         ...,\n",
      "         [ 0.9752, -1.2310,  0.2695,  ...,  1.0980,  3.8330,  1.0806],\n",
      "         [-0.3269,  1.4608,  1.0954,  ...,  1.0098,  1.0093,  1.4375],\n",
      "         [ 0.1361, -0.2443, -0.3630,  ...,  0.6604,  1.9624,  1.1708]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[[-2.3082,  0.7651,  0.5504,  ...,  1.5358,  2.5475, -0.5222],\n",
      "         [ 0.1283,  0.9206,  0.6221,  ...,  0.3005,  1.4119,  0.8977],\n",
      "         [ 1.4134,  0.6622,  0.7189,  ...,  0.0411,  1.2354,  0.1287],\n",
      "         ...,\n",
      "         [ 0.6860, -0.8400,  0.8861,  ...,  1.1734,  2.3122,  1.3912],\n",
      "         [ 0.6597,  0.3663,  0.4045,  ...,  0.9252,  1.9168,  0.9830],\n",
      "         [ 0.3159, -0.3887, -0.1658,  ...,  0.6615,  1.9060,  1.3306]],\n",
      "\n",
      "        [[-2.7485,  1.1340,  0.2809,  ...,  1.4203,  2.3373, -1.0806],\n",
      "         [-0.3841,  1.2072,  0.9493,  ..., -0.8414,  1.8459,  0.7758],\n",
      "         [ 1.9556, -0.0805,  0.0745,  ...,  1.4761,  1.6672,  0.7070],\n",
      "         ...,\n",
      "         [ 0.9752, -1.2310,  0.2695,  ...,  1.0980,  3.8330,  1.0806],\n",
      "         [-0.3269,  1.4608,  1.0954,  ...,  1.0098,  1.0093,  1.4375],\n",
      "         [ 0.1361, -0.2443, -0.3630,  ...,  0.6604,  1.9624,  1.1708]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[[[-2.3082,  0.7651,  0.5504,  ...,  0.7859, -0.6492, -0.1826],\n",
      "          [-0.5673, -1.0926,  0.1068,  ...,  1.5358,  2.5475, -0.5222]],\n",
      "\n",
      "         [[ 0.1283,  0.9206,  0.6221,  ..., -1.9348,  1.4713, -0.1107],\n",
      "          [ 0.1369,  0.3139,  0.5301,  ...,  0.3005,  1.4119,  0.8977]],\n",
      "\n",
      "         [[ 1.4134,  0.6622,  0.7189,  ...,  0.3717, -0.1601, -0.8259],\n",
      "          [ 0.0255, -0.5125, -0.7821,  ...,  0.0411,  1.2354,  0.1287]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.6860, -0.8400,  0.8861,  ...,  0.8988,  0.5393,  0.6203],\n",
      "          [ 1.1425, -1.2861, -1.0775,  ...,  1.1734,  2.3122,  1.3912]],\n",
      "\n",
      "         [[ 0.6597,  0.3663,  0.4045,  ..., -0.4761, -1.2813, -1.1802],\n",
      "          [ 1.2211, -1.1852, -1.6645,  ...,  0.9252,  1.9168,  0.9830]],\n",
      "\n",
      "         [[ 0.3159, -0.3887, -0.1658,  ...,  0.4399,  0.0549,  0.4000],\n",
      "          [ 2.0346, -0.0538, -0.8923,  ...,  0.6615,  1.9060,  1.3306]]],\n",
      "\n",
      "\n",
      "        [[[-2.7485,  1.1340,  0.2809,  ...,  0.9466, -0.1298,  0.2705],\n",
      "          [-1.2361, -0.5180,  0.7297,  ...,  1.4203,  2.3373, -1.0806]],\n",
      "\n",
      "         [[-0.3841,  1.2072,  0.9493,  ..., -1.0432,  2.3208,  0.4396],\n",
      "          [-1.5662, -0.3035,  0.8778,  ..., -0.8414,  1.8459,  0.7758]],\n",
      "\n",
      "         [[ 1.9556, -0.0805,  0.0745,  ...,  1.0531, -0.0900, -0.2324],\n",
      "          [ 0.9524, -0.2288, -1.7176,  ...,  1.4761,  1.6672,  0.7070]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.9752, -1.2310,  0.2695,  ...,  1.2128, -1.0370,  0.1020],\n",
      "          [ 1.1809, -1.2216, -0.5878,  ...,  1.0980,  3.8330,  1.0806]],\n",
      "\n",
      "         [[-0.3269,  1.4608,  1.0954,  ..., -0.9398, -0.2387,  0.1908],\n",
      "          [-0.1161, -0.2387, -0.1029,  ...,  1.0098,  1.0093,  1.4375]],\n",
      "\n",
      "         [[ 0.1361, -0.2443, -0.3630,  ...,  0.5157,  0.3278,  0.6913],\n",
      "          [ 1.8884,  0.3167, -0.6189,  ...,  0.6604,  1.9624,  1.1708]]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[[-0.2502,  0.1916,  0.6468,  ..., -0.2498, -0.1294, -0.5669],\n",
      "          [-0.1131,  0.1815,  0.5153,  ..., -0.0164, -0.2026, -0.4950],\n",
      "          [-0.1127,  0.2553,  0.4889,  ..., -0.0324, -0.1396, -0.4642],\n",
      "          ...,\n",
      "          [-0.0632,  0.2089,  0.4554,  ...,  0.0394, -0.1776, -0.4562],\n",
      "          [-0.0068,  0.1558,  0.5107,  ..., -0.0260, -0.1577, -0.5035],\n",
      "          [-0.0694,  0.1837,  0.5020,  ...,  0.0347, -0.1825, -0.4828]],\n",
      "\n",
      "         [[ 0.7569, -0.7743, -1.0517,  ...,  0.7334,  1.6199,  0.8744],\n",
      "          [ 0.8545, -0.7449, -0.9325,  ...,  0.6133,  1.6644,  0.8931],\n",
      "          [ 0.9055, -0.7429, -0.9926,  ...,  0.6479,  1.7060,  0.8781],\n",
      "          ...,\n",
      "          [ 0.9016, -0.7733, -0.9184,  ...,  0.6361,  1.7070,  0.8978],\n",
      "          [ 0.9676, -0.7355, -0.9783,  ...,  0.6122,  1.7108,  0.9099],\n",
      "          [ 0.8519, -0.7426, -0.8937,  ...,  0.6593,  1.7450,  0.8492]]],\n",
      "\n",
      "\n",
      "        [[[-0.1789,  0.8188, -0.3516,  ...,  0.2704,  0.6805,  0.5076],\n",
      "          [-0.2730,  0.6272,  0.0153,  ...,  0.2915,  0.4262,  0.3405],\n",
      "          [-0.1819,  0.4895,  0.1813,  ...,  0.4085,  0.1136,  0.1850],\n",
      "          ...,\n",
      "          [-0.1777,  0.4989,  0.1872,  ...,  0.4557,  0.0876,  0.1601],\n",
      "          [-0.0207,  0.2811,  0.1789,  ...,  0.4471, -0.0180,  0.0547],\n",
      "          [-0.0998,  0.3918,  0.2235,  ...,  0.4772, -0.0237,  0.0673]],\n",
      "\n",
      "         [[-0.6077, -0.6956, -0.2468,  ...,  0.4680,  1.1940, -0.3626],\n",
      "          [-0.0921, -0.6624, -0.4816,  ...,  0.8057,  1.6584,  0.3867],\n",
      "          [ 0.0743, -0.6579, -0.5267,  ...,  0.7932,  1.7956,  0.4678],\n",
      "          ...,\n",
      "          [-0.0617, -0.6459, -0.4916,  ...,  0.8121,  1.6992,  0.4797],\n",
      "          [ 0.2770, -0.6372, -0.5914,  ...,  0.8014,  1.7871,  0.5592],\n",
      "          [-0.1670, -0.6105, -0.3007,  ...,  0.7562,  1.7350,  0.3021]]]],\n",
      "       grad_fn=<ScaledDotProductFlashAttentionForCpuBackward0>)\n",
      "tensor([[[[-0.2502,  0.1916,  0.6468,  ..., -0.2498, -0.1294, -0.5669],\n",
      "          [ 0.7569, -0.7743, -1.0517,  ...,  0.7334,  1.6199,  0.8744]],\n",
      "\n",
      "         [[-0.1131,  0.1815,  0.5153,  ..., -0.0164, -0.2026, -0.4950],\n",
      "          [ 0.8545, -0.7449, -0.9325,  ...,  0.6133,  1.6644,  0.8931]],\n",
      "\n",
      "         [[-0.1127,  0.2553,  0.4889,  ..., -0.0324, -0.1396, -0.4642],\n",
      "          [ 0.9055, -0.7429, -0.9926,  ...,  0.6479,  1.7060,  0.8781]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0632,  0.2089,  0.4554,  ...,  0.0394, -0.1776, -0.4562],\n",
      "          [ 0.9016, -0.7733, -0.9184,  ...,  0.6361,  1.7070,  0.8978]],\n",
      "\n",
      "         [[-0.0068,  0.1558,  0.5107,  ..., -0.0260, -0.1577, -0.5035],\n",
      "          [ 0.9676, -0.7355, -0.9783,  ...,  0.6122,  1.7108,  0.9099]],\n",
      "\n",
      "         [[-0.0694,  0.1837,  0.5020,  ...,  0.0347, -0.1825, -0.4828],\n",
      "          [ 0.8519, -0.7426, -0.8937,  ...,  0.6593,  1.7450,  0.8492]]],\n",
      "\n",
      "\n",
      "        [[[-0.1789,  0.8188, -0.3516,  ...,  0.2704,  0.6805,  0.5076],\n",
      "          [-0.6077, -0.6956, -0.2468,  ...,  0.4680,  1.1940, -0.3626]],\n",
      "\n",
      "         [[-0.2730,  0.6272,  0.0153,  ...,  0.2915,  0.4262,  0.3405],\n",
      "          [-0.0921, -0.6624, -0.4816,  ...,  0.8057,  1.6584,  0.3867]],\n",
      "\n",
      "         [[-0.1819,  0.4895,  0.1813,  ...,  0.4085,  0.1136,  0.1850],\n",
      "          [ 0.0743, -0.6579, -0.5267,  ...,  0.7932,  1.7956,  0.4678]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.1777,  0.4989,  0.1872,  ...,  0.4557,  0.0876,  0.1601],\n",
      "          [-0.0617, -0.6459, -0.4916,  ...,  0.8121,  1.6992,  0.4797]],\n",
      "\n",
      "         [[-0.0207,  0.2811,  0.1789,  ...,  0.4471, -0.0180,  0.0547],\n",
      "          [ 0.2770, -0.6372, -0.5914,  ...,  0.8014,  1.7871,  0.5592]],\n",
      "\n",
      "         [[-0.0998,  0.3918,  0.2235,  ...,  0.4772, -0.0237,  0.0673],\n",
      "          [-0.1670, -0.6105, -0.3007,  ...,  0.7562,  1.7350,  0.3021]]]],\n",
      "       grad_fn=<TransposeBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO    \u001b[0m \u001b[34mPruning module: bert_encoder_layer_0_attention_self_query\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPruning module: bert_encoder_layer_0_attention_self_key\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPruning module: bert_encoder_layer_0_attention_self_value\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPruning module: bert_encoder_layer_0_intermediate_dense\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPruning module: bert_encoder_layer_0_output_dense\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPruning module: bert_encoder_layer_1_attention_self_key\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPruning module: bert_encoder_layer_1_attention_output_dense\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPruning module: bert_encoder_layer_1_intermediate_dense\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPruning module: bert_encoder_layer_1_output_dense\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPruning module: bert_pooler_dense\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPruning module: classifier\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from chop.pipelines import CompressionPipeline\n",
    "from chop import MaseGraph\n",
    "\n",
    "mg = MaseGraph(model)\n",
    "pipe = CompressionPipeline()\n",
    "\n",
    "quantization_config = {\n",
    "    \"by\": \"type\",\n",
    "    \"default\": {\n",
    "        \"config\": {\n",
    "            \"name\": None,\n",
    "        }\n",
    "    },\n",
    "    \"linear\": {\n",
    "        \"config\": {\n",
    "            \"name\": \"integer\",\n",
    "            # data\n",
    "            \"data_in_width\": 8,\n",
    "            \"data_in_frac_width\": 4,\n",
    "            # weight\n",
    "            \"weight_width\": 8,\n",
    "            \"weight_frac_width\": 4,\n",
    "            # bias\n",
    "            \"bias_width\": 8,\n",
    "            \"bias_frac_width\": 4,\n",
    "        }\n",
    "    },\n",
    "}\n",
    "\n",
    "pruning_config = {\n",
    "    \"weight\": {\n",
    "        \"sparsity\": 0.5,\n",
    "        \"method\": \"l1-norm\",\n",
    "        \"scope\": \"local\",\n",
    "    },\n",
    "    \"activation\": {\n",
    "        \"sparsity\": 0.5,\n",
    "        \"method\": \"l1-norm\",\n",
    "        \"scope\": \"local\",\n",
    "    },\n",
    "}\n",
    "\n",
    "mg, _ = pipe(\n",
    "    mg,\n",
    "    pass_args={\n",
    "        \"quantize_transform_pass\": quantization_config,\n",
    "        \"prune_transform_pass\": pruning_config,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, export the MaseGraph for the compressed checkpoint to be used in future tutorials for hardware generation and distributed deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO    \u001b[0m \u001b[34mExporting MaseGraph to /home/neil/tutorial_5_nas_compressed.pt, /home/neil/tutorial_5_nas_compressed.mz\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mExporting GraphModule to /home/neil/tutorial_5_nas_compressed.pt\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mSaving state_dict format\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mExporting MaseMetadata to /home/neil/tutorial_5_nas_compressed.mz\u001b[0m\n",
      "\u001b[33mWARNING \u001b[0m \u001b[34mFailed to pickle call_function node: finfo\u001b[0m\n",
      "\u001b[33mWARNING \u001b[0m \u001b[34mcannot pickle 'torch.finfo' object\u001b[0m\n",
      "\u001b[33mWARNING \u001b[0m \u001b[34mFailed to pickle call_function node: getattr_3\u001b[0m\n",
      "\u001b[33mWARNING \u001b[0m \u001b[34mcannot pickle 'torch.finfo' object\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "mg.export(f\"{Path.home()}/tutorial_5_nas_compressed\", save_format=\"state_dict\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mase",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
